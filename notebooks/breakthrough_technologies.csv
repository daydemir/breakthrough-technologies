name,year,link,specific_link,blurb,gdp_impact,outlook,type,phase
Brain-Machine Interfaces,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"M I G U E L N I C O L E L I S
Brain-Machine Interfaces
Belle, a nocturnal owl monkey small enough to fit comfortably
in a coat pocket, blinks her out-
sized eyes as a technician plugs four con-
nectors into sockets installed in the top
of her skull. In the next room, measure-
ments of the electrical signals from some
90 neurons in Belle's brain pulse across a
computer screen. Recorded from four
separate areas of Belles cerebral cortex,
the signals provide a window into what
her brain is doing as she reaches to touch
one of four assigned buttons to earn her
reward—a few drops of apple juice.
Miguel Nicolelis, a Duke University neu-
robiologist who is pioneering the use of
neural implants to study the brain, points
proudly to the captured data on the com-
puter monitor and says: ""This readout is
one of a kind in the world.""
The same might be said of Nicolelis,
who is a leader in a competitive and
highly significant field. Only about a
half-dozen teams around the world are
pursuing the same goals: gaining a better
understanding of how the mind works
and then using that knowledge to build
implant systems that would make brain
control of computers and other machines
possible. Nicolelis terms such systems
""hybrid brain-machine interfaces"" or
HBMIs. Recently, working with the Lab-
oratory for Human and Machine Haptics
at MIT, he scored an important first on
the HBMI front, sending signals from
individual neurons in Belle's brain to a
robot, which used the data to mimic the
monkey's arm movements in real time.
In the long run, Nicolelis predicts that
HBMIs will allow human brains to control
artificial devices designed to restore lost
sensory and motor functions. Paralysis
sufferers, for example, might gain control
over a motorized wheelchair or a pros-
thetic arm-perhaps even regain control
over their own limbs. ""Imagine,"" says
Nicolelis, ""if someone could do for the
brain what the pacemaker did for the
heart."" And, in much the same way that a
musician grows to feel that her instrument
is a part of her own body, Nicolelis believes
the brain will prove capable of readily
assimilating human-made devices.
Ongoing experiments in other labs
are showing that this idea is credible. 
At Emory University, neurologist Phillip
Kennedy has helped severely paralyzed
people communicate via a brain implant
that allows them to move a cursor on a
computer screen (see ""Mind Over Mus-
cles,"" TR March/April 2000). And
implants may also shed light on some of
the brain's unresolved mysteries. Nicole-
lis and other neuroscientists still know
relatively little about how the electrical
and chemical signals emitted by the
brain's millions of neurons let us per-
ceive color and smell, or give rise to the
precise movements of Brazilian soccer
players-whose photos adorn the walls
of the São Paolo native's office. ""We
don't have a finished model of how the
brain works,"" says Nicolelis. ""All we have
are first impressions.""

Nicolelis latest experiments, how-
ever, show that by tapping into multiple
neurons in different parts of the brain, it
is possible to glean enough information
to get a general idea of what the brain is
up to. In Belle's case, it's enough informa-
tion to detect the monkey's intention of
making a specific movement a few tenths
of a second before it actually happens.
And it was Nicolelis' team's success at
reliably measuring tens of neurons simul-
taneously over many months-previous-
ly a key technological barrier-that
enabled the r e m a r k a b l e d e m o n s t r a t i o n
with the robot arm.
Still, numerous stumbling blocks
r e m a i n to be o v e r c o m e b e f o r e h u m a n
brains can interface reliably and comfort-
ably with artificial devices, making mind-
controlled prosthetic limbs or computers
more than just lab curiosities. Among the
key challenges is developing electrode
devices and surgical methods that will
allow safe, long-term recording of neu-
ronal activities. Nicolelis says he's begun
working with Dukes biomedical engineer-
ing department to develop a telemetry
chip that would collect and transmit data
through the skull, without unwieldy sock-
ets and cables. And this year Nicolelis will
become co-director of Duke's new Center
of Neuroengineering and Neurocomputa-
tion, which will explore new combinations
of computer science, chip design and neu-
roscience. Nicolelis sees the effort as part
of an impending revolution that could
eventually make HBMIs as commonplace
as Palm Pilots and spawn a whole new
industry-centered around the brain.
— Antonio Regalado

Others in Brain-Machine Interfaces
Organization Project
Andy Schwartz
(Arizona State University)
Neural control of robotic arm
John Donoghue
(Brown University)
Brain representation of
movement
Richard Andersen
(Caltech) Improved neuroelectrode
systems
Phillip Kennedy, Roy Bakay (Emory University)
Communication systems for
paralyzed patients",<0.01% - <0.1%,Positive,Biotech,methodology
Flexible Transistors,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"CHERIE KAGAN
Flexible Transistors
The implementation of pervasive
computing— the spread of digital
information throughout soci-
ety-will require electronics capable of
bringing information technology off the
desktop and out into the world (see
""Computing Goes Everywhere,"", p. 52). To
digitize newspapers, product labels and
clothing, integrated circuits must be
cheap and flexible—a tough combination
for today's silicon technology. Even the
cheapest form of silicon electronics-the
cut-rate ""amorphous"" silicon used to
drive laptop display screens-is too pric-
ey. What's more, it's difficult to incorpo-
rate silicon electronics on bendable sur-
faces such as plastics.
Technology innovators are taking a
couple of routes around these limits.
Some researchers are trying to reinvent
amorphous silicon. Others have aban-
doned inorganic compounds like silicon
to develop transistors based on organic
(carbon-based) molecules or polymers.
These organic electronics are inexpensive
to manufacture and compatible with
plastic substrates. Indeed, research teams
at places such as Lucent Technologies'
Bell Labs, England's University of Cam-
bridge and Pennsylvania State University
have made impressive progress, and com-
mercial products are nearing the market. Last fall, for example, Philips Research in Eindhoven, the Netherlands, showed off the first prototype of a rudimentary dis-
play driven by polymer semiconductors.
But there's a catch: Organics are far slow-
er than their silicon cousins.
Now, a 31-year-old materials scientist
at IBM, Cherie Kagan, may have opened
the door to cheap, flexible electronics that
pack the mojo needed to bring ubiquitous
computing closer. Her breakthrough? A
compromise: transistors made from mate-
rials that combine the charge-shuttling
power and speed of inorganics with the
affordability and flexibility of organics.
These hybrids were created by chem-
ist David Mitzi at IBM's Thomas J. Wat-
son Research Center in Yorktown
Heights, N.Y. By the time Kagan arrived
at Watson in 1998 following a stint at Bell
Labs (she earned a PhD from MIT in
1996), Mitzi had already shown that his
materials possessed intriguing electronic
properties. Kagan had a hunch they
might make good transistors. But she
needed quick results; shed been hired as
a postdoc—a limited-time offer.
At the outset, the transistors flipped
on and off sluggishly. ""The first times, didn't want to calculate [the speed],"" says
Kagan. But she kept tweaking, and in less
than a year she had increased the mobil-
ity of electric charges through her tran-
sistors by four orders of magnitude—
matching the speed of amorphous silicon
and far exceeding most organic transis-
tors. The results won her a staff position
and her own lab at IBM.
Kagan has since increased the speed
by another 50 percent; further fine-tun-
ing, she believes, could provide at least
another doubling in acceleration. Not
only may the hybrids be far faster than
amorphous silicon, they have a key
advantage over silicon-based electronics.
Like some organic materials used to make
transistors, the hybrid materials can be
dissolved and printed onto paper or plas-
tic just like particles of ink. ""I make my
materials in a different lab and carry
them over and add some liquid and spin
them on,"" says Kagan. ""It's not very
sophisticated, which is sort of the goal,
right? You really want it to be cheap.""
Thomas Jackson, a transistor expert
at Penn State who is developing organic
circuits, says Kagan's ""fledgling results""
could pave the way for fast yet flexible
electronics. Jackson credits Kagan with
seizing the opportunity. ""Not only does
she have her own pocket of competence,
but she's able to look around and see excit-
ing possibilities and then bring things
together. IBM has been working on these
sorts of materials for some time, but it
took the energy and enthusiasm and vision
and perspective of Cherie to translate that
into a thin-film transistor.""
The transistors could compete with
organic electronics in a variety of applica-
tions, such as radio-frequency product
ID tags. And then there's the $20 billion-
per-year market for flat-panel video dis-
plays, where the speed of Kagan's transis-
tors could really make a difference.
Quicker circuits would deliver sharper
displays than those driven by amorphous
silicon at a fraction of the cost. That
would open the door to affordable wall-
sized displays or high-quality displays
that pop out of your pen. If all goes well,
the materials could be used in cheap,
flexible displays within five years.
Of course, bright displays that fit in
your pocket will require portable power,
and that has Kagan pondering her next
research challenge: cheap, flexible mate-
rials for solar cells to liberate pervasive
computing from bulky batteries.
""You
aren't going to want to carry a battery
around with your lightweight flexible
display,"" she says. -Peter Fairley

Others in Flexible Transistors
Organization, Project
Lucent/Bell Labs (Murray Hil, NJ), Organic circuits
Richard Friend (University of Cambridge), Organic light-emitting diodes
Joseph Jacobson (MIT), Printed inorganics
Thomas Jackson(Penn State), Organic transistors",0.05% - 0.5%,Positive,Hardware,product
Data Mining,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"U S A M A F A Y Y A D
Data Mining
ello again, Sidney P. Manyclicks.
We have recommendations for
you. Customers who bought this
title also bought..""
Intrusive? A touch of personal atten-
tion in the sterile world of e-shopping?
Both, perhaps-but definitely a tour de force of database technology. Conventional databases sort though a few mega-
bytes of structured data to find answers to
specific queries. But compiling a simple
recommendation list requires a system
that can burrow through gigabytes of Web
site visitor logs in search of patterns no
one can anticipate in advance.
Welcome to data mining, also known
as knowledge discovery in databases
(KDD): the rapidly emerging technology that lies behind the personalized Web and much else besides. The emphasis here is
on ""emerging,"" says Usama Fayyad, who
should know: data mining didn't exist as
a field until he helped pioneer it.
In 1987, the Tunisian-born computer
scientist was a graduate student at the
University of Michigan. He had taken a
summer job with General Motors, which
was compiling a huge database on car
repairs. The idea, he says, was to enable
any GM service technician to ask the data-
base a question based on the model of car,
the engine capacity, and so on, and get a
quick, appropriate response. Sounds
straightforward. But, recalls Fayyad, ""there
were hundreds of millions of records—no
human being could go through it all"" The
pattern recognition algorithm he devised
to solve that problem became his 1991
doctoral dissertation, which is still among
the most cited publications in the data-
mining field.
Data mining proved to have surpris-
ingly broad application. Fayyad left Mich-
igan for NASA's Jet Propulsion Laboratory,
where he applied his techniques to astro-
nomical research. In particular, his algo-
rithm helped in automatically determining
which of some two billion observed celes-
tial objects were stars and which were
galaxies. The tool also helped find volca-
noes on Venus from the huge number of
radar images being transmitted from space
probes. A geologist could retrieve the
image of a previously identified volcano;
the computer would then examine the
picture for patterns and search through
other images for similar patterns. That
worked so well, Fayyad says, that ""pretty
soon the military intelligence people were
all over us, wanting to use it. And so were
doctors, who wanted to do automatic
searches of radiology images."" In 1995, in
response to this widening interest, Fayyad
and his colleagues planned a full-scale
international conference on KDD. The
conference drew about 500 participants,
more than double what had been expect-
ed. (KDD 2000 drew 950.)
By this time, with the Internet gush-
ing information onto everyone's desktop,
the urgency for data mining was becom-
ing evident in the corporate world. IBM
and other industry giants sensed a mar-
k e t - a n d wanted in. Microsoft set its
sights on Fayyad and enticed him to join
the company's research labs. ""They sug-
gested that I take a look at databases in
the corporate world,"" says Fayyad. ""It was
pretty sad. In many companies, the data
warehouses' were actually 'data tombs':
the data went in and were never looked
at again."" Fayyad joined Microsoft in
1996 and organized a new research group
in data mining. ""We looked at new algo-
rithms for scaling up to very large data-
bases—gigabytes or larger,"" he says.
By decade's end, Fayyad had caught the entrepreneurial bug sweeping through computer science labs. ""I realized that even the organizations that loved the idea of data mining were having trouble just maintaining their data."" What they needed, he reasoned, was a company to host their databases for them, and provide data-mining services on top of that. The result was digiMine, a Kirkland, Wash., startup that opened for business in March 2000 with Fayyad as CEO.
And the future of data-mining technology? Wide open, says Fayyad--especially as researchers begin to move byond the field's original focus on highly structured relational databases. One very hot area is ""text data mining"": extracting unexpected relationships from huge collections of free-form text documents. The results
are still preliminary, as various labs exper-
iment with natural-language processing,
statistical word counts and other tech-
niques. But the University of California at
Berkeley's LINDI system, to take one
example, has already been used to help
geneticists search the biomedical literature
and produce plausible hypotheses for the
function of newly discovered genes.
Another hot area, says Fayyad, is
""video mining"": using a combination of
speech recognition, image understanding
and natural-language processing tech-
niques to open up the world's vast video
archives to efficient computer searching.
For instance, when Carnegie Mellon Uni-
versity's Informedia II system is given an
archive of, say, CNN news clips, it pro-
duces a computer-searchable index by
automatically dividing each clip into
individual scenes accompanied by tran-
scripts and headlines.
Fayyad hopes that ultimately the
techniques of data mining will become so
successful and so thoroughly integrated
into standard database systems that they
will no longer be thought of as exotic.
""People will just assume that their data-
base software will do what they need.""
—M. Mitchell Waldrop


Others in Data Mining
Organization, Project
Howard Wactlar (Carnegie Melon), Search very large video collections
Marti Hearst (University of California, Berkeley), Automated discovery of new information from large text collections
Nokia Research Center (Helsinki, Finland), Finding recurrent episodes in event sequence data
Raghu Ramakrishnan (University of Wisconsin), Visual exploration of data on the Web",3-30%,Positive,Software,methodology
Digital Rights Management,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"Ranjit Singh
Digital Rights Management
Sitting in his office in McLean, VA., Ranjit Singh is at ground zero of what may be the biggest--and bloodiest--of the many battles that will shape the Internet during the 21st century's initial decade. The battle lines are sharply drawn. On one side are owners of intellectual property, or content: books, music, video, photographic images and more. On the other are Internet users--think Napster-- who want content to be freely distributed.
And then there is Singh, president of ContentGuard, a company that spun out of research at Xerox's Palo Alto Research Center (PARC) on a mission to commercialize content protection in a wired world. ""The Internet changes everything,"" says Singh, 48, an England-born technology manager whose resume glitters with senior positions at Xerox, Citibank, and Digital Equipment plus a number of startups. ""The Internet,"" Singh continues, ""allows perfect reproduction of digital content and totally frictionless distribution."" A few mouse
clicks send a work to millions of users,
but the creators and owners of the con-
tent won't necessarily collect dime one
(see ""Your Work Is Mine!"" TR November/
December 2000).
Ouch! You can bet the pain felt by
content owners who see their stuff flying
everywhere via the Net will translate into
action. Which is what Singh and Con-
tentGuard are about. Digital rights man-
agement, or DRM, is ""the catalyst for a
revolution in e-content,"" says Singh.
""DRM will allow content owners to get
much wider and deeper distribution than ever before,"" he maintains. ""You can see
who is passing your content to whom.""
Stripped to its essence, DRM-as
provided by ContentGuard and a number
of competitors-amounts to an encryp-
tion scheme with a built-in e-business
cash register. Content is encoded, and to
get the key a user needs to do some-
thing-maybe paying money, maybe pro-
viding an e-mail address. DRM providers
deliver the protection tools; it is up to
content owners to set the conditions.
ContentGuard uses a ""multiple key""
approach to content protection; anyone
who received bootleg content would have
to crack into it all over again. Thus, Singh
explains, ""even if a hacker cracks into a
piece of content, he cannot distribute it."" So why isn't DRM ubiquitous? Two
reasons. First, content owners are in the
midst of a hard rethink about both pric-
ing and distribution. Suddenly they are
wrestling with issues of how to price
three listens to a song, say, or a download
of a low-resolution image that cannot be
forwarded to others. ""Content owners
now are trying out different economic
models for valuing content,"" says Singh,
whose company provides DRM tools to,
among others, John Wiley & Sons and
Houghton Mifflin. ""DRM opens many
possibilities,"" he adds.
The second issue may be the more
nettlesome: ""The user experience has to
hide the complexity of the protection
technologies,"" says Singh. Users need to be able to buy the content they want
""without needing special viewers or
downloads and without putting the user
through hoops, he argues. To resolve
that, ContentGuard has forged multiple
partnerships with digital standard-bear-
ers such as Microsoft and Adobe Systems,
and has extended its technology so that
it applies across many media, including
books, music and video.
Captivating as the possibilities of
DRM are, it is still in its early days. Says
John Schwarz, CEO of Reciprocal, Inc., a
distributor of ContentGuard and other
DRM solutions: ""We are probably a year
or so away from seeing broad adoption
of DRM by the marketplace.""
Some analysts are more skeptical: ""I'm not convinced content can be pro-
tected in the Internet era,""
says Eric
Scheirer, who tracks DRM issues for For-
rester Research. ""People want flexible
access to content."" Proof is Napster, of
course, which represents a phenomenon
Scheirer calls ""unstoppable."" Even if Nap-
ster is put out of business by the courts,
he predicts that the frictionless distribu-
tion of digital content among the millions
of Internet users will live on.
But Singh is betting heavily that
DRM will prevail and, ironically, he
thanks Napster. ""Napster turned this
whole issue into a CEO-level question.
The very highest corporate officers now
are looking into content management
issues, and they want to protect their
property."" That, says Singh, augurs wider use of
DRM. ""Here's the virtual cycle you will
see: The more content a business puts
online, the faster it will want to put still
more content up, because it will see the
economic benefits and users will see the
benefits of gaining access to more con-
tent. That's why we are seeing an explo-
sion here."" -Robert McGarvey

Others in Digital Rights
Organization, Project
InterTrust Technologies (Santa Clara, Calif.), Develops peer-to-peer distributed DRM technology
Reciprocal (New York), DRM clearinghouse
Digimarc (Tualatin, Ore.), Watermarking to embed an imperceptible code
Alchemedia (San Francisco), ""Clever Content"" platform safeguards digital content",-0.05% to -0.5%,Negative,Software,platform
Biometrics,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"JOSEPH ATICK
Biometrics
n one sense, the field of biomet-
rics-identifying individuals by
specific biological traits-has
already emerged. Large companies use
fingerprint sensors for logging on to cor-
porate networks, state driver's license
authorities employ face recognition for
capturing and storing digital photo-
graphs, and the first iris-scan-protected
ATM in the nation was introduced in
Texas in May 1999. Yet consumers have
been reluctant to adopt the technology,
and so far, it remains largely relegated to
military and government applications.
But the emergence of another tech-
nology-the wireless Web-could soon
change all that, according to Joseph Atick,
president and CEO of Visionics, one of
the leaders in face recognition technology.
""Personal digital assistants (PDAs) and
cell phones are becoming our portal to
the world, our transaction devices, our ID
and maybe one day our passport,"" says
Atick. But entrusting these small gadgets
with so much of our personal and finan-
cial information carries with it a great
risk. ""It is this need for security"""" Atick
says, ""that is going to drive biometrics.""
And while the need for security is
pushing the demand for biometric sys-
tems, other technology developments—
increased bandwidth, new cell phones
and handheld computers equipped with
digital cameras-will create an infra-
structure capable of putting biometrics
into the hands of consumers. Visionics is
taking advantage of this combination of
need and infrastructure by developing
tools to enable people to authenticate any
transaction they make over the wireless
Web using their own faces.
Even those in the industry who are skeptical of Atick's vision of a biometric-
enabled wireless Web can't deny his inge-
nuity and ambition. At the age of 15, while
living in Israel, Atick dropped out of
school to write a 600-page physics text-
book entitled Introduction to Modern
Physics. ""I was bored in school. I wanted
to show the establishment I was serious
about my interests,"" says Atick. ""This book
was my ticket to grad school."" Remarkably,
Stanford University accepted him at 16
into its graduate program, where he earned
his master's degree in physics and PhD in
mathematical physics.
After graduation, Atick applied his
math skills to the study of the human
mind. While heading the Computational
and Neuroscience Laboratory at Rocke-
feller University, he sought to understand
how the brain processes the abundance of
visual information thrown at it by the
environment. He and his colleagues dis-
covered that the brain deals with visual
information much as computer algo-
rithms compress files. Because everyone
has two eyes, a nose and lips, the brain
extracts only those features that typically
show deviations from the norm, such as
the bridge of the nose or the upper cheek-
bones. The rest it fills in. ""We soon real-
ized there was tremendous commercial value to this process,"" says Atick. In 1994,
he and colleagues Paul Griffin and Nor-
man Redlich founded Visionics.
Based in Jersey City, N.J., Visionics
develops and markets pattern-recognition
software called Facelt. In contrast to the
main competing technology, which relies
on data from the entire face, Facelt verifies
a person's identity based on a set of
14 facial features that are unique to the
individual and unaffected by the presence
of facial hair or changes in expression. In
the past few years, the system has found
success fighting crime in England and
election fraud in Mexico.
In October, the company signed a
merger agreement with Digital Biomet-
rics, a Minnetonka, Minn.-based biomet-
ric systems engineering firm. Together
they plan to build the first line of ""bio-
metric network appliances"" -computers
hooked to the Net with the capacity to
store and search large databases of facial
or other biometric information. The
appliances, containing customers identi-
fication data, can then receive queries
from companies wanting to authenticate
e-transactions. And while consumers will
be able to access the system from a cell
phone, PDA or desktop computer, Atick
expects handheld devices to be the biggest market. Visionics is also working with
companies in Japan and Europe to have
Facelt software installed on new Web-
ready mobile devices so consumers can
capture their own faces and submit
encrypted versions over the Net.
Is that it for PINs and passwords?
Atick predicts it will still be two to three
years before PDA- and cell-phone-wield-
ing consumers are likely to use biomet-
rics instead. A n d as futuristic as his vision
is, he is really striving toward something
a bit old-fashioned. ""Essentially, we are
bringing back an old element of human
commerce,"" says Atick-restoring the
confidence that comes with doing busi-
ness face to face. 
-Alexandra Stikeman

Others in Biometrics
Organization, Project
Viisage Technology (Littleton, Mass.), Face recognition
Iridian Technologies (Marlton, N.J.), Iris recognition
DigitalPersona (Redwood City, Calif.), Fingerprint recognition
Cyber-SIGN (San Jose, Calif.), Dynamic signature 
T-NETIX (Englewood, Colo.), Voice recognition",0.05% - 0.5%,Positive,Software,Platform.
Natural Language Processing,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"KAREN JENSEN
Natural Language Processing
The 1968 film 2001: A Space Odys-
sey gave us a vision of the mil-
l e n n i u m b a s e d o n the techno-
logical predictions of the day. One result:
HAL 9000, a computer that conversed
easily with its shipmates like any other
crew member. The timing was off: In the
real 2001, there's not a computer in the solar system as articulate as HAL.
But maybe it wasn't that far off. HAls
modern-day counterparts are catching up
fast (sans the homicidal tendencies, one
hopes). Already we have commercial
speech recognition software that can take
dictation, speech generation equipment
that can give mute people voices and
software that can ""understand"" a plain-
English query well enough to extract the
right answers from a database.
Emerging from the laboratories,
moreover, is a new generation of inter-
faces that will allow us to engage comput-
ers in extended conversation—an activ-
ity that requires a dauntingly complex
integration of speech recognition, natu-
ral-language understanding, discourse
analysis, world knowledge, reasoning
ability and speech generation. It's true
that the existing prototypes can only talk
about such well-defined topics as weath-
er forecasts (MIT's Jupiter), or local
movie schedules (Carnegie Mellon's Mov-
ieline). But the Defense Advanced
Research Projects Agency (DARPA) is
working on wide-ranging conversational
interfaces that will ultimately include
pointing, gesturing and other forms of
visual communication as well.
Parallel efforts are under way at
industry giants such as IBM and Microsoft, which see not only immediate applications for computer users who need to
keep their hands and eyes free but also
the rapid evolution of speech-enabled
""intelligent environments."" The day is
coming when every object big enough to
hold a chip actually has one. Wed better
be able to talk to these objects because very few of them will have room for a
keyboard.
Getting there will be a huge chal-
lenge-but that's exactly what attracts
investigators like Karen Jensen, the gung-
ho chief of the Natural Language Process-
ing group at Microsoft Research. Says
Jensen: ""I can't imagine anything that
would be more thrilling, or carry more
potential for the future, than to make it
possible for us to truly interact with our
computers. That would be so exciting!""
Such declarations are typical of Jens-
en, who at 62 remains as exuberant about
technology's promise as any teenager-
and just as ready to keep hacker's hours.
Indeed, Jensen was one of the first people
Microsoft hired when it opened its
research lab in 1991. Along with col-
leagues Stephen Richardson and George
Heidorn, she arrived at the Redmond,
Wash., campus from IBM's Thomas ).
Watson Research Center, where they had
worked on some of the earliest grammar-
checking software, and immediately
started building a group that now num-
bers some 40 people.
In Redmond, Jensen and her col-
leagues soon found themselves contribut-
ing to the natural-language query inter-
face for Microsoft's Encarta encyclopedia
and to the grammar checker that first
appeared in Word 97. And now, she says,
they ve begun to focus all their efforts on
a unique technology known as MindNet.
MindNet is a system for automatically
extracting a massively hyperlinked web of concepts from, say, a standard diction-
ary. If a dictionary defines ""motorist"" as
""a person who drives a car,"" for example,
MindNet will use its automatic parsing
technology to find the definition's under-
lying logical structure, identifying
""motorist"" as a kind of person, and
""drives"" as a verb taking motorist as a
subject and car as an object. The result is
a conceptual network that ties together
all of human understanding in words,
says Jensen.
The very act of putting this concep-
tual network into a computer takes the
machine a long way toward ""understand-
ing"" natural language. For example, to
figure out that ""Please arrange for a meet-
ing with John at 11 oclock"" means the
same thing as
""Make an appointment
with John at 11, the computer simply has
to parse the two sentences and show that they both map to the same logical struc-
tures in MindNet. ""I's not perfect grok-
king, Jensen concedes. ""But it's a darn
good first step.""
MindNet also promises to be a pow-
erful tool for machine translation, Jensen
says. The idea is to have MindNet create
separate conceptual webs for English and
another language, Spanish, for example,
and then align the webs so that the Eng-
lish logical forms match their Spanish
equivalents. MindNet then annotates
these matched logical forms with data
from the English-Spanish translation
memory, so that translation can proceed
smoothly in either direction.
Indeed, says Jensen, who is now in
the process of passing on the leadership
of the group to the younger generation,
MindNet seems to tie together everything
they ve been doing for the past nine years:
""All we see is doors opening. We don't see
any closing!"" 
- M. Mitchell Waldrop

Others in Language Processing
Organization, Project
Victor Zue (MIT Laboratory for Computer Science), Conversational interfaces
Alexander I. Rudnicky (Carnegie Melon), Verbal interaction with small computers
Ronald A. Cole (University of Colorado), Domain-specific conversational systems
BBN Technologies (Cambridge, Mass.), Dialog agent",3% to 30%,Positive,Software,Methodology
Microphotonics,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"JOHN JOANNOPOULOS
Microphotonics
ight bounces off the small yellow
square that MIT physics profes-
-
sor John Joannopoulos is show-
ing off. It looks like a scrap of metal,
something a child might pick up as a
plaything. But it isn't a toy, and it isn't
metal. Made of a few ultrathin layers of
non-conducting material, this photonic
crystal is the latest in a series of materials
that reflect various wavelengths of light
almost perfectly. Photonic crystals are on
the cutting edge of microphotonics: tech-
nologies for directing light on a micro-
scopic scale that will make a major impact
on telecommunications.
In the short term, microphotonics
could break up the logjam caused by the
rocky union of fiber optics and elec-
tronic switching in the telecommunica-
tions backbone. Photons barreling
through the network's optical core run
into bottlenecks when they must be con-
verted into the much slower streams of
electrons that are handled by electronic
switches and routers. To keep up with the
Internet's exploding need for bandwidth,
technologists want to replace electronic
switches with faster, miniature optical
devices, a transition that is already under
way (see ""The Microphotonics Revolution,""
TR July/August 2000).
Because of the large payoff-a much
faster, all-optical Internet-many com-
petitors are v y i n g to create such devices.
Large telecom equipment makers, includ-
ing Lucent Technologies, Agilent Tech-
nologies and Nortel Networks, as wel as
a number of startup companies, are
developing new optical switches and
devices. Their innovations include tiny
micromirrors, silicon waveguides, even
microscopic bubbles to better direct
light.
But none of these fixes has the tech-
nical elegance and widespread utility of
photonic crystals. In Joannopoulos' lab,
photonic crystals are providing the means
to create optical circuits and other small,
inexpensive, low-power devices that can
carry, route and process data at the speed
of light. ""The trend is to make light do as
many things as possible,"" Joannopoulos
says. ""You may not replace electronics
completely, but you want to make light
do as much as you can.""
Conceived in the late 1980s, photo nic crystals are to photons what semicon-
ductors are to electrons, offering an
excellent medium for controlling the flow
of light. Like the doorman of an exclusive
club, the crystals admit or reflect specific
photons depending on their wavelength
and the design of the crystal. In the 1990s,
Joannopoulos suggested that defects in
the crystals' regular structure could bribe
the doorman, providing an effective and
efficient method to trap the light or route
it through the crystal.
Since then, Joannopoulos has been a
pioneer in the field, writing the definitive
book on the subject in 1995: Photonic
Crystals: Molding the Flow of Light.
""That's the way John thinks about it,"" says
M I T materials scientist a n d collaborator
Edwin Thomas. ""Molding the flow of
light, by confining light and figuring out
ways to make light do his bidding— bend,
go straight, split, come back together-in
the smallest possible space.""
Joannopoulos group has produced
several firsts. They explained how crys-
tal filters could pick out specific streams
of light from the flood of beams in wave-
length division multiplexing, or WDM,
a technology used to increase the
amount of data carried per fiber (see
""Wavelength Division Multiplexing,"" TR
March/April 1999). The lab's work on
two-dimensional photonic crystals set
the stage for the world's smallest laser
and electromagnetic cavity, key compo-
nents in building integrated optical cir-
cuits. But even if the dream of an all-optical
Internet comes to pass, another problem
looms. So far, network designers have
found ingenious ways to pack more and
more information into fiber optics, both
by improving the fibers and by using
tricks like WDM. But within five to 10
years, some experts fear it won't be pos-
sible to squeeze any more data into exist-
ing fiber optics.
The way around this may be a type of
photonic crystal recently created by Joan-
nopoulos group: a ""perfect mirror"" that
reflects specific wavelengths of light from
every angle with extraordinary efficiency.
Hollow fibers lined with this reflector
could carry up to 1,000 times more data
than current fiber optics-offering a
solution when glass fibers reach their
limits. And because it doesn't absorb and
scatter light like glass, the invention may
also eliminate the expensive signal ampli-
fiers needed every 60 to 80 kilometers in today's optical networks (see ""Blazing
Data,"" TR November/December 2000).
Joannopoulos is now exploring the
theoretical limits of photonic crystals.
How much smaller can devices be made,
and how can they be integrated into opti-
cal chips for use in telecommunications
and, perhaps, ultrafast optical computers? Says Joannopoulos: ""Once you start
being able to play with light, a whole new
world opens up."" 
— Erika Jonietz

Others in Microphotonics
Organization, Project
Eli Yablonovitch (UCLA), Photonic crystals for optical and radio frequencies
Susumu Noda (Kyoto University, Japan), Optical integrated circuits
Axel Scherer (Caltech), Optical switches, waveguides and lasers
Nanovation Technologies (Miami), Integrated devices for telecom
Clarendon Photonics (Boston), Filters for WDM",0.01% - 0.10%,Positive.,Nanotech,methodology
Untangling Code,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"G R E G O R K I C Z A L E S
Untangling Code
ity software engineers. With the
P touch of a button, their programs
let us make global fixes in a long
text, say, or a spreadsheet, yet program-
mers often need to correct their own work
one tedious line at a time. That irony isn't
lost on Gregor Kiczales, principal scientist
at Xerox's Palo Alto Research Center
(PARC) and professor at the University of
British Columbia in Vancouver-and he
has a fix in mind. Kiczales champions
what he calls ""aspect-oriented program-
ming,"" a technique that will allow software
writers to make the same kinds of short-
cuts that those of us in other professions
have been making for years.
One such ""crosscutting"" capability is
logging-the ability to trace and record
every operation the application performs.
Since any given c o m m a n d m i g h t touch
down on functionally unrelated areas of
the code, programmers now must make a rule, such as: ""When adding a new func-
tion to this application, always put a trace
statement in."" Of course, the rule works
only if people remember to follow it.
Other crosscutting capabilities
include security and synchronization-
the ability to make sure that two users
don't try to access the same data at the
same time. Both require programmers to
write the same functionality into many
different areas of the application. Even a
modest-sized application can easily pres-
ent 100 crosscutting issues.
Programmers try to track these
instances of repetition, so that when a
capability needs to be changed or upgrad-
ed, it can be done uniformly throughout
the program. But keeping track of cross-
cutting concerns is an error-prone pro-
cess. Forget to upgrade just a few of these
instances, and your code starts collecting
bugs. ""We're forced to keep track of every-
thing in our heads,"" says Kiczales.
Kiczales proposed solution is to cre-
ate a new category within a programming language called an ""aspect."" Aspects allow
programmers to write, view and edit a
crosscutting concern as a separate entity.
Once the programmer is happy with it, a
single keystroke will weave the aspect
into the code wherever it is needed. It's a
smart, intuitive, neat solution to an old
problem. And what's good for program-
mers is good for the rest of us: Wide-
spread adoption of aspects holds out the
promise of less buggy upgrades, shorter
product cycles and, ultimately, better and
less expensive software.
The idea of aspects has been around
for many years and with many different
names. It's called ""adaptive programming""
at Northeastern University, ""subjective
programming"" at IBM, ""composition fil-
tering"" at the University of Twente in the
Netherlands and ""multidimensional sepa-
ration of concerns"" elsewhere. But unlike
these other research projects, Kiczales and
his team at PARC have taken the concept
out of the lab and into the real world by
incorporating the idea of aspects into a
new extension of the programming lan-
guage Java. The beta version of this exten-
sion (called Aspect)) is available for free at
www.aspectj.org, and Kiczales plans to
make release 1.0 ready by June. ""Major
changes in programming methodology
can take 30 years to gain widespread accep-
tance,"" he says. Making aspects an exten-
sion to an existing standard should, he
predicts, ""cut the cycle by 15 or 20 years.""
While Kiczales admits the tools are
still a little raw, there are nevertheless
about 500 users of Aspect] today-most
of them finding existing tools inadequate
for creating long, complicated programs
in Java. Some have already found Aspect)
so solid that they've used it in produc-
tion. One of these is Checkfree.com, a
company that makes software for auto-
matic bill payment. Checkfree sells both
C++ and Java versions of the software.
Rich Price, senior engineer, estimates
that Aspect] allowed his team to imple-
ment an important crosscutting capabil-
ity in the Java version in four program-
mer-hours, whereas the C++ team, with
no aspect-oriented programming tools at
their disposal, took two programmer-
weeks to do the same thing. Using
aspects, he says, ""I make one change, in
one place, and it gets woven in where it
needs to be. I love that.""
By folding their ideas into a practical
Java extension, Kiczales and his team hope to make aspects part of the vernacular of programming languages.
""Aspect) lets programmers work more
quickly and at a higher design level,"" says
Kiczales. ""We've learned that crosscutting
concerns are actually not hard to work
with-once you have the proper pro-
gramming support."" 

— Claire Tristram


Others Untangling Code
Organization, Project
Mehmet Aksit (University of Twente, the Netherlands), Composition filters
Karl Lieberherr (Northeastern University), Adaptive programming
IBM Research (Yorktown Heights, N.Y.), HyperJ system for Java
programming
Mira Mezini (Univ of Siegen, Germany), Enhancing modularity and",0.05-0.5%,Positive.,Software,methodology
Robot Design,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"JORDAN POLLACK
Robot Design
obot builders make a convincing
case that in 2001, robots are
where personal computers were
in 1980-poised to break into the market-
place as common corporate tools and
ubiquitous consumer products perform-
ing life's tedious chores. One big obstacle
remains: It is expensive to design and
make robots smart enough to adapt read-
ily to different tasks and physical environ-
ments, the way human beings do.
That's the reason why robotics have,
so far, found a commercial niche only in
simple and highly repetitive jobs, such as
working on an automotive assembly line,
or mass-producing identical items, such
as toys. The challenge for builders of
robots is to build more complexity into
them without the huge investment of
custom-tailoring each robot for a differ-
ent task.
One promising approach is to fully
automate the design and manufacture of
robotics by deploying computers to con-
ceive, test and even build the configura-
tions of each robotic system: in short, to
use robots to build robots. Last year, in a
cramped lab at Brandeis University in
Waltham, Mass., Jordan Pollack demon-
strated how this automated robotic design
and manufacturing might work.
Pollack, an associate professor of
computer science, together with postdoc
Hod Lipson, directed a computer to
design a moving creature using a limited
set of simple parts: plastic rods, ball joints,
small motors and a ""brain"" (neural net-
work). The computer-using an algo-
rithm inspired by biological evolution—
""evolved"" hundreds of generations of
potential designs, killing off the sluggish
and refining the strong. Eventually, sev-
eral of the fastest and fittest came to life, manufactured in a rapid-prototyping
machine. Pollack and Lipson snapped on
the motors, and the creatures moved.
""I think the important point of our
coevolutionary design and automated man-
ufacturing for robotics is to get small-quan-
tity production to be economical,"" Pollack
says. He predicts that the evolutionary
approach to robot building could lead to
the first cheap industrial robots in five to 10
years. ""If we are successful, we could see an
industry within a decade which makes low-
quantity custom machinery worth more
than it costs to make.""
For now, Pollack's ""automated"" pro-
cess still takes plenty of human interven-
tion and money: Pollack and his col-
league wrote the computer program and
spent $50,000 on the human-built fabri-
cating device. Still, the team's advance,
reported last August in the journal
Nature, garnered wide publicity. ""The
importance is symbolic,"" says Hans
Moravec, principal research scientist at
the Robotics Institute at Carnegie Mellon
University in Pittsburgh. ""You have sys-
tems that develop robots out of thin air,
not by humans. In the future, there will
be real robots designed that way.""
Pollack's design and manufacturing
methods have plenty of competition.
Academic and industrial labs around the
world are busy building new generations of robots. Within this decade, experts
predict a steady evolution in commercial
utility robots: robots that can clean floors
and pick up things. ""There will be a mass
market for robots,"" suggests George
Bekey, founder of the robotics lab at the
University of Southern California in Los
Angeles. ""This next decade will be the
decade of the robot.""
Before robots reach out into the every-
day world of business and the household, though, they will need their own version
of Moores Law: becoming dramatically
more affordable and powerful over time.
In spite of intriguing experiments such as
Pollack's, designing even relatively simple
robots is a painstaking task. In Japan, for
example, Honda has spent over 14 years
building a humanoid robot able to walk,
open a door and navigate stairs.
A walk around Pollack's lab suggests,
perhaps, a better way to design robots.
On a workbench sits one example of his
computer-designed and computer-build-
able machines; it moves eerily like an
inchworm. Pollack trims excess plastic
from a newly fabricated plastic-rod
machine, oblivious to the shavings col-
lecting on his shirt and around his chair.
In a few years Pollack may well evolve a
cheap robot able to sweep those shavings
off the floor. 
— David Talbot


Others in Robot Design
Project, Organization
Sarcos (Salt Lake City, Utah), Robots for industry / medicine /Hollywood
IKobot (Somerville, Mass.), Household communications robot
Humanoid Interaction Lab, (Tsukuba, Japan), Interactions between humanoid robots and humans
MIT Artificial Intelligence Lab (Cambridge, Mass.),Machine learning / robot legs/ faces
Robotics Institute (Carnegie Melon), Mobile robots and face recognition",1.5% - 15%,Positive.,Hardware,project
Microfluidics,2001.0,https://wp.technologyreview.com/wp-content/uploads/2021/02/MIT-Technology-Review-2001-TR10.pdf,,"S T E P H E N Q U A K E
Microfluidics
Ih e forces of physics move oceans,
m o u n t a i n s and galaxies. But
applied physicist Stephen Quake
uses them to manipulate things on a
vastly reduced scale: tiny volumes of flu-
ids thousands of times smaller than a
dewdrop. Microfluidics, as Quake's field
is called, is a promising new branch of
biotechnology. The idea is that once you
master fluids at the microscale, you can
automate key experiments for genomics
and pharmaceutical development, per-
form instant diagnostic tests, even build
implantable drug-delivery devices-all
on mass-produced chips. It's a vision so
compelling that many industry observers
predict microfluidics will do for biotech
what the transistor did for electronics.
Quake's 11-person lab at Caltech is
not the only outfit bent on realizing this
vision. Over the past decade or so, scores
of researchers have set out to build
microscale devices for many of the basic
processes of biological research, from
sample mixing to DNA sequencing. But
many of those groups have run into road-
blocks in developing technology that can
be generalized to a broad range of applica-
tions and would allow several functions—
such as sample preparation, DNA extrac-
tion and detection of a gene mutation- to
be integratedon a single chip. Moreover,
some of the manufacturing approaches
involved, particularly silicon microma-
chining, are so expensive that experts in
the field question whether products rely-
ing on these techniques could ever be
economical to manufacture.
Quake's group is one of several now
working their way around these obstacles.
Last spring, the team unveiled a set of
microfabricated valves and pumps—a
critical first step in developing technol-
ogy general enough to work for any
microfluidics application. And to make
microfluidic devices cheaper, Quake and
others are casting them out of soft sili-
cone rubber in reusable molds, using a
technique called ""soft lithography."" The
potential payoff of these advances is huge:
mass-produced, disposable microfluidic
chips that make possible everything from
drug discovery on a massive scale to at-
h o m e tests for c o m m o n infections.
Because microfluidics is so promis-
ing and yet so technically frustrating,
expectation and hype have sometimes
outpaced the development of viable tech-
nology. Yet Quake and his group have
consistently turned out elegant devices
that actually work. First was a microscale
DNA analyzer that operates faster and on
different principles than the convention-
al, full-sized version, then a miniature
cell sorter and most recently, those valves
and pumps, described last April in the
journal Science. Al this while regularly
publishing important findings on the
basic physics of biological molecules.
If Quake seems adept at straddling
fields—in this case science and technol-
ogy-perhaps it's because that's exactly
the sort of challenge he has long craved.
Even as an undergraduate at Stanford
University, where he earned bachelor's
and master's degrees simultaneously in
only four years, Quake worried that phys-
ics was ""somewhat finished"" as an exper-
imental science, that it was hard to find
the field's frontiers. A pioneer at heart,
Quake started looking to tackle questions
that lay at the boundaries between disci-
plines. As he recalls: ""It was completely
obvious, even to an outsider, that biology
was going through this period of incred-
ible growth and intellectual excitement,
and there were going to be big questions
asked and answered, and the frontiers
were advancing at a tremendous rate in
all directions.""
After Quake finished his doctorate in
theoretical physics at Oxford University, he went back to Stanford as a fellow
working on the physics of DNA. When
Caltech's applied physics department
hired him in 1996, Quake says, ""it was an
experiment for them""-he was the first
faculty member in the department with
a biological bent. So far, the experiment
seems to be going smoothly; this past
summer, at only 31, Quake got tenure.
Quake's inventions are also thriving
in industry, through a startup called
Mycometrix. Founded in 1999 by Quake,
two of his college classmates and a con-
sultant, the South San Francisco-based
company has licensed all of Quake's
microfluidics patents from Caltech. When
TR went to press, the company was plan-
ning to deliver its first microfluidic devic-
es to selected university researchers and
industry partners by the end of 2000, and
was hoping for a commercial release by
the end of this year or early 2002. The
competition will be intense. Several start-
ups and even electronics giants like Hewl-
ett-Packard and Motorola are getting in
on the game. But to date, only one of
Mycometrix's competitors has brought a
microfluidic product to market.
Although Quake's work is rapidly
flowing into the commercial marketplace,
it's still the very early stages of science
and technology development that interest
him the most. And though he has built
quite a reputation as a technologist, he
hopes soon to focus more of his attention
on some of the most pressing questions
in basic biology: How do the proteins that
control gene expression work? How can
you do studies that cut across the entire
genome? ""Now that we've got some pret-
ty neat tools,"" Quake says, ""were going to
try and do some science with them.""
Quakes ability to work in areas from
basic research to hot commercial markets
make him a prototypical innovator. And
the same versatility makes microfluidics
a field to pay close attention to in the next
few years. 
— Rebecca Zacks

Others in Microfluidics
Organization, Project
Aclara BioSciences (Mountain View, Calif.), Genomics and drug screening
Caliper Technologies (Mountain View, Calif.), DNA / RNA and protein assays
Cepheid (Sunnyvale, Calif.), DNA analysis
Micronics (Redmond, Wash.), Diagnostics and chemical analysis
TECAN (Hombrechtikon, Switz.), Drug discovery",0.05% - 0.5%,Positive,Biotech,methodology
Wireless Sensor Networks,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Wireless Sensor Networks
Wireless Sensor Networks

Key players
Gaetano Borriello at U. Washington; Intel, Small embedded computers and communications protocols; Deborah Estrin at U. California, Los Angeles, Networking, middleware, data handling, and hardware for distributed sensors and actuators; Michael Horton at Crossbow Technology, Manufacture of sensors and motes; Kristofer Pister at U. California, Berkeley Millimeter-size sensing and communication devices


Great Duck Island, a 90-hectare expanse of rock and grass off the coast of Maine, is home to one of the world’s largest breeding colonies of Leach’s storm petrels-and to one of the world’s most advanced experiments in wireless networking. Last summer, researchers bugged dozens of the petrels’ nesting burrows with small monitoring devices called motes. Each is about the size of its power source-a pair of AA batteries-and is equipped with a processor, a tiny amount of computer memory, and sensors that monitor light, humidity, pressure, and heat. There’s also a radio transceiver just powerful enough to broadcast snippets of data to nearby motes and pass on information received from other neighbors, bucket brigadestyle.

This is more than the latest in avian intelligence gathering. The motes preview a future pervaded by networks of wireless battery-powered sensors that monitor our environment, our machines, and even us . It’s a future that David Culler, a computer scientist at the University of California, Berkeley, has been working toward for the last four years. “It’s one of the big opportunities” in information technology, says Culler. “Low-power wireless sensor networks are spearheading what the future of computing is going to look like.”

Culler is on partial leave from Berkeley to direct an Intel “lablet” that is perfecting the motes, as well as the hardware and software systems needed to clear the way for wireless networks made up of thousands or even millions of sensors. These networks will observe just about everything, including traffic, weather, seismic activity, the movements of troops on battlefields, and the stresses on buildings and bridges-all on a far finer scale than has been possible before.

Because such networks will be too distributed to have the sensors hard-wired into the electrical or communications grids, the lablet’s first challenge was to make its prototype motes communicate wirelessly with minimal battery power. “The devices have to organize themselves in a network by listening to one another and figuring out who can they hear…but it costs power to even listen,” says Culler. That meant finding a way to leave the motes’ radios off most of the time and still allow data to hop through the network, mote by mote, in much the same way that data on the Internet are broken into packets and routed from node to node.

Until Culler’s group attacked the problem, wireless networking had lacked an equivalent to the data-handling protocols that make the Internet work. The lablet’s solution: TinyOS, a compact operating system only a few kilobytes in size, that handles such administrative tasks as encoding data packets for relay and turning on radios only when they’re needed. The motes that run TinyOS should cost a few dollars apiece when mass produced and are being field-tested in several locations from Maine to California, where Berkeley seismologists are using them to monitor earthquakes.

Anyone is free to download and tinker with TinyOS, so researchers outside of Berkeley and Intel can test wireless sensor networks in a range of environments without having to reinvent the underlying technology. Culler’s motes have been “a tremendously enabling platform,” says Deborah Estrin, director of the Center for Embedded Networked Sensing at the University of California, Los Angeles. Estrin is rigging a nature reserve in the San Jacinto mountains with a dense array of wireless microclimate and imaging sensors.

Others are trying to make motes even smaller. A group led by Berkeley computer scientist Kristofer Pister is aiming for one cubic millimeter-the size of a few dust mites. At that scale, wireless sensors could permeate highway surfaces, building materials, fabrics, and perhaps even our bodies. The resulting data bonanza could vastly increase our understanding of our physical environment-and help us protect our own nests.

by Wade Roush",0.5% - 5.0%,Positive,Hardware,platform
Injectable Tissue Engineering,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Injectable Tissue Engineering
Injectable Tissue Engineering

Key players
Anthony Atala at Harvard Medical School Cartilage; Jim Burns at Genzyme Cartilage; Antonios Mikos at Rice U. Bone and cardiovascular tissue; David Mooney at U. Michigan Bone and cartilage


Every year, more than 700,000 patients in the United States undergo joint replacement surgery. The procedure-in which a knee or a hip is replaced with an artificial implant-is highly invasive, and many patients delay the surgery for as long as they can. Jennifer Elisseeff, a biomedical engineer at Johns Hopkins University, hopes to change that with a treatment that does away with surgery entirely: injectable tissue engineering. She and her colleagues have developed a way to inject joints with specially designed mixtures of polymers, cells, and growth stimulators that solidify and form healthy tissue. “We’re not just trying to improve the current therapy,” says Elisseeff. “We’re really trying to change it completely.”

Elisseeff is part of a growing movement that is pushing the bounds of tissue engineering-a field researchers have long hoped would produce lab-grown alternatives to transplanted organs and tissues. For the last three decades, researchers have focused on growing new tissues on polymer scaffolds in the lab. While this approach has had success producing small amounts of cartilage and skin, researchers have had difficulty keeping cells alive on larger scaffolds. And even if those problems could be worked out, surgeons would still have to implant the lab-grown tissues. Now, Elisseeff, as well as other academic and industry researchers, are turning to injectable systems that are less invasive and far cheaper. Many of the tissue-engineering applications to reach the market first could be delivered by syringe rather than implants, and Elisseeff is pushing to make this happen as soon as possible.

Elisseeff and her colleagues have used an injectable system to grow cartilage in mice. The researchers added cartilage cells to a light-sensitive liquid polymer and injected it under the skin on the backs of mice. They then shone ultraviolet light through the skin, causing the polymer to harden and encapsulate the cells. Over time, the cells multiplied and developed into cartilage. To test the feasibility of the technique for minimally invasive surgery, the researchers injected the liquid into the knee joints of cadavers. The surgeons used a fiber-optic tube to view the hardening process on a television monitor. “This has huge implications,” says James Wenz, an orthopedic surgeon at Johns Hopkins who is collaborating with Elisseeff.

While most research on injectable systems has focused on cartilage and bone, observers say this technology could be extended to tissues such as those of the liver and heart. The method could be used to replace diseased portions of an organ or to enhance its functioning, says Harvard University pediatric surgeon Anthony Atala. In the case of heart failure, instead of opening the chest and surgically implanting an engineered valve or muscle tissue, he says, simply injecting the right combination of cells and signals might do the trick.

For Elisseeff and the rest of the field, the next frontier lies in a powerful new tool: stem cells. Derived from sources like bone marrow and embryos, stem cells have the ability to differentiate into numerous types of cells. Elisseeff and her colleagues have exploited that ability to grow new cartilage and bone simultaneously-one of the trickiest feats in tissue engineering. They made layers of a polymer-and-stem-cell mixture, infusing each layer with specific chemical signals that triggered the cells to develop into either bone or cartilage. Such hybrid materials would simplify knee replacement surgeries, for instance, that require surgeons to replace the top of the shin bone and the cartilage above it.

Don’t expect tissue engineers to grow entire artificial organs anytime soon. Elisseeff, for one, is aiming for smaller advances that will make tissue engineering a reality within the decade. For the thousands of U.S. patients who need new joints every year, such small feats could be huge.

by Alexandra M. Goho",0.01% - 0.1%,Positive,Biotech,Methodology
Nano Solar Cells,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Nano Solar Cells
Nano Solar Cells

Key players
Richard Friend at U. Cambridge, Fullerene-polymer composite solar cells; Michael Grtzel at Swiss Federal Institute of Technology, Nanocrystalline dye-sensitized solar cells; Alan Heeger at U. California,Santa Barbara, Fullerene-polymer composite solar cells; N. Serdar Sariciftci at Johannes Kepler U., Polymer and fullerene-polymer composite solar cells

The sun may be the only energy source big enough to wean us off fossil fuels. But harnessing its energy depends on silicon wafers that must be produced by the same exacting process used to make computer chips. The expense of the silicon wafers raises solar-power costs to as much as 10 times the price of fossil fuel generation-keeping it an energy source best suited for satellites and other niche applications.

Paul Alivisatos, a chemist at the University of California, Berkeley, has a better idea: he aims to use nanotechnology to produce a photovoltaic material that can be spread like plastic wrap or paint. Not only could the nano solar cell be integrated with other building materials, it also offers the promise of cheap production costs that could finally make solar power a widely used electricity alternative.

Alivisatos’s approach begins with electrically conductive polymers. Other researchers have attempted to concoct solar cells from these plastic materials ( see ” Solar on the Cheap ,” TR January/ February 2002 ), but even the best of these devices aren’t nearly efficient enough at converting solar energy into electricity. To improve the efficiency, Alivisatos and his coworkers are adding a new ingredient to the polymer: nanorods, bar-shaped semiconducting inorganic crystals measuring just seven nanometers by 60 nanometers. The result is a cheap and flexible material that could provide the same kind of efficiency achieved with silicon solar cells. Indeed, Alivisatos hopes that within three years, Nanosys-a Palo Alto, CA, startup he cofounded-will roll out a nanorod solar cell that can produce energy with the efficiency of silicon-based systems.

The prototype solar cells he has made so far consist of sheets of a nanorod-polymer composite just 200 nanometers thick. Thin layers of an electrode sandwich the composite sheets. When sunlight hits the sheets, they absorb photons, exciting electrons in the polymer and the nanorods, which make up 90 percent of the composite. The result is a useful current that is carried away by the electrodes.

Early results have been encouraging. But several tricks now in the works could further boost performance. First, Alivisatos and his collaborators have switched to a new nanorod material, cadmium telluride, which absorbs more sunlight than cadmium selenide, the material they used initially. The scientists are also aligning the nanorods in branching assemblages that conduct electrons more efficiently than do randomly mixed nanorods. “It’s all a matter of processing,” Alivisatos explains, adding that he sees “no inherent reason” why the nano solar cells couldn’t eventually match the performance of top-end, expensive silicon solar cells.

The nanorod solar cells could be rolled out, ink-jet printed, or even painted onto surfaces, so “a billboard on a bus could be a solar collector,” says Nanosys’s director of business development, Stephen Empedocles. He predicts that cheaper materials could create a $10 billion annual market for solar cells, dwarfing the growing market for conventional silicon cells.

Alivisatos’s nanorods aren’t the only technology entrants chasing cheaper solar power. But whether or not his approach eventually revolutionizes solar power, he is bringing novel nanotechnology strategies to bear on the problem. And that alone could be a major contribution to the search for a better solar cell. “There will be other research groups with clever ideas and processes-maybe something we haven’t even thought of yet,” says Alivisatos. “New ideas and new materials have opened up a period of change. It’s a good idea to try many approaches and see what emerges.”

Thanks to nanotechnology, those new ideas and new materials could transform the solar cell market from a boutique source to the Wal-Mart of electricity production. 

by Eric Scigliano",0.001% - 0.01%,Positive,Nanotech,Product
Mechatronics,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Mechatronics

Key players
Lino Guzzella at Swiss Federal Institute of Technology, Engine modeling and control systems; Karl Hedrick and Masayoshi Tomizuka at U. California, Berkeley, Control systems and theory; Uwe Kiencke at U. Karlsruhe, Digital signal processing; Philip Koopman at Carnegie Mellon U., Fault tolerance in control software; Lars Nielsen at Linkping U., Engine control systems


To improve everything from fuel economy to performance, automotive researchers are turning to “mechatronics,” the integration of familiar mechanical systems with new electronic components and intelligent-software control. Take brakes. In the next five to 10 years, electromechanical actuators will replace hydraulic cylinders; wires will replace brake fluid lines; and software will mediate between the driver’s foot and the action that slows the car. And because lives will depend on such mechatronic systems, Rolf Isermann, an engineer at Darmstadt University of Technology in Darmstadt, Germany, is using software that can identify and correct for flaws in real time to make sure the technology functions impeccably. “There is a German word for it: grndlich ,” he says. “It means you do it really right.”

In order to do mechatronic braking right, Isermann’s group is developing software that tracks data from three sensors: one detects the flow of electrical current to the brake actuator; a second tracks the actuator’s position; and the third measures its clamping force. Isermann’s software analyzes those numbers to detect faults-such as an increase in friction-and flashes a dashboard warning light, so the driver can get the car serviced before the fault leads to failure.

“Everybody initially was worried about the safety of electronic devices. I think people are now becoming aware they are safer than mechanical ones,” says Karl Hedrick, a mechanical engineer at the University of California, Berkeley. “A large part of the reason they are safer is you can build in fault diagnoses and fault tolerance. Isermann is certainly in the forefront of people developing technology to do this.”

Isermann is also working to make engines run cleaner. He is developing software that detects combustion misfires, which can damage catalytic converters and add to pollution. Because it’s not practical to have a sensor inside a combustion chamber, Isermann’s system relies on data from sensors that measure oxygen levels in exhaust and track the speed of the crankshaft (the mechanism that delivers the engine’s force to the wheels). Tiny fluctuations in crankshaft speed accompanied by changes in emissions reveal misfires. If a misfire is detected, the software can warn the driver or, in the future, might automatically fix the problem.

Partnerships with manufacturing companies-including DaimlerChrysler and Continental Teves-merge the basic research from Isermann’s group with industry’s development of such technologies in actual cars. Isermann says that “80 to 90 percent of the innovations in the development of engines and cars these days are due to electronics and mechatronics.” Until recent years, mechatronic systems were found mainly in such big-ticket items as aircraft and industrial equipment or in small precision components for products such as cameras and photocopiers. But new applications in cars and trucks have helped prompt a surge in the number of groups working on mechatronics. The trend has been fueled by falling prices for microprocessors and sensors, more stringent vehicle-emissions regulations in Europe and California, and automakers’ wanting to enhance their vehicles with additional comfort and performance features.

Although the luxury market looms largest today-new high-end models from BMW contain more than 70 microprocessors that control more than 120 tiny motors-mechatronics will be moving into the wider car market within five years, says Lino Guzzella, codirector of the Institute of Measurement and Control at the Swiss Federal Institute of Technology. And with software like Isermann’s on board, the electronic guts of these new driving machines should be as sturdy and reliable as steel. 

by David Talbot",2-20%,Positive,Hardware,Methodology
Grid Computing,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Grid Computing
Grid Computing

Key players
Andrew Chien Entropia Peer-to-Peer Working Group Andrew Grimshaw Avaki; U. Virginia Commercial grid software Miron Livny U. Wisconsin, Madison Open-source system to harness idle workstations Steven Tuecke Argonne National Laboratory Globus Toolkit
In the 1980s “internetworking protocols” allowed us to link any two computers, and a vast network of networks called the Internet exploded around the globe. In the 1990s the “hypertext transfer protocol” allowed us to link any two documents, and a vast, online library-cum-shoppingmall called the World Wide Web exploded across the Internet. Now, fast emerging “grid protocols” might allow us to link almost anything else: databases, simulation and visualization tools, even the number-crunching power of the computers themselves. And we might soon find ourselves in the midst of the biggest explosion yet.

“We’re moving into a future in which the location of [computational] resources doesn’t really matter,” says Argonne National Laboratory’s Ian Foster. Foster and Carl Kesselman of the University of Southern California’s Information Sciences Institute pioneered this concept, which they call grid computing in analogy to the electric grid, and built a community to support it. Foster and Kesselman, along with Argonne’s Steven Tuecke, have led development of the Globus Toolkit, an open-source implementation of grid protocols that has become the de facto standard. Such protocols promise to give home and office machines the ability to reach into cyberspace, find resources wherever they may be, and assemble them on the fly into whatever applications are needed.

Imagine, says Kesselman, that you’re the head of an emergency response team that’s trying to deal with a major chemical spill. “You’ll probably want to know things like, What chemicals are involved? What’s the weather forecast, and how will that affect the pattern of dispersal? What’s the current traffic situation, and how will that affect the evacuation routes?” If you tried to find answers on today’s Internet, says Kesselman, you’d get bogged down in arcane log-in procedures and incompatible software. But with grid computing it would be easy: the grid protocols provide standard mechanisms for discovering, accessing, and invoking just about any online resource, simultaneously building in all the requisite safeguards for security and authentication.

Construction is under way on dozens of distributed grid computers around the world-virtually all of them employing Globus Toolkit. They’ll have unprecedented computing power and applications ranging from genetics to particle physics to earthquake engineering. The $88 million TeraGrid of the U.S. National Science Foundation will be one of the largest. When it’s completed later this year, the general-purpose, distributed supercomputer will be capable of some 21 trillion floating-point operations per second, making it one of the fastest computational systems on Earth. And grid computing is experiencing an upsurge of support from industry heavyweights such as IBM, Sun Microsystems, and Microsoft. IBM, which is a primary partner in the TeraGrid and several other grid projects, is beginning to market an enhanced commercial version of the Globus Toolkit.

Out of Foster and Kesselman’s work on protocols and standards, which began in 1995, “this entire grid movement emerged,” says Larry Smarr, director of the California Institute for Telecommunications and Information Technology. What’s more, Smarr and others say, Foster and Kesselman have been instrumental in building a community around grid computing and in advocating its integration with two related approaches: peer-to-peer computing, which brings to bear the power of idle desktop computers on big problems in the manner made famous by SETI@home, and Web services, in which access to far-flung computational resources is provided through enhancements to the Web’s hypertext protocol. By helping to merge these three powerful movements, Foster and Kesselman are bringing the grid revolution much closer to reality. And that could mean seamless and ubiquitous access to unfathomable computer power.

by M. Mitchell Waldrop",1.5% to 15.0%,Positive,Software,Platform
Molecular Imaging,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Molecular Imaging
Molecular Imaging

Key players
Ronald Blasberg at Memorial Sloan-Kettering Cancer Center, Imaging of gene expression; Harvey Herschman at U. California, Los Angeles, Tracking of gene therapy, gene activities; David Piwnica-Worms at Washington U., Protein interactions, imaging tools; Patricia Price at U. Manchester, Clinical oncology, imaging drug targets; Ralph Weissleder at Harvard Medical School, Cell tracking, molecular targets, drug discovery
At Massachusetts General Hospital’s Center for Molecular Imaging Research-a bustling facility nestled next to an old Navy shipyard-Umar Mahmood uses a digital camera to peer through the skin of a living mouse into a growing tumor. Using fluorescent tags and calibrated filters, the radiologist actually sees the effects of the cancer on a molecular scale: destructive enzymes secreted by the tumor show up on Mahmood’s computer screen as splotches of red, yellow, and green. In the future, he says, such “molecular imaging” may lead to earlier detection of human disease, as well as more effective therapies.

Molecular imaging-shorthand for a number of techniques that let researchers watch genes, proteins, and other molecules at work in the body-has exploded, thanks to advances in cell biology, biochemical agents, and computer analysis. Research groups around the world are joining the effort to use magnetic, nuclear, and optical imaging techniques to study the molecular interactions that underlie biological processes. Unlike x-ray, ultrasound, and other conventional techniques that give doctors only such anatomical clues as the size of a tumor, molecular imaging could help track the underlying causes of disease. The appearance of an unusual protein in a cluster of cells, say, might signal the onset of cancer. Mahmood is helping to lead the effort to put the technology into medical practice.

It is challenging, though, to detect a particular molecule in the midst of cellular activity. When researchers inject a tag that binds to the molecule, they face the problem of distinguishing the bound tags from the extra, unbound tags. So Mahmood has worked with chemists to develop “smart probes” that change their brightness or their magnetic properties when they meet their target. “This is a big deal,” says David Piwnica-Worms, director of the Molecular Imaging Center at Washington University in St. Louis. The method, he explains, “allows you to see selected proteins and enzymes that you might miss with standard tracer techniques.”

In a series of groundbreaking experiments, Mahmood’s team treated cancerous mice with a drug meant to block the production of an enzyme that promotes tumor growth. The researchers then injected fluorescent probes designed to light up in the presence of that enzyme. Under an optical scanner, treated tumors showed up as less fluorescent than untreated tumors, demonstrating the potential of molecular imaging to monitor treatments in real time-rather than waiting months to see whether a tumor shrinks. “The big goal is to select the optimum therapy for a patient and then to check that, say, a drug is hitting a particular receptor,” says John Hoffman, director of the Molecular Imaging Program at the National Cancer Institute. What’s more, molecular imaging could be used to detect cancer signals that precede anatomical changes by months or years, eliminating the need for surgeons to cut out a piece of tissue to make a diagnosis. “At the end of the day, we may replace a number of biopsies with imaging,” Mahmood says.

In Mahmood’s lab, clinical trials are under way for magnetic resonance imaging of blood vessel growth-an early indicator of tumor growth and other changes. For more advanced techniques such as those used in the mouse cancer study, clinical trials are two years away. The big picture: 10 years down the road, molecular imaging may take the place of mammograms, biopsies, and other diagnostic techniques. Although it won’t replace conventional imaging entirely, says Mahmood, molecular imaging will have a profound effect both on basic medical research and on high-end patient care. Indeed, as his work next door to the shipyard makes clear, an important new field of biotechnology has set sail.

by Gregory T. Huang",0.01% - 0.1%,Positive,Biotech,platform
Nanoimprint Lithography,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Nanoimprint Lithography
Nanoimprint Lithography

Key players
Yong Chen at Hewlett-Packard, High-density molecular electronic memory; John Rogers at Bell Labs, Patterning polymer electronics; George Whitesides at Harvard U., Contact printing on flexible substrates; Grant Willson at U. Texas, Molecular Imprints High-density microchip fabrication
A world of Lilliputian sensors, transistors, and lasers is in development at nanotechnology labs worldwide. These devices point to a future of ultrafast and cheap electronics and communications. But making nanotechnology relevant beyond the lab is difficult because of the lack of suitable manufacturing techniques. The tools used to mass-produce silicon microchips are far too blunt for nanofabrication, and specialized lab methods are far too expensive and time-consuming to be practical. “Right now everybody is talking about nanotechnology, but the commercialization of nanotechnology critically depends upon our ability to manufacture,” says Princeton University electrical engineer Stephen Chou.

A mechanism just slightly more sophisticated than a printing press could be the answer, Chou believes. Simply by stamping a hard mold into a soft material, he can faithfully imprint features smaller than 10 nanometers across. Last summer, in a dramatic demonstration of the potential of the technique, Chou showed that he could make nano features directly in silicon and metal. By flashing the solid with a powerful laser, he melted the surface just long enough to press in the mold and imprint the desired features.

Although Chou was not the first researcher to employ the imprinting technique, which some call soft lithography, his demonstrations have set the bar for nanofabrication, says John Rogers, a chemist at Lucent Technologies’ Bell Labs. “The kind of revolution that he has achieved is quite remarkable in terms of speed, area of patterning, and the smallest-size features that are possible. It’s leading edge,” says Rogers. Ultimately, nanoimprinting could become the method of choice for cheap and easy fabrication of nano features in such products as optical components for communications and gene chips for diagnostic screening. Indeed, NanoOpto, Chou’s startup in Somerset, NJ, is already shipping nanoimprinted optical-networking components. And Chou has fashioned gene chips that rely on nano channels imprinted in glass to straighten flowing DNA molecules, thereby speeding genetic tests.

Chou is also working to show that nanoimprinting can tackle lithography’s grand challenge: how to etch nano patterns into silicon for future generations of high-performance microchips. Chou says he can already squeeze at least 36 times as many transistors onto a silicon wafer as the most advanced commercial lithography tools. But to make complex chips, which have many layers, perfect alignment must be maintained through as many as 30 stamping steps. For Chou’s process, in which heat could distort the mold and the wafer, that means each round of heating and imprinting must be quick. With his recent laser-heating innovations, Chou has cut imprinting time from 10 seconds to less than a microsecond. As a result, he has demonstrated the ability to make basic multilayered chips, and he says complex processors and memory chips are next. Chou’s other startup, Nanonex in Princeton, NJ, is busy negotiating alliances with lithography tool manufacturers.

Chou’s results come at a time when the chipmaking industry has been spending billions of dollars developing exotic fabrication techniques that use everything from extreme ultraviolet light to electron beams. But, says Stanford University nanofabrication expert R. Fabian Pease, “If you look at what the extreme ultraviolet and the electron projection lithography techniques have actually accomplished, [imprint lithography], which has had a tiny fraction of the investment, is looking awfully good.” This is sweet vindication for Chou, who began working on nanofabrication in the 1980s, before most of his colleagues recognized that nano devices would be worth manufacturing. “Nobody questions the manufacturing ability of nanoimprint anymore,” says Chou. “Suddenly the doubt is gone.”

by Peter Fairley",0.05% - 0.5%,Positive,Nanotech,methodology
Software Assurance,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Software Assurance
Software Assurance

Key players
Gerard Holzmann at Bell Labs, Software to detect bugs in networked computers; Charles Howell at Mitre, Benchmarks for software assurance; Charles Simonyi at Intentional Software, Programming tools to improve software; Douglas Smith at Kestrel Institute, Mechanized software development
Computers crash. That’s a fact of life. And when they do, it’s usually because of a software bug. Generally, the consequences are minimal-a muttered curse and a reboot. But when the software is running complex distributed systems such as those that support air traffic control or medical equipment, a bug can be very expensive, and even cost lives. To help avoid such disasters, Nancy Lynch and Stephen Garland are creating tools they hope will yield nearly error-free software.

Working together at MIT’s Laboratory for Computer Science, Lynch and Garland have developed a computer language and programming tools for making software development more rigorous, or as Garland puts it, to “make software engineering more like an engineering discipline.” Civil engineers, Lynch points out, build and test a model of a bridge before anyone constructs the bridge itself. Programmers, however, often start with a goal and, perhaps after some discussion, simply sit down to write the software code. Lynch and Garland’s tools allow programmers to model, test, and reason about software before they write it. It’s an approach that’s unique among efforts launched recently by the likes of Microsoft, IBM, and Sun Microsystems to improve software quality and even to simplify and improve the programming process itself.

Like many of these other efforts, Lynch and Garland’s approach starts with a concept called abstraction. The idea is to begin with a high-level summary of the goals of the program and then write a series of progressively more specific statements that describe both steps the program can take to reach its goals and how it should perform those steps. For example, a high-level abstraction for an aircraft collision avoidance system might specify that corrective action take place whenever two planes are flying too close. A lower-level design might have the aircraft exchange messages to determine which should ascend and which should descend.

Lynch and Garland have taken the idea of abstraction further. A dozen years ago, Lynch developed a mathematical model that made it easier for programmers to tell if a set of abstractions would make a distributed system behave correctly. With this model, she and Garland created a computer language programmers can use to write “pseudocode” that describes what a program should do. With his students, Garland has also built tools to prove that lower levels of abstractions relate correctly to higher levels and to simulate a program’s behavior before it is translated into an actual programming language like Java. By directing programmers’ attention to many more possible bug-revealing circumstances than might be checked in typical software tests, the tools help assure that the software will always work properly. Once software has been thus tested, a human can easily translate the pseudocode into a standard programming language.

Not all computer scientists agree that it is possible to prove software error free. Still, says Shari Pfleeger, a computer scientist for Rand in Washington, DC, mathematical methods like Lynch and Garland’s have a place in software design. “Certainly using it for the most critical parts of a large system would be important, whether or not you believe you’re getting 100 percent of the problems out,” Pfleeger says.

While some groups have started working with Lynch and Garland’s software, the duo is pursuing a system for automatically generating Java programs from highly specified pseudocode. The aim, says Garland, is to “cut human interaction to near zero” and eliminate transcription errors. Collaborator Alex Shvartsman, a University of Connecticut computer scientist, says, “A tool like this will take us slowly but surely to a place where systems are much more dependable than they are today.” And whether we’re boarding planes or going to the hospital, we can all appreciate that goal.

by Erika Jonietz",0.5% - 1.5%,Positive,Software,Methodology
Glycomics,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Glycomics
Glycomics

Key players
Carolyn Bertozzi at U. California, Berkeley, Thios Pharmaceuticals Glycosylation and receptor binding in disease; Richard Cummings at U. Oklahoma, Sugars in cell adhesion; Stuart Kornfeld at Washington U. School of Medicine, Pathways of glycosylation and genetic disorders; John Lowe at U. Michigan, Sugars in immunity and cancer; Jamey Marth at U. California, San Diego, Abaron Biosciences Sugars in physiology and disease
James Paulson, a researcher at the Scripps Research Institute in La Jolla, CA, lifts a one-liter, orange-capped bottle from his desk. The bottle is filled with sugar, and Paulson estimates that, had the substance been purchased from a chemical supply house, it would have cost about $15 million. “If I could only sell it,” Paulson jokes, admiring what looks like the chunky, raw sugar served at health food restaurants.

In fact, Cytel, a biotech company Paulson once helped run, synthesized the sugar-one of thousands made by the human body-with hopes it could be sold to truly boost health. Cytel’s aim was to turn the sugar into a drug that could tame the immune system to minimize damage following heart attacks and surgery. That ambition failed, but the effort to understand and ultimately harness sugars-a field called glycomics-is thriving. And Paulson, who has gone on to cofound Abaron Biosciences in La Jolla, CA, is leading the way, developing new glycomic drugs that could have an impact on health problems ranging from rheumatoid arthritis to the spread of cancer cells.

The reason for the excitement around glycomics is that sugars have a vital, albeit often overlooked, function in the body. In particular, sugars play a critical role in stabilizing and determining the function of proteins through a process called glycosylation, in which sugar units are attached to other molecules including newly made proteins. “If you don’t have any glycosylation, you don’t have life,” says Paulson.

By manipulating glycosylation or sugars themselves, researchers hope to shut down disease processes, create new drugs, and improve existing ones. Biotech giant Amgen, for instance, made a more potent version of its best-selling drug (a protein called erythropoietin, which boosts red-blood-cell production) by attaching two extra sugars to the molecule. Other companies such as GlycoGenesys, Progenics Pharmaceuticals, and Oxford Glycoscience have glycomic drugs in human tests for ailments ranging from Gaucher’s disease to colorectal cancer. “The medical potential…is absolutely enormous,” says Abaron cofounder Jamey Marth, a geneticist at the University of California, San Diego.

Despite the importance of sugars, efforts to unravel their secrets long remained in the shadows of research into genes and proteins-in part because there is no simple “code” that determines sugars’ structures. But over the last few decades, researchers have slowly uncovered clues to sugars’ functions. In the late 1980s, Paulson and his team isolated a gene for one of the enzymes responsible for glycosylation. Since that watershed event, scientists have been piecing together an ever more detailed understanding of the ways sugars can in some instances ensure healthy functioning and in others make us susceptible to disease.

It’s a gargantuan task. Researchers estimate that as many as 40,000 genes make up each person, and each gene can code for several proteins. Sugars modify many of those proteins, and various cell types attach the same sugars in different ways, forming a variety of branching structures, each with a unique function. “It’s a nightmare” to figure all this out, says Paulson. “In order for the field to progress rapidly, we need to bring together the experts in the various subfields to think about the problems of bridging the technologies and beginning to move toward a true glycomics approach.” In an attempt do just that, Paulson heads the Consortium for Functional Glycomics. The group, comprising more than 40 academics from a number of disciplines, has a five-year $34 million grant from the National Institutes of Health.

Despite this large-scale effort and healthy dose of federal funding, however, Paulson stresses that the consortium cannot detail every sugar in the body. “We’re just taking a bite out of the apple.” But what a sweet, large apple it is. 

by Jon Cohen",0.05% - 0.5%,Positive,Biotech,Concept
Quantum Cryptography,2003.0,https://www.technologyreview.com/10-breakthrough-technologies/2003/,,"Quantum Cryptography
Quantum Cryptography

Key players
Nabil Amer at IBM, Quantum key exchange through optical fiber; Richard Hughes at Los Alamos National Laboratory, Ground-to-satellite optical communications; John Preskill at Caltech, Quantum information theory; John Rarity at QinetiQ, Through-air quantum-key transmission; Alexei Trifonov and Hoi-Kwong Lo at MagiQ Technologies, Quantum-cryptography hardware
The world runs on secrets. Governments, corporations, and individuals-to say nothing of Internet-based businesses-could scarcely function without secrecy. Nicolas Gisin of the University of Geneva is in the vanguard of a technological movement that could fortify the security of electronic communications. Gisin’s tool, called quantum cryptography, can transmit information in such a way that any effort to eavesdrop will be detectable.

The technology relies on quantum physics, which applies at atomic dimensions: any attempt to observe a quantum system inevitably alters it. After a decade of lab experiments, quantum cryptography is approaching feasibility. “We can now think about using it for practical purposes,” says Richard Hughes, a quantum cryptography pioneer at the Los Alamos National Laboratory in New Mexico. Gisin-a physicist and entrepreneur-is leading the charge to bring the technology to market.

The company that Gisin spun off from his University of Geneva laboratory in 2001, id Quantique, makes the first commercially available quantum-cryptography system, he says. The PC-size prototype system includes a random-number generator (essential for creating a decryption key) and devices that emit and detect the individual photons of light that make up the quantum signal.

Conventional cryptographers concentrate on developing strong digital locks to keep information from falling into the wrong hands. But even the strongest lock is useless if someone steals the key. With quantum cryptography, “you can be certain that the key is secure,” says Nabil Amer, manager of the physics of information group at IBM Research. Key transmission takes the form of photons whose direction of polarization varies randomly. The sender and the intended recipient compare polarizations, photon by photon. Any attempt to tap this signal alters the polarizations in a way that the sender and intended recipient can detect. They then transmit new keys until one gets through without disturbance.

Quantum cryptography is still ahead of its time. Nonquantum encryption schemes such as the public-key systems now commonly used in business have yet to be cracked. But the security of public-key systems relies on the inability of today’s computers to work fast enough to break the code. Ultimately, as computers get faster, this defense will wear thin. Public-key encryption, Gisin says, “may be good enough today, but someone, someday, will find a way to crack it. Only through quantum cryptography is there a guarantee that the coded messages sent today will remain secret forever.”

Gisin has no illusions about the challenges he faces. For one thing, quantum cryptography works only over the distance a light pulse can travel through the air or an optical fiber without a boost; the process of amplification destroys the quantum-encoded information. Gisin’s team holds the world’s distance record, having transmitted a quantum key over a 67-kilometer length of fiber connecting Geneva and Lausanne, Switzerland.

The work of Gisin and others could usher in a new epoch of quantum information technology. Ironically, it is in part the prospect that superfast quantum computers will someday supply fantastic code-breaking power that drives Gisin and others to perfect their method of sheltering secret information. In the coming decades, Gisin contends, “e-commerce and e-government will be possible only if quantum communication widely exists.” Much of the technological future, in other words, depends on the science of secrecy.

by Herb Brody",0.01% - 0.1%,Positive,Hardware,methodology
Universal Translation,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Universal Translation
Yuqing Gao is bilingual-and so is her computer. At IBM’s Watson Research Center in Yorktown Heights, NY, the computer scientist, role-playing a doctor, speaks Mandarin Chinese into a personal digital assistant. In a few seconds, a pleasant female voice emanating from the device asks, in English, “What are your symptoms?” Gao’s system, designed to help doctors communicate with patients, can be extended to other languages and situations. The ultimate goal, she says, is to develop “universal translation” software that gleans meaning from phrases in one language and conveys it in any other language, enabling people from different cultures to communicate.

Gao’s work is at the forefront of escalating efforts to use mathematical models and natural-language-processing techniques to make computerized translation more accurate and efficient, and more adaptable to new languages. Distinct from speech recognition and synthesis, the technology behind universal translation has matured in recent years, driven in part by global business and security needs. “Advances in automatic learning, computing power, and available data for translation are greater than we’ve seen in the history of computer science,” says Alex Waibel, associate director of Carnegie Mellon University’s Language Technologies Institute, which supports several parallel efforts in the field.

by MIT Technology Review Editors",0.05% to 0.5%,Positive.,Software,Concept
Synthetic Biology,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Synthetic Biology
Perched on the gently sloping hills of Princeton University’s brick and ivy campus, Ron Weiss’s biology laboratory is stocked with the usual array of microscopes, pipettes, and petri dishes. Less typical is its location: crammed into the Engineering Quadrangle, it stands out among the electrical and mechanical engineering labs. Yet it’s an appropriate spot for Weiss. A computer engineer by training, he discovered the allure of biology during graduate school-when he began programming cells instead of computers. In fact, he began to program cells as if they were computers.

Weiss is one of just a handful of researchers delving into the inchoate field of synthetic biology, assiduously assembling genes into networks designed to direct cells to perform almost any task their programmers conceive. Combined with simple bacteria, these networks could advance biosensing, allowing inspectors to pinpoint land mines or biological weapons; add human cells, and researchers might build entire organs for transplantation. “We want to create a set of biological components, DNA cassettes that are as easy to snap together, and as likely to function, as a set of Legos,” says Tom Knight, an MIT computer-engineer-cum-biologist, and the graduate advisor who turned Weiss on to the idea.",0.1% - 1.0%,Positive,Biotech,concept
Nanowires,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Nanowires
Few emerging technologies have offered as much promise as nanotechnology, touted as the means of keeping the decades-long electronics shrinkfest in full sprint and transfiguring disciplines from power production to medical diagnostics. Companies from Samsung Electronics to Wilson Sporting Goods have invested in nanotech, and nearly every major university boasts a nanotechnology initiative. Red hot, even within this R&D frenzy, are the researchers learning to make the nanoscale wires that could be key elements in many working nanodevices.

“This effort is critical for the success of the whole [enterprise of] nanoscale science and technology,” says nanowire pioneer Peidong Yang of the University of California, Berkeley. Yang has made exceptional progress in fine-tuning the properties of nanowires. Compared to other nanostructures, “nanowires will be much more versatile, because we can achieve so many different properties just by varying the composition,” says Charles Lieber, a Harvard University chemist who has also been propelling nanowire development.

by MIT Technology Review Editors",0.01% - 0.1%,Positive,nanotech,concept
Bayesian Machine Learning,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Bayesian Machine Learning
When a computer scientist publishes genetics papers, you might think it would raise colleagues’ eyebrows. But Daphne Koller’s research using a once obscure branch of probability theory called Bayesian statistics is generating more excitement than skepticism. The Stanford University associate professor is creating programs that, while tackling questions such as how genes function, are also illuminating deeper truths about the long-standing computer science conundrum of uncertainty-learning patterns, finding causal relationships, and making predictions based on inevitably incomplete knowledge of the real world. Such methods promise to advance the fields of foreign-language translation, microchip manufacturing, and drug discovery, among others, sparking a surge of interest from Intel, Microsoft, Google, and other leading companies and universities.

How does an idea conceived by an 18th-century minister (Thomas Bayes) help modern computer science? Unlike older approaches to machine reasoning, in which each causal connection (“rain makes grass wet”) had to be explicitly taught, programs based on probabilistic approaches like Bayesian math can take a large body of data (“it’s raining,” “the grass is wet”) and deduce likely relationships, or “dependencies,” on their own. That’s crucial because many decisions programmers would like to automate-say, personalizing search engine results according to a user’s past queries-can’t be planned in advance; they require machines to weigh unforeseen combinations of evidence and make their best guesses. Says Intel research director David Tennenhouse, “These techniques are going to impact everything we do with computers-from user interfaces to sensor data processing to data mining.”

by MIT Technology Review Editors",1-10%,Positive,Software,methodology
T-Rays,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"T-Rays
With the human eye responsive to only a narrow slice of the electromagnetic spectrum, people have long sought ways to see beyond the limits of visible light. X-rays illuminate the ghostly shadows of bones, ultraviolet light makes certain chemicals shine, and near-infrared radiation provides night vision. Now researchers are working to open a new part of the spectrum: terahertz radiation, or t-rays. Able to easily penetrate many common materials without the medical risks of x-rays, t-rays promise to transform fields like airport security and medical imaging, revealing not only the shape but also the composition of hidden objects, from explosives to cancers.

In the late 1990s, Don Arnone and his group at Toshiba’s research labs in Cambridge, England, were eyeing t-rays as an alternative to dental x-rays. The idea was that t-rays, operating in the deep-infrared region just before wavelengths stretch into microwaves, would be able to spot decay without harmful ionizing radiation. In tests, the researchers fired powerful but extremely short pulses of laser light at a semiconductor chip, producing terahertz radiation (so called because it has frequencies of trillions of waves per second). Passing through gaps or different thicknesses of material changes the rays’ flight time, so by measuring how long each t-ray took to pass through an extracted tooth and reach a detector, the researchers were able to assemble a 3-D picture of the tooth.

by MIT Technology Review Editors",0.01% - 0.1%,Positive,Hardware,methodology
Distributed Storage,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Distributed Storage
Whether it’s organizing documents, spreadsheets, music, photos, and videos or maintaining regular backup files in case of theft or a crash, taking care of data is one of the biggest hassles facing any computer user. Wouldn’t it be better to store data in the nooks and crannies of the Internet, a few keystrokes away from any computer, anywhere? A budding technology known as distributed storage could do just that, transforming data storage for individuals and companies by making digital files easier to maintain and access while eliminating the threat of catastrophes that obliterate information, from blackouts to hard-drive failures.

Hari Balakrishnan is pursuing this dream, working to free important data from dependency on specific computers or systems. Music-sharing services such as KaZaA, which let people download and trade songs from Internet-connected PCs, are basic distributed-storage systems. But Balakrishnan, an MIT computer scientist, is part of a coalition of programmers who want to extend the concept to all types of data. The beauty of such a system, he says, is that it would provide all-purpose protection and convenience without being complicated to use. “You can now move [files] across machines,” he says. “You can replicate them, remove them, and the way in which [you] get them is unchanged.” With inability to access data sometimes costing companies millions in revenue per hour of downtime, according to Stamford, CT-based Meta Group, a distributed-storage system could dramatically enhance productivity.

by MIT Technology Review Editors",2-20%,Positive,Hardware,Platform
RNAi Therapy,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"RNAi Therapy
From heart disease to hepatitis, cancer to AIDS, a host of modern ailments are triggered by our own errant genes-or by those of invading organisms. So if a simple technique could be found for turning off specific genes at will, these diseases could-in theory-be arrested or cured. Biochemist Thomas Tuschl may have found just such an off switch in humans: RNA interference (RNAi). While working at Germany’s Max Planck Institute for Biophysical Chemistry, Tuschl discovered that tiny double-stranded molecules of RNA designed to target a certain gene can, when introduced into human cells, specifically block that gene’s effects.

Tuschl, now at Rockefeller University in New York City, first presented his findings at a meeting in Tokyo in May 2001. His audience was filled with doubters who remembered other much hyped RNA techniques that ultimately didn’t work very well. “They were very skeptical and very critical,” recalls Tuschl. What the skeptics didn’t realize was that RNAi is much more potent and reliable than earlier methods. “It worked the first time we did the experiment,” Tuschl recalls. Within a year, the doubts had vanished, and now the technique has universal acceptance-spawning research at every major drug company and university and likely putting Tuschl on the short list for a Nobel Prize.

by MIT Technology Review Editors",0.01-0.1%,Positive,Biotech,methodology
Power Grid Control,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Power Grid Control
Power grids carry the seeds of their own destruction: massive flows of electricity that can race out of control in just seconds, threatening to melt the very lines that carry them. Built in the days before quick-reacting microprocessors and fiber optics, these networks were never designed to detect and squelch systemwide disturbances. Instead, each transmission line and power plant must fend for itself, shutting down when power flows spike or sag. The shortcomings of this system are all too familiar to the 50 million North Americans from Michigan to Ontario whose lights went out last August: as individual components sense trouble and shut down, the remaining power flows become even more disturbed, and neighboring lines and plants fall like multimillion-dollar dominoes. Often-needless shutdowns result, costing billions, and the problem is only expected to get worse as expanding economies push more power onto grids.

Christian Rehtanz thinks the time has come for modern control technology to take back the grid. Rehtanz, group assistant vice president for power systems technology with Zrich, Switzerland-based engineering giant ABB, is one of a growing number of researchers seeking to build new smarts into grid control rooms. These engineers are developing hardware and software to track electric flows across continent-wide grids several times a second, identify disturbances, and take immediate action. While such “wide area” control systems remain largely theoretical, Rehtanz and his ABB colleagues have fashioned one that is ready for installation today. If their design works as advertised, it will make power outages 100 times less likely, protecting grids against everything from consumption-inducing heat waves to terrorism. “We can push more power through the grid while, at the same time, making the system more predictable and more reliable,” says Rehtanz.

by MIT Technology Review Editors",2-20%,Positive,Hardware,product
Microfluidic Optical Fibers,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Microfluidic Optical Fibers
The blazing-fast Internet access of the future-imagine downloading movies in seconds-might just depend on a little plumbing in the network. Tiny droplets of fluid inside fiber-optic channels could improve the flow of data-carrying photons, speeding transmission and improving reliability. Realizing this radical idea is the goal of University of Illinois physicist John Rogers, whose prototype devices, called microfluidic optical fibers, may be the key to superfast delivery of everything from e-mail to Web-based computer programs, once “bandwidth” again becomes the mantra.

Rogers began exploring fluid-filled fibers more than two years ago as a researcher at Lucent Technologies’ Bell Labs. While the optical fibers that carry today’s phone and data transmissions consist of glass tubing that is flexible but solid, Rogers employs fibers bored through with microscopic channels, ranging from one to 300 micrometers in diameter, depending on their use. While Rogers didn’t invent the fibers, he and his team showed that pumping tiny amounts of various fluids into them-and then controlling the expansion, contraction, and movement of these liquid “plugs”-causes the optical properties of the fibers to change. Structures such as tiny heating coils printed directly on the fiber precisely control the size, shape, and position of the plugs. Modifying the plugs’ properties enables them to perform critical functions, such as correcting error-causing distortions and directing data flows more efficiently, thus boosting bandwidth far more cheaply than is possible today.

by MIT Technology Review Editors",0.05-0.5%,Positive,Nanotech,platform
Personal Genomics,2004.0,https://www.technologyreview.com/10-breakthrough-technologies/2004/,,"Personal Genomics
Three billion. That’s the approximate number of DNA “letters” in each person’s genome. The Human Genome Project managed a complete, letter-by-letter sequence of a model human-a boon for research. But examining the specific genetic material of each patient in a doctor’s office by wading through those three billion letters just isn’t practical. So to achieve the dream of personalized medicine-a future in which a simple blood test will determine the best course of treatment based on a patient’s genes-many scientists are taking a shortcut: focusing on only the differences between people’s genomes.

David Cox, chief scientific officer of Perlegen Sciences in Mountain View, CA, is turning that strategy into a practical tool that will enable doctors and drug researchers to quickly determine whether a patient’s genetic makeup results in greater vulnerability to a particular disease, or makes him or her a suitable candidate for a specific drug. Such tests could eventually revolutionize the treatment of cancer, Alzheimer’s, asthma-almost any disease imaginable. And Cox, working with some of the world’s leading pharmaceutical companies, has gotten an aggressive head start in making it happen.

by MIT Technology Review Editors",0.01% - 0.1%,Positive,Biotech,product
Airborne Networks,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Airborne Networks
AVIATION An Internet in the sky could let planes fly safely without ground controllers.
Of the numerous technologies now in gestation at companies and universities, we have chosen 10 that we think will make particularly big splashes. They’re raw, but they’ll transform the Internet, computing, medicine, energy, nanotechnology, and more.

The technology that underpins the air traffic control system hasn’t changed much in a half-century. Planes still depend on elaborate ground-based radar systems, plus thousands of people who watch blips on screens and issue verbal instructions, for takeoffs, landings, and course changes. The system is expensive, hard to scale up, and prone to delays when storms strike.

An entirely different approach is possible. Each plane could continually transmit its identity, precise location, speed, and heading to other planes in the sky via an airborne network. Software would then take over, coördinating the system by issuing instructions to pilots on how to stay separated, optimize routes, avoid bad weather, and execute precise landings in poor visibility.

In the near term, such technology could save travelers time and might reduce fuel consumption. Long term, it could revolutionize air travel by enabling more planes to fill the sky without the addition of infrastructure and staff. Vastly greater numbers of small planes could zip in and out of thousands of small airfields (there are 5,400 in the U.S. alone), even those with no radar at all. “The biggest holdback to the number of airplanes that can be in the sky is that air traffic controllers are separating aircraft by hand,” says Sally Johnson, an aerospace engineer at NASA’s Langley Research Center. “Until you get away from that paradigm, we are at the limits of what you can do.”

As a practical matter, airborne networks that rely on software and cockpit computers rather than humans to issue instructions are still decades away. But in June, NASA plans to demonstrate a prototype of such an automated system at a small airport in Danville, VA. A computer at a ground station near the airport will receive data from multiple planes and give the pilots their initial holding fixes, then tell them what planes they’re following and where to go if they miss their approaches. In the planes, cockpit displays will show pilots where the other planes are, and a computer will give them instructions that guide their trajectories.

Future systems might go further: planes would communicate not just via a computer on the ground (or via satellite) but directly with each other, relaying information from other planes in an Internet-like fashion. This radical advance in airborne networking could come from research funded by the Pentagon – the midwife of today’s terrestrial Internet. The vision is that not only navigational data but information about targets, real-time intelligence, and bombing results would flow freely among manned and unmanned military planes, to vehicles on the ground, and up and down chains of command. “There is a terrestrial backbone of hardwired connections, and there will be a space backbone between satellites. What we are talking about adding, for aircraft, is an equivalent third backbone in the sky,” says Dave Kenyon, division chief of the Technical Architectures Division at the U.S. Air Force Electronic Systems Center in Bedford, MA.

The U.S. Air Force is beginning to define the architecture of an airborne network and hopes to begin actively developing and testing the network itself between 2008 and 2012, Kenyon says. Taken together, the military research and the related air traffic control research into airborne communications networks could change how we travel in the decades to come.

by David Talbot",0.05% - 0.5%,Positive,Hardware,platform
Quantum Wires,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Quantum Wires
POWER TRANSMISSION Wires spun from carbon nanotubes could carry electricity farther and more efficiently.
Richard Smalley toys with a clear plastic tube that holds a thin, dark gray fiber. About 15 centimeters long, the fiber comprises billions of carbon nanotubes, and according to the Rice University chemist, it represents the first step toward a new type of wire that could transform the electrical power grid.

Smalley’s lab has embarked on a four-year project to create a prototype of a nanotube-based “quantum wire.” Cables made from quantum wires should conduct much better than copper. The wires’ lighter weight and greater strength would also allow existing towers to carry fatter cables with a capacity ten times that of the heavy and inefficient steel-reinforced aluminum cables used in today’s aging power grid.

The goal is to make a wire with so little electrical resistance that it does not dissipate electricity as heat. Smalley says quantum wires could perform at least as well as existing superconductors – without the need for expensive cooling equipment. The reason: on the nanometer scale, the weird properties of quantum physics take over, and a wire can carry current without resistance. But until a couple of years ago, no one knew whether this amazing property would hold up when nanotubes were assembled into a macroscopic system. Then Jianping Lu, a physicist at the University of North Carolina at Chapel Hill, calculated that electrons could travel down a wire of perfectly aligned, overlapping carbon nanotubes with almost no loss of energy.

Smalley’s group has already produced 100-meter-long fibers consisting of well-aligned nanotubes. But the fibers are mixtures of 150 different types of nanotubes, which limits their conductivity. The best wire would consist of just one kind of nanotube – ideally the so-called 5,5-armchair nanotube, named for the arrangement of its carbon atoms. Existing production techniques generate multiple types of nanotubes, indiscriminately. But Smalley believes that adding tiny bits of a single carbon nanotube at the beginning of the process could catalyze the production of huge numbers of identical nanotubes – in essence, “cloning” the original tube.

by Erika Jonietz",0.001-0.01%,Positive,Nanotech,project
Silicon Photonics,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Silicon Photonics
OPTOELECTRONICS Making the material of computer chips emit light could speed data flow.
The Internet lives on beams of light. One hair-thin glass fiber can carry as much data as thousands of copper wires. But inside your computer, copper still rules. The advantages of light haven’t translated from long-distance connections on the Internet to the short jump between computer chips, in part because the lasers used in optical communications are made from exotic semiconductors incompatible with the standard processes for making silicon computer chips. As computers get faster and faster, they’re nearing the physical limit of copper’s ability to carry more information, and they’ll need something like the fiber-­optic network in order to keep improving at the rate we’ve come to expect.

Getting silicon to emit light could be the solution. A light signal’s frequency is much higher than an electrical signal’s, so it can carry thousands of times as much information. Light also overcomes another problem with electrical signals; as transistors get closer together, the electrical signals passing through them start to interfere with each other, like radio stations broadcasting at the same frequency. But turning silicon into a light emitter has proved an extraordinarily difficult challenge. The problem is rooted in an energy-level mismatch between silicon’s electrons and its positively charged “holes” (electron vacancies in its crystal structure): when an electron meets a hole, it’s more likely to release its excess energy as vibration than as light.

But last fall, a team at the University of California, Los Angeles, became the first to make a laser out of silicon. In February, ­Intel scientists upped the ante, reporting a silicon laser that put out a continuous instead of a pulsed beam, a necessity for data communications. “Once you identify the right piece of physics, everything falls into place,” says UCLA electrical-engineering professor Bahram Jalali, who made the first silicon laser.

The right piece of physics is the Raman effect. Some photons of light that pass through a material pick up energy from the natu­ral vibration of its atoms and change to another frequency. Jalali fires light from a nonsilicon laser into silicon. Because of the ­Raman effect, the photons emerge as a ­laser beam at a different frequency. This Raman laser is “a fundamental scientific breakthrough,” says Mario Paniccia, director of Intel’s Photonics Technology Lab, which is working to create the devices needed for optical communications in silicon. In addition to building a laser, he and his colleagues created a silicon modulator, which allows them to encode data onto a light beam by making it stronger or weaker. Paniccia’s group is working to more than double the speed at which it can modulate a beam. A multibillion-­dollar infrastructure is already in place for making silicon chips, so Intel believes silicon lasers will be a cost-effective way to raise the computing speed limit.

Photonics-based interconnects between chips should start to appear in about five years, researchers say. The ultimate goal is to enable light-wave communication between components on the same chip, which is several years further out. Philippe Fauchet, professor of optics at the University of Rochester, believes on-chip optical communications will require a silicon laser powered by electricity, which would be cheaper and less complicated than one that depends on an external laser. If such a laser can be built, it will mean that everything from supercomputers on opposite sides of the globe down to the tiniest transistors can talk to each other at the speed of light.

by Neil Savage",0.1% - 1.0%,Positive,Hardware,Platform
Metabolomics,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Metabolomics
MEDICINE A new diagnostic tool could mean spotting diseases earlier and more easily.
In their quest to develop more-accurate medical diagnostic tests, researchers are turning to a new field called metabolomics – the analysis of the thousands of small molecules such as sugars and fats that are the products of metabolism. If metabolomic information can be translated into diagnostic tests, it could provide earlier, faster, and more accurate diagnoses for many diseases.

Doctors have been measuring a few metabolites for decades to tell what’s wrong with patients; glucose for diabetes is a familiar example. Metabolomics researchers, however, sort through hundreds of molecules to tease out a dozen or so that can serve as the signature of a particular disease. “We’re hoping that many diseases will have metabolic fingerprints that we can measure,” says Maren Laughlin, codirector of a new National Institutes of Health (NIH) metabolomics initiative. Initially, metabolic researchers are hunting for the signatures of conditions such as autism and Huntington’s disease.

Metabolomics is, in some ways, a natural offshoot of recent advances in genomics and proteomics, which have allowed researchers to begin to identify many of the genes and proteins involved in diseases. Now researchers are realizing that they need to study metabolites in the same systematic fashion to get a complete picture of the body’s processes. And new software and increasingly powerful computers are helping them do it.

A few small companies aim to have their metabolite-based diagnostic tests on the market within several years. Metabolon of Research Triangle Park, NC, for ­example, is working with Massachusetts General Hospital to look for metabolic markers for amyotrophic lateral sclerosis (ALS), or Lou Gehrig’s disease, for which there’s no definitive blood test. To determine ALS’s biochemical profile, the researchers analyzed more than 1,000 molecules in patient blood samples. Using new software to sift through the mountains of data, they found 13 chemicals that showed up consistently at high levels in ALS patients. If larger human trials confirm this 13-chemical profile to be an accurate ALS indicator, it could form the basis of a quick and easy blood test for the deadly disease. ­Another company, Phenomenome Discoveries of Saskatoon, Saskatchewan, is developing metabolite-based diagnostics for Alzheimer’s disease and bipolar disorder.

There are drawbacks to using metabolites as disease markers. Their concentrations tend to fluctuate, since they’re heavily influenced by diet; doctors will therefore need to make sure samples are taken from patients under the proper conditions. But that’s true of many existing diagnostic tests, says Arthur Castle, the other codirector of the NIH metabolomics initiative. Metabolites may also prove not to be the best markers for every disease; in some cases, analysis of proteins may give a more reliable diagnosis. But metabolomics will give researchers a more comprehensive look at the complex changes under way in hundreds of molecules as a disease begins to develop – which can’t help but add to our store of medical knowledge.

by Corie Lok",0.01% - 0.1%,Positive,Biotech,methodology
Magnetic-Resonance Force Microscopy,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Magnetic-Resonance Force Microscopy
IMAGING The promise is a 3-D view of the molecular world.
In nanotechnology and molecular biology, researchers are often severely limited by the inability to observe atoms and molecules in three dimensions. Proteins, for instance, fold into complex patterns that are largely invisible to the biologists trying to work out their functions of the biomolecules.

So researchers are working to develop a tool that could provide a 3-D view of the nanoworld. The technology – called magnetic-resonance force microscopy (MRFM) – is a hybrid of magnetic-resonance imaging (MRI) and atomic force microscopy (AFM), which is widely used in nanotech. Physicists at the IBM Almaden Research Center in San Jose, CA, led by Daniel Rugar, recently used MRFM to detect the faint magnetic signal – the “spin” – of a single electron. While that accomplishment is still far from the goal of a 3-D snapshot of an atom or molecule, it is a critical step in proving that MRFM could perform atomic-scale imaging. MRFM works by dangling a tiny magnetic tip from the end of an ultrasensitive cantilever that bends in response to even an exceedingly small force. Under just the right conditions, the magnetic force between the tip and an electron changes the vibrations of the cantilever in a measurable way. Scanning a molecule in a 3-D raster pattern could, in theory, generate an image.

By helping pharmaceutical researchers more directly work out the structures of proteins, MRFM could provide invaluable clues toward the development of safer and more effective drugs. The standard technique for determining the complex three-­dimensional structure of proteins involves crystallizing them and then analyzing the diffraction pattern of x-rays that bounce off atoms in the crystal. But not all proteins crystallize, and puzzling out x-ray diffraction patterns is painstaking and tricky.

Researchers at IBM developed the scanning tunneling microscope, which provides images of atoms, and coinvented AFM, which has become a standard tool for atomic-scale manipulation, making possible much of nanotechnology. Whether MRFM will have the same impact is uncertain. But IBM’s experimental result is an encouraging signal for those desperate for a clearer, fuller view of the atomic and molecular world.

by David Rotman",0.01% - 0.1%,Positive,Nanotech,methodology
Universal Memory,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Universal Memory
NANOELECTRONICS Nanotubes make possible ultradense data storage.
Nantero CEO Greg Schmergel holds a circular wafer of silicon, about the size of a compact disc, sealed in an acrylic container. It’s a piece of hardware that stores 10 billion bits of ­digi­tal information, but what’s remarkable about it is the way it does it. Each bit is encoded not by the electric charge on a circuit element, as in conventional electronic ­memory, nor by the direction of a magnetic field, as in hard drives, but by the physical orientation of nanoscale structures. This technology could eventually allow vastly greater amounts of data to be stored on computers and mobile devices. Experts estimate that within 20 years, you may be able to fit the content of all the DVDs ever made on your laptop computer or store a digital file containing every conversation you have ever had on a handheld device.

Nantero’s approach is part of a broader effort to develop “universal memory” – next-generation memory systems that are ultradense and low power and could replace everything from the flash memory in digital cameras to hard drives. Nantero’s technology is based on research that the Woburn, MA, company’s chief sci­entist, Thomas Rueckes, did as a graduate student at Harvard University. Rueckes noted that no existing memory technologies seemed likely to prove adequate in the long run. Static and dynamic random-access memory (RAM), used in laptops and PCs, are fast but require too much space and power; flash memory is dense and nonvolatile – it doesn’t need power to hold data – but is too slow for computers. “We were thinking of a memory that combines all the advantages,” says Rueckes.

The solution: a memory each of whose cells is made of carbon nanotubes, each less than one-ten-thousandth the width of a human hair and suspended a few nanometers above an electrode. This default position, with no electric current flow between the nanotubes and the electrode, represents a digital 0. When a small voltage is applied to the cell, the nanotubes sag in the middle, touch the electrode, and complete a circuit – storing a digital 1. The nanotubes stay where they are even when the voltage is switched off. That could mean instant-on PCs and possibly the end of flash memory; the technology’s high storage density would also bring much larger memory capacities to mobile devices. Nantero claims that the ultimate refinement of the technology, where each nanotube encodes one bit, would enable storage of trillions of bits per square centimeter – thousands of times denser than what is possible today. (By comparison, a typi­cal DVD holds less than 50 billion bits total.) The company is not yet close to that limit, however; its prototypes store only about 100 million bits per square centimeter.

Nantero has partnered with chip makers such as Milpitas, CA-based LSI Logic to integrate its nanotube memory with sili­con circuitry. The memory sits on top of a layer of conventional transistors that read and write data, and the nanotubes are processed so that they don’t contaminate the accessing circuits. By late 2006, Schmergel predicts, Nantero’s partners should have produced samples of nanotube memory chips. Early applications may come in laptops and PDAs. Ultimately, however, the goal is to replace all memory and disk storage in all computers.

Suspending nanotubes is not the only way to build a universal memory. Other strategies include magnetic random-access memory, which Motorola and IBM and are pursuing, and molecular memory, where Hewlett-Packard is a research leader. But industry experts are watching Nantero’s progress with cautious optimism. “They have a very good approach, and it’s further along than any other,” says Ahmed Busnaina, professor of electrical engineering at Northeastern University and director of the National Science Foundation-funded Center for High-Rate Nanomanufacturing. If successful, this new kind of memory could put a world of data at your fingertips instantly, wherever you go.

by Gregory T. Huang",1-10%,Positive,Hardware,Product
Bacterial Factories,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Bacterial Factories
PHARMACEUTICALS Overhauling a microbe’s metabolism could yield a cheap malaria drug.
In the valleys of central China, a fernlike weed called sweet wormwood grows in fields formerly dedicated to corn. The plant is the only source of artemisinin, a drug that is nearly 100 percent effective against malaria. But even with more farmers planting the crop, demand for artemisinin exceeds supply, driving its cost out of reach for many of the 500 million afflicted with malaria every year. University of California, Berkeley, bioengineer Jay Keasling aims to solve the supply problem – and reduce the cost of treatment to less than 25 cents – by mass-producing the compound in specially engineered bacteria.

Keasling’s efforts are an example of metabolic engineering, a field in which researchers try to optimize the complex processes whereby a cell produces or breaks down a particular substance. These processes rely on the step-by-step direction of genes; changing even one gene can alter the outcome. Most metabolic engineering has previously focused on modifying a cell’s natural processes by inserting, mutating, or deleting a few key genes. According to James Collins, a biological engineer at Boston University, “what Jay is doing is a bit more radical”: creating entirely new metabolic pathways by integrating multiple genes from different organisms into a host microbe.

Keasling began his artemisinin project by inserting a set of yeast genes into the common bacterium E. coli. These genes induce the bacterium to make the chemical precursor to terpenes – the family of compounds to which artemisinin belongs. Adding in another two genes causes the bacterium to make a specific artemisinin precursor. Introducing a few more genes from sweet wormwood should get the microbe to make artemisinic acid, which is one simple chemical step away from artemisinin. But since E. coli don’t normally produce these chemicals, each step of the process will have to be carefully contrived and optimized. “There’s a lot of engineering still,” says Keasling.

A $42.6 million grant from the Bill and Melinda Gates Foundation should help. In December, the foundation awarded the money to Keasling, his Emeryville, CA, startup Amyris Biotechnologies, and San Francisco’s Institute for OneWorld Health, a nonprofit that aims to secure U.S. Food and Drug Administration approval for bacteria-derived artemisinin within five years.

The promise of bacterial factories doesn’t end with artemisi­nin. Amyris Biotechnologies hopes to adapt Keasling’s terpene precursor pathway to make prostratin, a promising anti-HIV compound found in the bark of the mamala tree on Samoa. With different alterations to the pathway, bacteria could make paclitaxel, the breast cancer drug sold under the brand Taxol and now isolated from yew trees.

Ultimately, Keasling believes, new technologies for analyzing and understanding cellular pathways will enable researchers to engineer microbes to produce a huge range of chemicals, from drugs to plastics. And unlike conventional chemical ­engineering, bacteria do their job cleanly, without requiring or producing environmentally harmful compounds. “We’ve got all these great tools,” Keasling says. “Now we can start to put these to use to solve this one particular problem: how to engineer a cell to do the kinds of chemistries that you want it to do.”

by Erika Jonietz",0.1% - 1.0%,Positive,Biotech,Project
Enviromatics,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Enviromatics
ENVIRONMENT Computer forecasts enhance farm production and species diversity.
Environmental scientists think of computers as old friends. They’ve long used them to crunch the data they collect in the field, whether to map the habitats of endangered species or predict the effects of greenhouse gas emissions on the global climate. But three trends are pushing information technology from the periphery of environmental studies to its very core, according to the proponents of a new field called environmental informatics, or enviromatics.

First, there’s a fresh avalanche of raw data about the environment, a product of networked sensors that monitor ecosystems in real time. Second, there’s the rise of Internet standards such as the Extensible Markup Language (XML), which can tie together data stored in varying formats in different locations. The third trend – the decreasing cost of computing power – means that researchers can use inexpensive desktop machines to run analyses and simulations that once required supercomputers. Just as the invention of fast gene sequencers a decade ago gave rise to bioinformatics, a new wealth of data about the oceans, the atmosphere, and the land is leading to a wider embrace of sensing, simulation, and mapping tools – and hopefully to more reliable predictions about the future.

Environmental modeling, of course, is nothing new: the ratification of the Kyoto Protocol was spurred in part by global climate models that predict average temperature increases of 1 °C to 6 °C over the next century. But such large-scale, long-range climate ­models don’t help with more immediate and local questions – such as whether the humidity this month in Butler County, PA, means that farmers should apply fungicides early to prevent infections. At Pennsylvania State University’s Center for Environmental Informatics, researcher Douglas Miller is pouring data from weather stations throughout the wheat-growing states into a Web-based program that can predict where a devastating wheat fungus infection called fusarium head blight may strike next. Farmers can log into a website, enter their locations and the flowering dates of their crops, and get local maps showing color-coded levels of risk. “We’re putting environmental information into people’s hands so they can make decisions,” says Miller.

Enviromatics is even helping to manage urban growth. In San Diego County, officials compiled a detailed geographical and biological database mapping which vernal pools – basins that fill with rainwater in the winter and spring – harbor the most-endangered strains of species such as the San Diego fairy shrimp and therefore deserve the most protection. Science is rarely the main driver of land management or other decisions affecting the natural environment, but enviromatics may make it harder than ever for politicians to skirt the long-term implications of their decisions.

by Wade Roush",1-10%,Positive,Biotech,methodology
Cell-Phone Viruses,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Cell-Phone Viruses
TELECOM Wireless devices catch bad code through the air and then infect supposedly secure computer systems.
ValleZ has released a digital epidemic – or maybe he’s delivered an early inoculation.

ValleZ is the online handle of a 24-year-old computer programmer from Spain who, last June, wrote the first malicious program targeting cellular phones, the Cabir worm. Now, security experts fear that the rush to integrate cell phones into every aspect of our daily lives might make them the perfect carriers for digital diseases. Bruce Schneier, founder and chief technology officer of Counterpane Internet Security in Mountain View, CA, assesses the threat bluntly: “We’re screwed,” he says.

Or maybe not. ValleZ is a member of an international cabal of programmers called 29A, which specializes in malicious software, or “malware.” These “ethical hobbyists” send their creations to security labs so that experts can research cures. “[Cabir] was a manner of saying that the antiviral people should be watching out for this,” says ValleZ, whom Technology Review tracked down via e-mail.

ValleZ shared the code for his original, nonmalicious version of the worm with other members of 29A. Shortly after, it was passed to a Brazilian programmer who posted his own variation on his website in December. Now, bad guys everywhere are spinning off new versions that are melded with other malware that locks up phones or autodials obscure numbers. As of March, the Helsinki, Finland-based security company F-Secure reported that 15 variations of Cabir had popped up in 14 countries.

Cabir spreads like an airborne disease through Bluetooth wireless connections, a popular means of transferring data at close proximity between cell phones and everything from other phones to car GPS navigation systems. Even antiviral researchers have found themselves worrying that viruses under examination might spread wirelessly to mobile devices outside their labs’ doors. Travis Witteveen, vice president of F-Secure’s North American division, says his company now runs its main mobile-security lab out of an old military bomb shelter.

The cell-phone worm’s task could be as simple as swiping your address book or spewing out costly and annoying text-­message spam. Or it could mount a “denial of service” attack on your wireless-service provider by making your phone rapidly dial many numbers in succession. As people start using their “smart” cell phones to tap into computer networks, the damage caused by malware could grow more severe. If, as promised, cell phones soon begin to serve as payment devices, mobile malware that nabs your identity and taps directly into your credit line could follow. Theoretically, a corporate accountant’s phone could pick up a worm and, when synched to a PC, let it loose on the company’s network, jumbling accounts.

And mobile malware will be able to infect systems not vulnerable to conventional viruses. A car owner could link her Bluetooth-enabled phone to her dashboard computer, so that she can control the phone via buttons on his steering wheel. As she drives down the road, her phone might connect to another in a passing car. Suddenly, her navigation system fails. “This type of threat is probably inevitable,” says Schneier. In the future, cars will include computer systems that permit remote diagnosis of problems. They should be kept physically separate from hardware that regulates mechanical systems – performing calibrations, for instance – lest a virus cause steering or brake controls to fail.

Protection against this nascent peril is beginning to appear. Symbian, the company whose mobile-device operating system has been targeted by every cell-phone virus so far, has released a version of its software that grants Bluetooth access only to programs tagged with secure digital IDs. Antiviral software is not currently bundled with the software preinstalled on most privately purchased cell phones and so is found almost exclusively in business-issued phones. But companies like McAfee and ­InnoPath Software are developing easy ways for individual consumers to download antiviral software. According to research firm IDC, spending on mobile security will leap from around $100 million in 2004 to nearly $1 billion by 2008 – with a significant portion going toward antiviral protection.

ValleZ says he’s done coding mobile malware – for a little while, at least. Of course, that won’t stop others from concocting their own electronic pests. Another, completely new and more virulent mobile virus, CommWarrior, was found in late February. It sends out costly multimedia messages but contains so many bugs that it doesn’t pose a major threat. The next malicious piece of code, however, may be neither a warning exercise nor a self-defeating pest but a full-bore attack on the wireless world.

by Stu Hutson",0.05% - 0.5%,Negative,Software,product
Biomechatronics,2005.0,https://www.technologyreview.com/10-breakthrough-technologies/2005/,,"Biomechatronics
PROSTHETICS Mating robotics with the nervous system creates a new generation of artificial limbs that work like the real thing.
Conventional leg prostheses frequently leave their users, especially above-the-knee amputees, stumbling and falling or walking with abnormal gaits. Hugh Herr, a professor at MIT’s Media Laboratory, is building more-reliable prostheses that users can control more precisely. Some of the latest prosthetic knees on the market already have microprocessors built into them that can be programmed to help the limbs move more naturally. But Herr has taken this idea one step further. He has developed a knee with built-in sensors that can measure how far the knee is bent, as well as the amount of force the user applies to it while walking. This artificial knee – recently commercialized by the Icelandic company Össur – also contains a computer chip that analyzes the sensor data to create a model of the user’s gait, and adapt the movement and resistance of the knee accordingly.

Now Herr is working to distribute those sensors beyond the knee joint, using them to detect not just the mechanical forces of the body but also neural signals from the muscles near the joint. This work is part of an emerging discipline called biomechatronics, in which researchers are building robotic prostheses that can communicate with users’ nervous systems. In five to seven years, predicts Herr, spinal-cord injury patients will move their limbs again by controlling robotic exoskeletons strapped onto them (or at least they will in research settings). Biomechatronics is receiving more attention now in part because of the Iraq War, which is sending a high number of U.S. soldiers home with crippling injuries. Herr, who leads the Media Lab’s biomechatronics group, is part of a new $7.2 million research project run by the U.S. Department of Veterans Affairs (VA) to develop new technologies for amputees who lost limbs as the result of combat injuries.

Herr, a double leg amputee, plans on becoming his own first test subject for his latest prosthetic ankle prototype. By early next year, at least three small sensors will be implanted into the muscles of one of his legs below the knee. As Herr flexes his leg muscles in ways that once moved his ankle, these sensors will measure electrical activity in the muscles and transmit that information to a computer chip in the prosthetic ankle, which will translate those impulses into instructions for the ankle’s motors. Herr hopes to be able to move the ankle by firing up the residual muscles near the joint and feeling it respond, just as he would with a natural joint. Nor will communication be just one way. Herr should also be able to sense the ankle’s position through vibrations emanating from the joint. “We regard this work as extraordinarily promising,” says Roy Aaron, a professor of orthopedics at Brown Medical School who is heading up the VA project.

Having lost his lower legs to frostbite while mountain climbing as a teenager, Herr says he’s looking forward to trying out the device. “I think it will be quite profound to control my ankles again,” he says. Herr’s vision for the field is to combine biomechatronics with tissue engineering and create limbs made of both artificial materials and human tissue. Says Herr, “I think, inevitably, we’ll end up with hybrid devices.”

by Corie Lok",0.05% - 0.5%,Positive,Biotech,methodology
Nanomedicine,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Nanomedicine
James Baker designs nanoparticles to guide drugs directly into cancer cells, which could lead to far safer treatments.
Nanomedicine

Key players
Raoul Kopelman – Nanoparticles for cancer imaging and therapy University of Michigan; Robert Langer – Nanoparticle drug delivery for prostate cancer MIT; Charles Lieber – Nanowire devices for virus detection and cancer screening Harvard University; Ralph Weissleder – Magnetic nano-particles for cancer imaging Harvard University
The treatment begins with an injection of an unremarkable-looking clear fluid. Invisible inside, however, are particles precisely engineered to slip past barriers such as blood vessel walls, latch onto cancer cells, and trick the cells into engulfing them as if they were food. These Trojan particles flag the cells with a fluorescent dye and simultaneously destroy them with a drug.

Developed by University of Michigan physician and researcher James Baker, these multipurpose nanoparticles – which should be ready for patient trials later this year – are at the leading edge of a nanotechnology-based medical revolution. Such methodically designed nanoparticles have the potential to transfigure the diagnosis and treatment of not only cancer but virtually any disease. Already, researchers are working on inexpensive tests that could distinguish a case of the sniffles from the early symptoms of a bioterror attack, as well as treatments for disorders ranging from rheumatoid arthritis to cystic fibrosis. The molecular finesse of nanotechnology, Baker says, makes it possible to “find things like tumor cells or inflammatory cells and get into them and change them directly.”

[To view an illustration of nanoparticles delivering a drug, click here.]

Cancer therapies may be the first nanomedicines to take off. Treatments that deliver drugs to the neighborhood of cancer cells in nanoscale capsules have recently become available for breast and ovarian cancers and for Kaposi’s sarcoma. The next generation of treatments, not yet approved, improves the drugs by delivering them inside individual cancer cells. This generation also boasts multifunction particles such as Baker’s; in experiments reported last June, Baker’s particles slowed and even killed human tumors grown in mice far more efficiently than conventional chemotherapy.

“The field is dramatically expanding,” says Piotr Grodzinski, program director of the National Cancer Institute’s Alliance for Nanotechnology in Cancer. “It’s not an evolutionary technology; it’s a disruptive technology that can address the problems which former approaches couldn’t.”

The heart of Baker’s approach is a highly branched molecule called a dendrimer. Each dendrimer has more than a hundred molecular “hooks” on its surface. To five or six of these, Baker connects folic-acid molecules. Because folic acid is a vitamin, most cells in the body have proteins on their surfaces that bind to it. But many cancer cells have significantly more of these receptors than normal cells. Baker links an anticancer drug to other branches of the dendrimer; when cancer cells ingest the folic acid, they consume the deadly drugs as well.

The approach is versatile. Baker has laden the dendrimers with molecules that glow under MRI scans, which can reveal the location of a cancer. And he can hook different targeting molecules and drugs to the dendrimers to treat a variety of tumors. He plans to begin human trials later this year, potentially on ovarian or head and neck cancer.

Mauro Ferrari, a professor of internal medicine, engineering, and materials science at Ohio State University, is hopeful about what Baker’s work could mean for cancer patients. “What Jim is doing is very important,” he says. “It is part of the second wave of approaches to targeted therapeutics, which I think will have tremendous acceleration of progress in the years to come.”

To hasten development of nano-based therapies, the NCI alliance has committed $144.3 million to nanotech-related projects, funding seven centers of excellence for cancer nanotechnology and 12 projects to develop diagnostics and treatments, including Baker’s.

Baker has already begun work on a modular system in which dendrimers adorned with different drugs, imaging agents, or cancer-targeting molecules could be “zipped together.” Ultimately, doctors might be able to create personalized combinations of nanomedicines by simply mixing the contents of vials of dendrimers.

Such a system is at least 10 years away from routine use, but Baker’s basic design could be approved for use in patients in as little as five years. That kind of rapid progress is a huge part of what excites doctors and researchers about nanotechnology’s medical potential. “It will completely revolutionize large branches of medicine,” says Ferrari.

by Kevin Bullis",0.05-0.5%,Positive,Biotech,Concept
Epigenetics,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Epigenetics
Alexander Olek has developed tests to detect cancer early by measuring its subtle DNA changes.
Epigenetics

Key players
Stephan Beck – Epigenetics of the immune system at Wellcome Trust Sanger Institute, Cambridge, England; Joseph Bigley – Cancer diagnosis and drug development at OncoMethylome Sciences, Durham, NC; Thomas Gingeras – Gene chips for epigenetics at Affymetrix, Santa Clara, CA
Sequencing the human genome was far from the last step in explaining human genetics. Researchers still need to figure out which of the 20,000-plus human genes are active in any one cell at a given moment. Chemical modifications can interfere with the machinery of protein manufacture, shutting genes down directly or making chromosomes hard to unwind. Such chemical interactions constitute a second order of genetics known as epigenetics.

In the last five years, researchers have developed the first practical tools for identifying epigenetic interactions, and German biochemist Alexander Olek is one of the trailblazers. In 1998, Olek founded Berlin-based Epigenomics to create a rapid and sensitive test for gene methylation, a common DNA modification linked to cancer. The company’s forthcoming tests will determine not only whether a patient has a certain cancer but also, in some cases, the severity of the cancer and the likelihood that it will respond to a particular treatment. “Alex has opened up a whole new way of doing diagnostics,” says Stephan Beck, a researcher at the Wellcome Trust Sanger Institute in Cambridge, England, and an epigenetics pioneer.

Methylation adds four atoms to cytosine, one of the four DNA “letters,” or nucleotides. The body naturally uses methylation to turn genes on and off: the additional atoms block the proteins that transcribe genes. But when something goes awry, methylation can unleash a tumor by silencing a gene that normally keeps cell growth in check. Removing a gene’s natural methylation can also render a cell cancerous by activating a gene that is typically “off” in a particular tissue.

The problem is that methylated genes are hard to recognize in their native state. But Olek says Epigenomics has developed a method to detect as little as three picograms of methylated DNA; it will spot as few as three cancer cells in a tissue sample.

To create a practical diagnostic test for a given cancer, Epigenomics compares several thousand genes from cancerous and healthy cells, identifying changes in the methylation of one or more genes that correlate with the disease. Ultimately, the test examines the methylation states of only the relevant genes. The researchers go even further through a sort of epigenetic archeology: by examining the DNA in tissues from past clinical trials, they can identify the epigenetic signals in the patients who responded best or worst to a given treatment.

Philip Avner, an epigenetics pioneer at the Pasteur Institute in Paris, says that Epigenomics’ test is a powerful tool for accurately diagnosing and understanding cancers at their earliest stages. “If we can’t prevent cancer, at least we can treat it better,” says Avner.

Roche Diagnostics expects to bring Epigenomics’ first product, a screening test for colon cancer, to market in 2008. The test is several times more likely to spot a tumor than the current test, which measures the amount of blood in a stool sample. And thanks to the sensitivity of its process, Epigenomics can detect the tiny amounts of methylated DNA such tumors shed into the bloodstream, so only a standard blood sample is required. The company is working on diagnostics for three more cancers: non-Hodgkin’s lymphoma, breast cancer, and prostate cancer.

Olek believes that epigenetics could also have applications in helping explain how lifestyle affects the aging process. It might reveal, for example, why some individuals have a propensity toward diabetes or heart disease.

Olek’s goal is a human-epigenome mapping project that would identify the full range of epigenetic variation possible in the human genome. Such a map, Olek believes, could reveal the missing links between genetics, disease, and the environment. Today, progress on the methylation catalogue is accelerating, thanks to Epigenomics and the Wellcome Trust Sanger Institute, which predict that the methylation status of 10 percent of human genes will be mapped by the end of this year.

by Peter Fairley",0.05% - 0.5%,Positive,Biotech,concept
Cognitive Radio,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Cognitive Radio
To avoid future wireless traffic jams, Heather “Haitao” Zheng is finding ways to exploit unused radio spectrum.
Cognitive Radio

Key players
Bob Broderson – Advanced communication algorithms and low-power devices at University of California, Berkeley; John Chapin – Software-defined radios at Vanu, Cambridge, MA; Michael Honig – Pricing algorithm for spectrum sharing at Northwestern University; Joseph Mitola III – Cognitive radios at Mitre, McLean, VA; Adam Wolisz – Protocols for communications networks at Technical University of Berlin, Germany
Growing numbers of people are making a habit of toting their laptops into Starbuck’s, ordering half-caf skim lattes, and plunking down in chairs to surf the Web wirelessly. That means more people are also getting used to being kicked off the Net as computers competing for bandwidth interfere with one another. It’s a local effect – within 30 to 60 meters of a transceiver – but there’s just no more space in the part of the radio spectrum designated for Wi-Fi.

Imagine, then, what happens as more devices go wireless – not just laptops, or cell phones and BlackBerrys, but sensor networks that monitor everything from temperature in office buildings to moisture in cornfields, radio frequency ID tags that track merchandise at the local Wal-Mart, devices that monitor nursing-home patients. All these gadgets have to share a finite – and increasingly crowded – amount of radio spectrum.

Heather Zheng, an assistant professor of computer science at the University of California, Santa Barbara, is working on ways to allow wireless devices to more efficiently share the airwaves. The problem, she says, is not a dearth of radio spectrum; it’s the way that spectrum is used.

The Federal Communications Commission in the United States, and its counterparts around the world, allocate the radio spectrum in swaths of frequency of varying widths. One band covers AM radio, another VHF television, still others cell phones, citizen’s-band radio, pagers, and so on; now, just as wireless devices have begun proliferating, there’s little left over to dole out.

But as anyone who has twirled a radio dial knows, not every channel in every band is always in use. In fact, the FCC has determined that, in some locations or at some times of day, 70 percent of the allocated spectrum may be sitting idle, even though it’s officially spoken for.

Zheng thinks the solution lies with cognitive radios, devices that figure out which frequencies are quiet and pick one or more over which to transmit and receive data. Without careful planning, however, certain bands could still end up jammed. Zheng’s answer is to teach cognitive radios to negotiate with other devices in their vicinity. In Zheng’s scheme, the FCC-designated owner of the spectrum gets priority, but other devices can divvy up unused spectrum among themselves.

But negotiation between devices uses bandwidth in itself, so Zheng simplified the process. She selected a set of rules based on “game theory” – a type of mathematical modeling often used to find the optimal solutions to economics problems – and designed software that made the devices follow those rules. Instead of each radio’s having to tell its neighbor what it’s doing, it simply observes its neighbors to see if they are transmitting and makes its own decisions.

Zheng compares the scheme to a driver’s reacting to what she sees other drivers doing. “If I’m in a traffic lane that is heavy, maybe it’s time for me to shift to another lane that is not so busy,” she says. When shifting lanes, however, a driver needs to follow rules that prevent her from bumping into others.

Zheng has demonstrated her approach in computer simulations and is working toward testing it on actual hardware. But putting spectrum-sharing theory into practice will take engineering work, from designing the right antennas to writing the software that will run the cognitive radios, Zheng acknowledges. “This is just a starting phase,” she says.

Nonetheless, cognitive radios are already making headway in the real world. Intel has plans to build reconfigurable chips that will use software to analyze their environments and select the best protocols and frequencies for data transmission. The FCC has made special allowances so that new types of wireless networks can test these ideas on unused television channels, and the Institute of Electrical and Electronics Engineers, which sets many of the technical standards that continue to drive the Internet revolution, has begun considering cognitive-radio standards.

It may be 10 years before all the issues get sorted out, Zheng says, but as the airwaves become more crowded, all wireless devices will need more-efficient ways to share the spectrum.

by Neil Savage",0.5% - 5.0%,positive,Software,methodology
Nuclear Reprogramming,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Nuclear Reprogramming
Hoping to resolve the embryonic-stem-cell debate, Markus Grompe envisions a more ethical way to derive the cells.
Embryonic stem cells may spark more vitriolic argument than any other topic in modern science. Conservative Christians aver that the cells’ genesis, which requires destroying embryos, should make any research using them taboo. Many biologists believe that the cells will help unlock the secrets of devastating diseases such as Parkinson’s and multiple sclerosis, providing benefits that far outweigh any perceived ethical harm.


BRYAN CHRISTIE DESIGN
Nuclear Reprogramming

Key players
George Daley – Studying nanog’s ability to reprogram nuclei at Harvard Medical Schoo; Kevin Eggan – Reprogramming adult cells using stem cells at Harvard University; Rudolf Jaenisch – Creating tailored stem cells using altered nuclear transfer (CDX2) at MIT
Markus Grompe, director of the Oregon Stem Cell Center at Oregon Health and Science University in Portland, hopes to find a way around the debate by producing cloned cells that have all the properties of embryonic stem cells – but don’t come from embryos.

His plan involves a variation on the cloning procedure that produced Dolly the sheep. In the original procedure, scientists transferred the genetic material from an adult cell into an egg stripped of its own DNA. The egg’s proteins reprogrammed the adult DNA, creating an embryo genetically identical to the adult donor. Grompe believes that by forcing the donor cell to produce a protein called nanog, which is normally found only in embryonic stem cells, he can alter the reprogramming process so that it never results in an embryo. Instead, it would yield a cell with many of the characteristics of an embryonic stem cell.

Grompe’s work is part of a growing effort to find alternative ways to create cells with the versatility of embryonic stem cells. Many scientists hope to use proteins to directly reprogram, say, skin cells to behave like stem cells.

Others think smaller molecules may do the trick; Scripps Research Institute chemist Peter Schultz has found a chemical that turns mouse muscle cells into cells able to form fat and bone cells. And Harvard University biologist Kevin Eggan believes it may be possible to create stem cells whose DNA matches a specific patient’s by using existing stem cells stripped of their DNA to reprogram adult cells.

Meanwhile, researchers have tested methods for extracting stem cells without destroying viable embryos. Last fall, MIT biologist Rudolf Jaenisch and graduate student Alexander Meissner showed that by turning off a gene called CDX2 in the nucleus of an adult cell before transferring it into a nucleus-free egg cell, they could create a biological entity unable to develop into an embryo – but from which they could still derive normal embryonic stem cells.

Also last fall, researchers at Advanced Cell Technology in Worcester, MA, grew embryonic stem cells using a technique that resembles something called preimplantation genetic diagnosis (PGD). PGD is used to detect genetic abnormalities in embryos created through in vitro fertilization; doctors remove a single cell from an eight-cell embryo for testing. The researchers separated single cells from eight-cell mouse embryos, but instead of testing them, they put each in a separate petri dish, along with embryonic stem cells. Unidentified factors caused the single cells to divide and develop some of the characteristics of stem cells. When the remaining seven-cell embryos were implanted into female mice, they developed into normal mice.

Such methods, however, are unlikely to resolve the ethical debate because, in the eyes of some, they still endanger embryos. Grompe’s approach holds out the promise of unraveling the moral dilemma. If it works, no embryo will have been produced – so no potential life will be harmed. As a result, some conservative ethicists have endorsed Grompe’s proposal.

Whether it is actually a feasible way to harvest embryonic stem cells remains uncertain. Some are skeptical. “There’s really no evidence it would work,” says Jaenisch. “I doubt it would.” But the experiments Grompe proposes, Jaenisch says, would still be scientifically valuable in helping explain how to reprogram cells to create stem cells. Harvard Stem Cell Institute scientist George Daley agrees. In fact, Daley’s lab is also studying nanog’s ability to reprogram adult cells.

Still, many biologists and bioethicists have mixed feelings about efforts to reprogram adult cells to become pluripotent. While they agree the research is important, they worry that framing it as a search for a stem cell compromise may slow funding – private and public – for embryonic-stem-cell research, hampering efforts to decipher or even cure diseases that affect thousands of desperate people. Such delays, they argue, are a greater moral wrong than the loss of cells that hold only the potential for life.

Many ethicists – and the majority of Americans – seem to agree. “We’ve already decided as a society that it’s perfectly okay to create and destroy embryos to help infertile couples to have babies. It seems incredible to me that we could say that that’s a legitimate thing to do, but we can’t do the same thing to help fight diseases that kill children,” says David Magnus, director of the Stanford Center for Biomedical Ethics.

by Erika Jonietz",-0.1% to -0.01%,Positive,Biotech,Concept
Diffusion Tensor Imaging,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Diffusion Tensor Imaging
Kelvin Lim is using a new brain-imaging method to understand schizophrenia.
Flipping through a pile of brain scans, a neurologist or psychiatrist would be hard pressed to pick out the one that belonged to a schizophrenic. Although schizophrenics suffer from profound mental problems – hallucinated conversations and imagined conspiracies are the best known – their brains look more or less normal.

This contradiction fascinated Kelvin Lim, a neuroscientist and psychiatrist at the University of Minnesota Medical School, when he began using imaging techniques such as magnetic resonance imaging (MRI) to study the schizophrenic brain in the early 1990s. Lim found subtle hints of brain structures gone awry, but to understand how these problems led to the strange symptoms of schizophrenia, he needed a closer look at the patients’ neuroanatomy than standard scans could provide.

Then, in 1996, a colleague told him about diffusion tensor imaging (DTI), a newly developed variation of MRI that allowed scientists to study the connections between different brain areas for the first time.


Diffusion tensor imaging (DTI) yields images of nerve fiber tracts; different colors indicate the organization of the nerve fibers. Here, a tract originating at the cerebellum is superimposed on a structural-MRI image of a cross section of the brain.
Diffusion Tensor Imaging

Key players
Peter Basser – Development of higher-resolution diffusion imaging techniques at National Institute of Child Health and Human Development; Aaron Field – Neurosurgery planning at University of Wisconsin-Madison; Michael Moseley – Assessment and early treatment of stroke at Stanford University
Lim has pioneered the use of DTI to understand psychiatric disease. He was one of the first to use the technology to uncover minute structural aberrations in the brains of schizophrenics. His group has recently found that memory and cognitive problems associated with schizophrenia, major but undertreated aspects of the disease, are linked to flaws in nerve fibers near the hippocampus, a brain area crucial for learning and memory. “DTI allows us to examine the brain in ways we hadn’t been able to before,” says Lim.

Conventional imaging techniques, such as structural MRI, reveal major anatomical features of the brain – gray matter, which is made up of nerve cell bodies. But neuroscientists believe that some diseases may be rooted in subtle “wiring” problems involving axons, the long, thin tails of neurons that carry electrical signals and constitute the brain’s white matter. With DTI, researchers can, for the first time, look at the complex network of nerve fibers connecting the different brain areas. Lim and his colleagues hope this sharper view of the brain will help better define neurological and psychiatric diseases and yield more-targeted treatments.

In DTI, radiologists use specific radio-frequency and magnetic field-gradient pulses to track the movement of water molecules in the brain. In most brain tissue, water molecules diffuse in all different directions. But they tend to diffuse along the length of axons, whose coating of white, fatty myelin holds them in. Scientists can create pictures of axons by analyzing the direction of water diffusion.

Following Lim’s lead, other neuroscientists have begun using DTI to study a host of disorders, including addiction, epilepsy, traumatic brain injury, and various neurodegenerative diseases. For instance, DTI studies have shown that chronic alcoholism degrades the white-matter connections in the brain, which may explain the cognitive problems seen in heavy drinkers. Other DTI projects are examining how the neurological scars left by stroke, multiple sclerosis, and amyotrophic lateral sclerosis (better known as Lou Gehrig’s disease) are linked to patients’ disabilities.

Lim is pushing the technology even further by combining it with findings from other fields, such as genetics, to unravel the mysteries of neurological and psychiatric disorders. Lim’s group has found, for instance, that healthy people with a genetic risk for developing Alzheimer’s disease have tiny structural defects in specific parts of the brain that are not shared by noncarriers. How these defects might be linked to the neurological problems of Alzheimer’s isn’t clear, but the researchers are trying to find the connection.

Lim and others also continue to refine DTI itself, striving for an even closer look at the brain’s microarchitecture. For example, current DTI techniques can easily image brain areas with large bundles of fibers all moving in the same direction, such as the corpus callosum, which connects the two hemispheres of the brain. But it has difficulty with areas such as the one where fibers leave the corpus callosum for other parts of the brain, which is a tangle of wires.

Researchers hope tools for studying white matter, like DTI, will help illuminate the mysteries of both healthy and diseased brains. Lim believes his own research into diseases like schizophrenia and Alzheimer’s could yield better diagnostics within 10 to 20 years – providing new hope for the next generation of patients.

by Emily Singer",0.01% - 0.1%,Positive,Biotech,methodology
Universal Authentication,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Universal Authentication
Leading the development of a privacy-protecting online ID system, Scott Cantor is hoping for a safer Internet.
If you’re like most people, you’ve established multiple user IDs and passwords on the Internet – for your employer or school, your e-mail accounts, online retailers, banks, and so forth. It’s cumbersome and confusing, slowing down online interactions if only because it’s so easy to forget the plethora of passwords. Worse, the diversity of authentication systems increases the chances that somewhere, your privacy will be compromised, or your identity will be stolen.


Security with privacy: Shibboleth software could create a far more trustworthy Internet by allowing a one-step login that carries through to many online organizations, confirming identity but preserving privacy. In this example, a student logs in to his university's site, then clicks through to a second university. Shibboleth confirms that the person is a student but doesn't give his name.
BRYAN CHRISTIE DESIGN
Universal Authentication

Key players
Stefan Brands – Cryptology, identity management, and authentication technologies at McGill University; Kim Cameron – “InfoCard” system to manage and employ a range of digital identity information at Microsoft, Redmond, WA; Robert Morgan – “Person registry” that gathers identity data from source systems; scalable authentication infrastructure at University of Washington; Tony Nadalin – Personal-identity software platform at IBM, Armonk, NY
The balkanization of today’s online identity-verifying systems is a big part of the Internet’s fraud and security crisis. As Kim Cameron, Microsoft’s architect of identity and access, puts it in his blog, “If we do nothing, we will face rapidly proliferating episodes of theft and deception that will cumulatively erode public trust in the Internet.” Finding ways to bolster that trust is critically important to preserving the Internet as a useful, thriving medium, argues David D. Clark, an MIT computer scientist and the Internet’s onetime chief protocol architect.

Scott Cantor, a senior systems developer at Ohio State University, thinks the answer may lie in Web “authentication systems” that allow users to hop securely from one site to another after signing on just once. Such systems could protect both users’ privacy and the online businesses and other institutions that offer Web-based services.

Cantor led the technical development of Shibboleth, an open-standard authentication system used by universities and the research community, and his current project is to expand its reach. He has worked, not only to make the system function smoothly, but also to build bridges between it and parallel corporate efforts. “Scott is the rock star of the group,” says Steven Carmody, an IT architect at Brown University who manages a Shibboleth project for Internet2, an Ann Arbor, MI-based research consortium that develops advanced Internet technologies for research laboratories and universities. “Scott’s work has greatly simplified the management of these Internet-based relationships, while ensuring the required security and level of assurance for each transaction.”

Shibboleth acts not only as an authentication system but also – counterintuitively – as a guardian of privacy. Say a student at Ohio State wishes to access Brown’s online library. Ohio State securely holds her identifying information – name, age, campus affiliations, and so forth. She enters her user ID and password into a page on Ohio State’s website. But when she clicks through to Brown, Shibboleth takes over. It delivers only the identifying information Brown really needs to know: the user is a registered Ohio State student.

While some U.S. universities have been using Shibboleth since 2003, adoption of the system grew rapidly in 2005. It’s now used at 500-plus sites worldwide, including educational systems in Australia, Belgium, England, Finland, Denmark, Germany, Switzerland, and the Netherlands; even institutions in China are signing on. Also in late 2005, Internet2 announced Shibboleth’s interoperability with a Microsoft security infrastructure called the Active Directory Federation Service.

Critically, the system is moving into the private sector, too. The science and medical division of research publishing conglomerate Reed Elsevier has begun granting university-based subscribers access to its online resources through Shibboleth, rather than requiring separate, Elsevier-specific logins. And Cantor has forged ties with the Liberty Alliance, a consortium of more than 150 companies and other institutions dedicated to creating shared identity and authentication systems.

With Cantor’s help, the alliance, which includes companies such as AOL, Bank of America, IBM, and Fidelity Investments, is basing the design of its authentication systems on a common standard known as SAML. The alliance, Cantor says, was “wrestling with lots of the same hard questions that we were, and we were starting to play in the same kind of territories. Now there is a common foundation….we’re trying to make it ubiquitous.” With technical barriers overcome, the companies can now roll out systems as their business needs dictate.

Of course, Cantor is not the only researcher, nor Shibboleth the only technology, in the field of Internet authentication. In 1999, for instance, Microsoft launched its Passport system, which let Windows users access any participating website using their e-mail addresses and passwords. Passport, however, encountered a range of security and privacy problems.

But thanks to the efforts of the Shibboleth team and the Liberty Alliance, Web surfers could start accessing multiple sites with a single login in the next year or so, as companies begin rolling out interoperable authentication systems.

by David Talbot",0.05% to 0.5%,Positive,Software,platform
Nanobiomechanics,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Nanobiomechanics
Measuring the tiny forces acting on cells, Subra Suresh believes, could produce fresh understanding of diseases.
Most people don’t think of the human body as a machine, but Subra Suresh does. A materials scientist at MIT, Suresh measures the minute mechanical forces acting on our cells.

Medical researchers have long known that diseases can cause – or be caused by – physical changes in individual cells. For instance, invading parasites can distort or degrade blood cells, and heart failure can occur as muscle cells lose their ability to contract in the wake of a heart attack. Knowing the effect of forces as small as a piconewton – a trillionth of a newton – on a cell gives researchers a much finer view of the ways in which diseased cells differ from healthy ones.


Optical tweezers stretch a healthy red blood cell (top row), increasing the applied force slowly, by a matter of piconewtons. A cell in a late stage of malarial infection is stretched in a similar fashion (bottom row). The experiment illustrates how the infected cell becomes rigid, which prevents it from traveling easily through blood capillaries and helps cause the symptoms of malaria.
SUBRA SURESH
Nanobiomechanics

Key players
Eduard Arzt – Structure and mobility of pancreatic cancer cells Max Planck at Institute, Stuttgart, Germany; Peter David and Genevieve Milon – Parasite-host interaction; mechanics of the spleen at Pasteur Institute, Paris, France; Ju Li – Models of internal cellular structures at Ohio State University; C. T. Lim and Kevin Tan – Red-blood-cell mechanics at National University of Singapore
Suresh spent much of his career making nanoscale measurements of materials such as the thin films used in microelectronic components. But since 2003, Suresh’s laboratory has spent more and more time applying nanomeasurement techniques to living cells. He’s now among a pioneering group of materials scientists who work closely with microbiologists and medical researchers to learn more about how our cells react to tiny forces and how their physical form is affected by disease. “We bring to the table expertise in measuring the strength of materials at the smallest of scales,” says Suresh.

One of Suresh’s recent studies measured mechanical differences between healthy red blood cells and cells infected with malaria parasites. Suresh and his collaborators knew that infected blood cells become more rigid, losing the ability to reduce their width from eight micrometers down to two or three micrometers, which they need to do to slip through capillaries. Rigid cells, on the other hand, can clog capillaries and cause cerebral hemorrhages. Though others had tried to determine exactly how rigid malarial cells become, Suresh’s instruments were able to bring greater accuracy to the measurements. Using optical tweezers, which employ intensely focused laser light to exert a tiny force on objects attached to cells, Suresh and his collaborators showed that red blood cells infected with malaria become 10 times stiffer than healthy cells – three to four times stiffer than was previously estimated.

Eduard Arzt, director of materials research at the Max Planck Institute in Stuttgart, Germany, says that Suresh’s work is important because cell flexibility is a vital characteristic not only of malarial cells but also of metastasizing cancer cells. “Many of the mechanical concepts we’ve been using for a long time, like strength and elasticity, are also very important in biology,” says Arzt.

Arzt and Suresh both caution that it’s too early to say that understanding the mechanics of human cells will lead to more effective treatments. But what excites them and others in the field is the ability to measure the properties of cells with unprecedented precision. That excitement seems to be spreading: in October, Suresh helped inaugurate the Global Enterprise for Micro-Mechanics and Molecular Medicine, an international consortium that will use nanomeasurement tools to tackle major health problems, including malaria, sickle-cell anemia, cancer of the liver and pancreas, and cardiovascular disease. Suresh serves as the organization’s founding director.

“We know mechanics plays a role in disease,” says Suresh. “We hope it can be used to figure out treatments.” If it can, the tiny field of nanomeasurement could have a huge impact on the future of medicine.

by Michael Fitzgerald",0.01% - 1%,Positive,Biotech,concept
Pervasive Wireless,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Pervasive Wireless
Can’t all our wireless gadgets just get along? It’s a question that Dipankar Raychaudhuri is trying to answer.
Pervasive Wireless

Key players
David Culler – Operating systems and middleware for wireless sensors at University of California, Berkeley; Kazuo Imai – Integrating cellular with other network technology at NTT DoCoMo, Tokyo, Japan; Lakshman Krishnamurthy and Steven Conner – Wireless network architecture at Intel, Santa Clara, CA
In New Brunswick, NJ, is a large, white room with an army of yellow boxes hanging from the ceiling. Eight hundred in all, the boxes are actually a unique grid of radios that lets researchers design and test ways to link mobile, radio-equipped computers in configurations that can change on the fly.

The ability to form such ad hoc networks, says Dipankar Raychaudhuri, director of the Rutgers University lab that houses the radios, will be critical to the advent of pervasive computing–in which everything from your car to your coffee cup “talks” to other devices in an attempt to make your life run more smoothly.

Wireless transactions already take place; anybody who speeds through tolls with an E-ZPass transmitter participates in them daily. But Raychaudhuri foresees a not-too-distant day when radio frequency identification (RFID) tags embedded in merchandise call your cell phone to alert you to sales, cars talk to each other to avoid collisions, and elderly people carry heart and blood-pressure monitors that can call a doctor during a medical emergency. Even mesh networks, collections of wireless devices that pass data one to another until it reaches a central computer, may need to be connected to pagers, cell phones, or other gadgets that employ diverse wireless protocols.

Hundreds of researchers at universities, large companies such as Microsoft, Intel, and Nortel, and small startups are developing embedded radio devices and sensors. But making computing truly pervasive entails tying these disparate pieces together, says Raychaudhuri, a professor of electrical and computer engineering at Rutgers. Finding ways to do that is what the radio test grid, which Raychaudhuri built with computer scientists Ivan Seskar and Max Ott, is for.

One problem the researchers are addressing is that different devices communicate using different radio standards: RFID tags use one set of standards, cell phones still others, and various Wi-Fi devices several versions of a third. Linking such devices into a pervasive network means providing them with a common protocol.

Take, for example, the issue of automotive safety. Enabling cars to communicate with each other could prevent crashes; in Raychaudhuri’s vision, each car would have a Global Positioning System unit and send its exact location to nearby vehicles. But realizing that vision requires a protocol that allows the cars not only to communicate but also to decide how many other cars they should include in their networks and how close another car should be to be included. As programmers develop candidates for such a protocol, they try them out on the radio test bed. Each yellow box contains a computer and three different radios, two for handling the various Wi-Fi standards and one that uses either Bluetooth or ZigBee, short-range wireless protocols for personal electronics and for monitoring or control devices, respectively. The researchers configure the radios to mimic the situation they want to test and load their protocols to see, for instance, how long it takes each radio to detect neighbors and send data. “If I want cars not to collide, it cannot take 10 seconds to determine that a car is nearby,” says Raychaudhuri. “It has to take a few microseconds.”

The Rutgers radio grid is the first large-scale shared research facility that researchers can use to study multiple wireless devices and network technologies. “The sort of real-world complexity, dealing with real-world numbers that [the test bed] allows you to do, is something that really makes it quite unique,” says Tod Sizer, director of the Wireless Technology Research Department at Lucent Technologies’ Bell Labs.

Sizer’s group is working with Raychaudhuri to build cognitive-radio boxes that can be programmed to employ a wide variety of wireless standards, such as RFID, Wi-Fi, or cellular-phone protocols.

While hordes of researchers are developing new networked devices, Raychaudhuri says it is the standardization of communications protocols that will make pervasive computing take off. In just five years, he believes, networks of embedded devices will be all around us. His aim is to reduce “friction” in daily life, eliminating lines, saving time in searching for objects, automating security checkpoints in airports, and the like. “You save 10 seconds here, two minutes there, but it’s significant,” he says. He claims that just a 2 percent reduction of friction in the world’s economy could be worth hundreds of billions of dollars in productivity. “Each transaction is small, but the benefit to society is very large.”

by Neil Savage",2-20%,Positive,Hardware,Concept
Stretchable Silicon,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Stretchable Silicon
By teaching silicon new tricks, John Rogers is reinventing the way we use electronics.
These days, most electronic circuitry comes in the form of rigid chips, but devices thin and flexible enough to be rolled up like a newspaper are fast approaching. Already, “smart” credit cards carry bendable microchips, and companies such as Fujitsu, Lucent Technologies, and E Ink are developing “electronic paper” – thin, paperlike displays.

But most truly flexible circuits are made of organic semiconductors sprayed or stamped onto plastic sheets. Although useful for roll-up displays, organic semiconductors are just too slow for more intense computing tasks. For those jobs, you still need silicon or another high-speed inorganic semiconductor. So John Rogers, a materials scientist at the University of Illinois at Urbana-Champaign, found a way to stretch silicon.


Releasing the tension on the elastic produces “waves” of silicon that can later be stretched out again as needed. Such flexible silicon could be used to make wearable electronics.
BRYAN CHRISTIE DESIGN
Stretchable Silicon

Key players
Stephanie Lacour – Neuro-electronic prosthesis to repair damage to the nervous system at University of Cambridge, England; Takao Someya – Large-area electronics based on organic transistors at University of Tokyo; Sigurd Wagner – Electronic skin based on thin-film silicon at Princeton University
If bendable is good, stretchable is even better, says Rogers, especially for high-performance conformable circuits of the sort needed for so-called smart clothes or body armor. “You don’t comfortably wear a sheet of plastic,” he says. The potential applications of circuitry made from Roger’s stretchable silicon are vast. It could be used in surgeons’ gloves to create sensors that would read chemical levels in the blood and alert a surgeon to a problem, without impairing the sense of touch. It could allow a prosthetic limb to use pressure or temperature cues to change its shape.

What makes Rogers’s work particularly impressive is that he works with single-crystal silicon, the same type of silicon found in microprocessors. Like any other single crystal, single-crystal silicon doesn’t naturally stretch. Indeed, in order for it even to bend, it must be prepared as an ultrathin layer only a few hundred nanometers thick on a bendable surface. Rogers exploits the flexibility of thin silicon, but instead of attaching it to plastic, he affixes it in narrow strips to a stretched-out, rubber-like polymer. When the stretched polymer snaps back, the silicon strips buckle but do not break, forming “waves” that are ready to stretch out again.

Rogers’s team has fabricated diodes and transistors – the basic building blocks of electronic devices – on the thin ribbons of silicon before bonding them to the polymer; the wavy devices work just as well as conventional rigid versions, Rogers says. In theory, that means complete circuits of the sort found in computers and other electronics would also work properly when rippled.

Rogers isn’t the first researcher to build stretchable electronics. A couple of years ago, Princeton University’s Sigurd Wagner and colleagues began making stretchable circuits after inventing elastic-metal interconnects. Using the stretchable metal, Wagner’s group connected together rigid “islands” of silicon transistors. Although the silicon itself couldn’t stretch, the entire circuit could. But, Wagner notes, his technique isn’t suited to making electrically demanding circuits such as those in a Pentium chip. “The big thing that John has done is use standard, high-performance silicon,” says Wagner.

Going from simple diodes to the integrated circuits needed to make sensors and other useful microchips could take at least five years, says Rogers. In the meantime, his group is working to make silicon even more flexible. When the silicon is affixed to the rubbery surface in rows, it can stretch only in one direction. By changing the strips’ geometry, Rogers hopes to make devices pliable enough to be folded up like a T-shirt. That kind of resilience could make silicon’s future in electronics stretch out a whole lot further.

by Kate Greene",0.01% - 0.1%,Positive,Nanotech,product
Comparative Interactomics,2006.0,https://www.technologyreview.com/10-breakthrough-technologies/2006/,,"Comparative Interactomics
By creating maps of the body’s complex molecular inter­actions, Trey Ideker is providing new ways to find drugs.
Comparative Interactomics

Key players
James Collins – Synthetic gene networks at Boston University; Bernhard Palsson – Metabolic networks at University of California, San Diego; Marc Vidal – Comparison of interactomes among species at Dana-Farber Cancer Institute, Boston, MA
Biomedical research these days seems to be all about the “omes”: genomes, proteomes, metabolomes. Beyond all these lies the mother of all omes – or maybe just the ome du jour: the interactome. Every cell hosts a vast array of interactions among genes, RNA, metabolites, and proteins. The impossibly complex map of all these interactions is, in the language of systems biology, the interactome.

Trey Ideker, a molecular biotechnologist by way of electrical engineering, has recently begun comparing what he calls the “circuitry” of the interactomes of different species. “It’s really an incremental step in terms of the concepts, but it’s a major leap forward in that we can gather and analyze completely new types of information to characterize biological systems,” says Ideker, who runs the Laboratory for Integrative Network Biology at the University of California, San Diego. “I think it’s going to be cool to map out the circuitry of all these cells.”

Beyond the cool factor, Ideker and other leaders in the nascent field of interactomics hope that their work may help uncover new drugs, improve existing drugs by providing a better understanding of how they work, and even lead to computerized models of toxicity that could replace studies now conducted on animals. “Disease and drugs are about pathways,” Ideker says.

Ideker made a big splash in the field in 2001 while still a graduate student with Leroy Hood at the Institute for Systems Biology in Seattle. In a paper for Science, Ideker, Hood, and coworkers described in startling detail how yeast cells use sugar. They presented a wiring-like diagram illustrating everything from the suite of genes involved, to the protein-protein interactions, to how perturbing the system altered different biochemical pathways. “His contribution was really special,” says geneticist Marc Vidal of the Dana-Farber Cancer Institute in Boston, who introduced the concept that interactomes can be conserved between species. “He came up with one of the first good visualization tools.”

Last November, Ideker’s team turned heads by reporting in Nature that it had aggregated in one database all the available protein-protein interactomes of yeast, the fruit fly, the nematode worm, and the malaria-causing parasite Plasmodium falciparum. Though there’s nothing particularly novel about comparing proteins across species, Ideker’s lab is one of the few that has begun hunting for similarities and differences between the protein-protein interactions of widely different creatures. It turns out that the interactomes of yeast, fly, and worm include interactions called protein complexes that have some similarities between them. This conservation across species indicates that the interactions may serve some vital purpose. But Plasmodium, oddly, shares no protein complexes with worm or fly and only three with yeast. “For a while, we struggled to figure out what was going wrong with our analysis,” says Ideker. After rechecking their data, Ideker and his team concluded that Plasmodium probably just had a somewhat different interactome.

For pharmaceutical makers, the discovery of unique biological pathways, such as those found in the malaria parasite, suggests new drug targets. Theoretically, a drug that can interrupt such a pathway will have limited, if any, impact on circuits in human cells, reducing the likelihood of toxic side effects. Theoretically. In reality, pharmaceutical companies aren’t exactly tripping over themselves to make new drugs for malaria – a disease that strikes mainly in poor countries. But the general idea has great promise, says Ideker, who now plans to compare the interactomes of different HIV strains to see whether any chinks in that virus’s armor come to light.

George Church, who directs the Lipper Center for Computational Genetics at Harvard Medical School, has high respect for Ideker but adds another caveat: existing interactome data comes from fast, automated tests that simply aren’t that accurate yet. “The way I divide the omes is by asking, Are these data permanent, or are they going to be replaced by something better?” says Church. Data on the DNA sequences of genomes, Church says, is permanent. But interactome data? “There’s a 50-50 chance that this will be true or accepted in two years,” says Church. “That’s not Trey’s fault. He’s one of the people who is trying to make it more rigorous.”

Ideker agrees that “there’s a lot of noise in the system,” but he says the continuing flood of interactome data is making what happens inside different cells ever more clear. “Within five years, we hope to take these interaction data and build models of cellular circuitry to predict actions of drugs before they’re in human trials. That’s the billion-dollar application.”

by Jon Cohen",0.01% - 0.1%,Positive,Biotech,concept
Peering into Video's Future,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Peering into Video’s Future
The Internet is about to drown in digital video. Hui Zhang thinks peer-to-peer networks could come to the rescue.

JOHN HERSEY
Ted Stevens, the 83-year-old senior senator from Alaska, was widely ridiculed last year for a speech in which he described the Internet as ""a series of tubes."" Yet clumsy as his metaphor may have been, Stevens was struggling to make a reasonable point: the tubes can get clogged. And that may happen sooner than expected, thanks to the exploding popularity of digital video.

TV shows, YouTube clips, animations, and other video applications already account for more than 60 percent of Internet traffic, says CacheLogic, a Cambridge, England, company that sells media delivery systems to content owners and Internet service providers (ISPs). ""I imagine that within two years it will be 98 percent,"" adds Hui Zhang, a computer scientist at Carnegie Mellon University. And that will mean slower downloads for everyone.

Zhang believes help could come from an unexpected quarter: peer-to-peer (P2P) file distribution technology. Of course, there's no better playground for piracy, and millions have used P2P networks such as Gnutella, Kazaa, and BitTorrent to help themselves to copyrighted content. But Zhang thinks this black-sheep technology can be reformed and put to work helping legitimate content owners and Internet-backbone operators deliver more video without overloading the network.

For Zhang and other P2P proponents, it's all a question of architecture. Conventionally, video and other Web content gets to consumers along paths that resemble trees, with the content owners' central servers as the trunks, multiple ""content distribution servers"" as the branches, and consumers' PCs as the leaves. Tree architectures work well enough, but they have three key weaknesses: If one branch is cut off, all its leaves go with it. Data flows in only one direction, so the leaves'--the PCs'--capacity to upload data goes untapped. And perhaps most important, adding new PCs to the network merely increases its congestion--and the demands placed on the servers.

In P2P networks, by contrast, there are no central servers: each user's PC exchanges data with many others in an ever-shifting mesh. This means that servers and their overtaxed network connections bear less of a burden; data is instead provided by peers, saving bandwidth in the Internet's core. If one user leaves the mesh, others can easily fill the gap. And adding users actually increases a P2P network's power.

There are just two big snags keeping content distributors and their ISPs from warming to mesh architectures. First, to balance the load on individual PCs, the most advanced P2P networks, such as BitTorrent, break big files into blocks, which are scattered across many machines. To re­assemble those blocks, a computer on the network must use precious bandwidth to broadcast ""metadata"" describing which blocks it needs and which it already has.

Second, ISPs are loath to carry P2P traffic, because it's a big money-loser. For conventional one-way transfers, ISPs can charge content owners such as Google or NBC.com according to the amount of bandwidth they consume. But P2P traffic is generated by subscribers themselves, who usually pay a flat monthly fee regardless of how much data they download or upload.

Zhang and others believe they're close to solving both problems. At Cornell University, computer scientist Paul Francis is testing a P2P system called Chunkyspread that combines the best features of trees and meshes. Members' PCs are arranged in a classic tree, but they can also connect to one another, reducing the burden on the branches.

Just as important, Chunkyspread reassembles files in ""slices"" rather than blocks. A slice consists of the nth bit of every block--for example, the fifth bit in every block of 20 bits. Alice's PC might obtain a commitment from Bob's PC to send bit five from every block it possesses, from Carol's PC to send bit six, and so forth. Once these commitments are made, no more metadata need change hands, saving bandwidth. In simulations, Francis says, Chunkyspread far outperforms simple tree-based multicast methods.

Zhang thinks new technology can also make carrying P2P traffic more palatable for ISPs. Right now, opera­tors have little idea what kind of data flows through their networks. At his Pittsburgh-based stealth startup, Rinera Networks, Zhang is developing software that will identify P2P data, let ISPs decide how much of it they're willing to carry, at what volume and price, and then deliver it as reliably as server-based content distribution systems do--all while tracking everything for accounting purposes. ""We want to build an ecosystem such that service providers will actually benefit­ from P2P traffic,"" Zhang explains. Heavy P2P users might end up paying extra fees--but in the end, content owners and consumers won't gripe, he argues, since better accounting should make the Internet function more effectively for everyone.

If this smells like a violation of the Internet's tradition of network neu­trality­--­­­­­­­­­­­­­­­­­­­­­
the­ principle that ISPs should treat all bits equally, regardless of their origin--then it's because the tradition needs to be updated for an era of very large file transfers, Zhang believes. ""It's all about volume,"" he says. ""Of course, we don't want the service providers to dictate what they will carry on their infra­structure. On the other hand, if P2P users benefit from transmitting and receiving more bits, the guys who are actually transporting those bits should be able to share in that.""

Networking and hardware companies have their eyes on technologies emerging from places like Rinera and Francis's Cornell lab, even as they build devices designed to help consumers download video and other files over P2P networks. Manufacturers Asus, Planex, and QNAP, for example, are working with BitTorrent to embed the company's P2P software in their home routers, media servers, and storage devices. With luck, ­Senator ­Stevens's tubes may stay unblocked a little longer.

by Wade Roush",1-10%,Positive,Software,concept
Nano Charging Solar,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Nanocharging Solar
Arthur Nozik believes quantum-dot solar power could boost output in cheap photovoltaics.

Arthur Nozik hopes quantum dots will enable the production of more efficient and less expensive solar cells, finally making solar power competitive with other sources of electricty.
LANCE W. CLAYTON
No renewable power source has as much theoretical potential as solar energy. But the promise of cheap and abundant solar power remains unmet, largely because today’s solar cells are so costly to make.

Photovoltaic cells use semiconductors to convert light energy into electrical current. The workhorse photo­voltaic material, silicon, performs this conversion fairly efficiently, but silicon cells are relatively expensive to manufacture. Some other semiconductors, which can be deposited as thin films, have reached market, but although they’re cheaper, their efficiency doesn’t compare to that of silicon. A new solution may be in the offing: some chemists think that quantum dots–tiny crystals of semi­conductors just a few nanometers wide–could at last make solar power cost-competitive with electricity from fossil fuels.

By dint of their size, quantum dots have unique abilities to interact with light. In silicon, one photon of light frees one electron from its atomic orbit. In the late 1990s, Arthur Nozik, a senior research fellow at the National Renewable Energy Laboratory in Golden, CO, postulated that quantum dots of certain semiconductor materials­ could release two or more electrons when struck by high-energy photons, such as those found toward the blue and ultraviolet end of the spectrum.

In 2004, Victor Klimov of Los Alamos­ National Laboratory in New Mexico provided the first experimental proof that Nozik was right; last year he showed that quantum dots of lead selenide could produce up to seven electrons per photon when exposed to high-energy ultraviolet light. Nozik’s team soon demonstrated the effect in dots made of other semiconductors, such as lead sulfide and lead telluride.

These experiments have not yet produced a material suitable for commercialization, but they do suggest that quantum dots could someday increase the efficiency of converting sunlight into electricity. And since quantum dots can be made using simple chemical reactions, they could also make solar cells far less expensive. Researchers in Nozik’s lab, whose results have not been published, recently demonstrated the extra-electron effect in quantum dots made of silicon; these dots would be far less costly to incorporate into solar cells than the large crystalline sheets of silicon used today.

To date, the extra-electron effect has been seen only in isolated quantum dots; it was not evident in the first proto­type photovoltaic devices to use the dots. The trouble is that in a working solar cell, electrons must travel out of the semiconductor and into an external electrical circuit. Some of the electrons freed in any photovoltaic cell are inevitably “lost,” recaptured by positive “holes” in the semiconductor. In quantum dots, this recapture happens far faster than it does in larger pieces of a semiconductor; many of the freed electrons are immediately swallowed up.

The Nozik team’s best quantum­-dot solar cells have managed only about 2 percent efficiency, far less than is needed for a practical device. However, the group hopes to boost the efficiency by modifying the surfaces of the quantum dots or improving electron transport between dots.

The project is a gamble, and Nozik readily admits that it might not pay off. Still, the enormous potential of the nanocrystals keeps him going. Nozik calculates that a photovoltaic device based on quantum dots could have a maximum efficiency of 42 percent, far better than silicon’s maximum efficiency of 31 percent. The quantum dots themselves would be cheap to manufacture, and they could do their work in combination with materials like conducting polymers that could also be produced inexpensively. A working quantum dot-polymer cell could eventually place solar electricity on a nearly even economic footing with electricity from coal. “If you could [do this], you would be in Stockholm–it would be revolutionary,” says Nozik.

A commercial quantum-dot solar cell is many years away, assuming it’s even possible. But if it is, it could help put our fossil-fuel days behind us.

by David Talbot",0.01% - 0.1%,positive,Nanotech,project
Neuron Control,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Neuron Control
Karl Deisseroth’s genetically engineered “light switch,” which lets scientists turn selected parts of the brain on and off, may help improve treatments for depression and other disorders.

ELAINE KURIE
In his psychiatry practice at the Stanford Medical Center, Karl Deisseroth­ sometimes treats patients who are so severely depressed that they can’t walk, talk, or eat. Intensive treatments, such as electro­convulsive therapy, can literally save such patients’ lives, but often at the cost of memory loss, headaches, and other serious side effects. Deisseroth, who is both a physician and a bioengineer, thinks he has a better way: an elegant new method for controlling neural cells with flashes of light. The technology could one day lead to precisely targeted treatments for psychiatric and neurological dis­orders; that precision could mean greater effectiveness and fewer side effects.

While scientists know something about the chemical imbalances underlying depression, it’s still unclear exactly which cells, or networks of cells, are responsible for it. In order to identify the circuits involved in such diseases, scientists must be able to turn neurons on and off. Standard methods­, such as electrodes that activate neurons with jolts of electricity, are not precise enough for this task, so Deisseroth, postdoc Ed Boyden­ (now an assistant professor at MIT; see “Engineering the Brain”), and graduate student Feng Zhang developed a neural controller that can activate specific sets of neurons.

They adapted a protein from a green alga to act as an “on switch” that neurons can be genetically engineered to produce (see “Artificially Firing Neurons,” TR35, September/October 2006). When the neuron is exposed to light, the protein triggers electrical activity within the cell that spreads to the next neuron in the circuit. Researchers can thus use light to activate certain neurons and look for specific responses–a twitch of a muscle, increased energy, or a wave of activity in a different part of the brain.

Deisseroth is using this genetic light switch to study the biological basis of depression. Working with a group of rats that show symptoms similar to those seen in depressed humans, researchers in his lab have inserted the switch into neurons in different brain areas implicated in depression. They then use an optical fiber to shine light onto those cells, looking for activity patterns that alleviate the symptoms. Deisseroth says the findings should help scientists develop better anti­depressants: if they know exactly which cells to target, they can look for molecules or delivery systems that affect only those cells. “Prozac goes to all the circuits in the brain, rather than just the relevant ones,” he says. “That’s part of the reason it has so many side effects.”

In the last year, Deisseroth has sent his switch to more than 100 research labs. “Folks are applying it to all kinds of animals, including mice, worms, flies, and zebrafish,” he says. Scientists are using this and similar switches to study everything from movement to addiction to appetite. “These technologies allow us to advance from observation to active intervention and control,” says Gero Miesenböck, a neuroscientist at Yale University. By evoking sensations or movements directly, he says, “you can forge a much stronger connection between mental activity and behavior.”

Deisseroth hopes his technology will one day become not just a research tool but a treatment in itself, used alongside therapies that electrically stimulate large areas of the brain to treat depression or Parkinson’s disease. By activating only specific neurons, a specially engineered light switch could limit those therapies’ side effects. Of course, the researchers will need to solve some problems first: they’ll need to find safe gene-therapy methods for delivering the switch to the target cells, as well as a way to shine light deep into the brain. “It’s a long way off,” says Deisseroth. “But the obstacles aren’t insurmountable.” In the meantime, neuroscientists have the use of a powerful new tool in their quest to uncover the secrets of the brain.

by Emily Singer",0.1% - 1.0%,Positive,Biotech,methodology
Nano Healing,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Nanohealing
Tiny fibers will save lives by stopping bleeding and aiding recovery from brain injury, says Rutledge Ellis-Behnke.

Rutledge Ellis-Behnke
ASIA KEPKA
In the break room near his lab in MIT’s brand-new neuroscience building, research scientist Rutledge Ellis-Behnke provides impromptu narration for a video of himself performing surgery. In the video, Ellis-Behnke makes a deep cut in the liver of a rat, intentionally slicing through a main artery­. As the liver pulses from the pressure of the rat’s beating heart, blood spills from the wound. Then Ellis­-­Behnke covers the wound with a clear liquid, and the bleeding stops almost at once. Untreated, the wound would have proved fatal, but the rat lived on.

The liquid Ellis-Behnke used is a novel material made of nanoscale protein fragments, or peptides. Its ability to stop bleeding almost instantly could be invaluable in surgery, at accident sites, or on the battlefield. Under conditions like those inside the body, the peptides self-assemble into a fibrous mesh that to the naked eye appears to be a transparent gel. Even more remarkably, the material creates an environment that may accelerate healing of damaged brain and spinal tissue.

Ellis-Behnke stumbled on the material’s capacity to stanch bleeding by chance, during experiments designed to help restore vision to brain-­damaged hamsters. And his discovery was itself made possible by earlier serendipitous events. In the early 1990s, Shuguang Zhang, now a biomedical engineer at MIT, was working in the lab of MIT biologist Alexander Rich. Zhang had been studying a repeating DNA sequence that coded for a peptide. He and a colleague inadvertently found that under certain conditions, copies of the peptide would combine into fibers. Zhang and his colleagues began to reëngineer the peptides to exhibit specific responses to electric charges and water. They ended up with a 16-amino-acid peptide that looks like a comb, with water-loving teeth projecting from a water-repelling spine. In a salty, aqueous environment–such as that inside the body–the spines spontaneously cluster together to avoid the water, forming long, thin fibers that self-assemble into curved ribbons. The process transforms a liquid peptide solution into a clear gel.

Originally, Ellis-Behnke intended to use the material to promote the healing of brain and spinal-cord injuries. In young animals, neurons are surrounded by materials that help them grow; Ellis-Behnke thought that the peptide gel could create a similar environment and prevent the formation of scar tissue, which obstructs the regrowth of severed neurons. “It’s like if you’re walking through a field of wheat, you can walk easily because the wheat moves out of the way,” he says. “If you’re walking through a briar patch, you get stuck.” In the hamster experiments, the researchers found that the gel allowed neurons in a vision-related tract of the brain to grow across a lesion and reëstablish connections with neurons on the other side, restoring the hamster’s sight.

It was during these experiments that Ellis-Behnke discovered the gel’s ability to stanch bleeding. Incisions had been made in the hamsters’ brains, but when the researchers applied the new material, all residual bleeding suddenly stopped. At first, Ellis-Behnke says, “we thought that we’d actually killed the animals. But the heart was still going.” Indeed, the rodents survived for months, apparently free of negative side effects.

The material has several advantages over current methods for stopping bleeding. It’s faster and easier than cauterization and does not damage tissue. It could protect wounds from the air and supply amino-acid building blocks to growing cells, thereby accelerating healing. Also, within a few weeks the body completely breaks the peptides down, so they need not be removed from the wound, unlike some other blood-stanching agents. The synthetic material also has a long shelf life, which could make it particularly useful in first-aid kits.

The material’s first application will probably come in the operating room. Not only would it stop the bleeding caused by surgical incisions, but it could also form a protective layer over wounds. And since the new material is transparent, surgeons should be able to apply a layer of it and then operate through it. “When you perform surgery, you are constantly suctioning and cleaning the site to be able to see it,” says Ram Chuttani, a gastroenterologist and professor at Harvard Medical School. “But if you can seal it, you can continue to perform the surgery with much clearer vision.” The hope is that surgeons will be able to operate faster, thus reducing complications. The material may also make it possible­ to perform more procedures in a minimally invasive way by allowing a surgeon to quickly stop bleeding at the end of an endoscope.

Chuttani, who was not involved with the research, cautions that the work is still “very preliminary,” with no tests yet on large animals or humans. But if such tests go well, Ellis-Behnke estimates, the material could be approved for use in humans in three to five years. “I don’t know what the impact is going to be,” he says. “But if we can stop bleeding, we can save a lot of people.” Ellis-Behnke and his colleagues are also continuing to explore the material’s nerve regeneration capabilities. They’re looking for ways to increase the rate of neuronal growth so that doctors can treat larger brain injuries, such as those that can result from stroke. But such a treatment will take at least five to ten years to reach humans, Ellis-Behnke says.

Even without regenerating nerves, the material could save countless lives in surgery or at accident sites. And already, the material’s performance is encouraging research by demonstrating how engineering nanostructures to self-assemble in the body could profoundly improve medicine.

by Kevin Bullis",0.01% - 0.1%,Positive,Nanotech,product
Augmented Reality,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Augmented Reality
Markus Kähäri wants to superimpose digital information on the real world.

Boxes appear on the phone’s screen, highlighting known businesses and landmarks, such as the Empire State Building. The user can click one of these boxes to download information about that location from the Web. In Nokia’s mobile-augmented-reality prototype, a user can point a phone’s camera at a nearby building; the system calculates the building’s location and uses that information to identify it.
JEAN PROBERT
Finding your way around a new city can be exasperating: juggling maps and guidebooks, trying to figure out where you are on roads with no street signs, talking with locals who give directions by referring to unfamiliar landmarks. If you’re driving, a car with a GPS navigation system can make things easier, but it still won’t help you decide, say, which restaurant suits both your palate and your budget. Engineers at the Nokia Research Center in Helsinki, Finland­, hope that a project called Mobile Augmented Reality Applications will help you get where you’re going–and decide what to do once you’re there.

Last October, a team led by Markus Kähäri unveiled a proto­type of the system at the International Symposium on Mixed and Augmented Reality. The team added a GPS sensor, a compass, and accelerometers to a Nokia smart phone. Using data from these sensors, the phone can calculate the location of just about any object its camera is aimed at. Each time the phone changes location, it retrieves the names and geographical coördinates of nearby landmarks from an external database. The user can then download additional information about a chosen location from the Web–say, the names of businesses in the Empire State Building, the cost of visiting the building’s observatories, or hours and menus for its five eateries.

The Nokia project builds on more than a decade of academic research into mobile augmented reality. Steven Feiner, the director of Columbia University’s Computer Graphics and User Interfaces Laboratory, undertook some of the earliest research in the field and finds the Nokia project heartening. “The big missing link when I started was a small computer,” he says. “Those small computers are now cell phones.”

Despite the availability and fairly low cost of the sensors the Nokia team used, some engineers believe that they introduce too much complexity for a commercial application. “In my opinion, this is very exotic hardware to provide,” says Valentin Lefevre, chief technology officer and cofounder of Total Immersion­, an augmented-reality company in Suresnes, France. “That’s why we think picture analysis is the solution.” Relying on software alone, Total Immersion’s­ system begins with a single still image of whatever object the camera is aimed at, plus a rough digital model of that object; image-­recognition algorithms then determine what data should be super­imposed on the image. The company is already marketing a mobile version of its system to cell-phone operators in Asia and Europe and expects the system’s first applications to be in gaming and advertising.

Nokia researchers have begun working on real-time image-recognition algorithms as well; they hope the algorithms will eliminate the need for location sensors and improve their system’s accuracy and reliability. “Methods that don’t rely on those components can be more robust,” says Kari Pulli, a research fellow at the Nokia Research Center in Palo Alto, CA.

All parties agree, though, that mobile augmented reality is nearly ready for the market. “For mobile-phone applications, the technology is here,” says Feiner. One challenge is convincing carriers such as Sprint or Verizon that customers would pay for augmented-reality services. “If some big operator in the U.S. would launch this, it could fly today,” Pulli says.

by Erika Jonietz",0.1% - 1.0%,Positive,Software,Platform
Invisible Revolution,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Invisible Revolution
Artificially structured metamaterials could transform telecommunications, data storage, and even solar energy, says David R. Smith.

David R. Smith led the team that built the world’s first “invisibility shield” (above). The shield consists of concentric circles of fiberglass circuit boards, printed with C-shaped split rings. Microwaves of a particular frequency behave as if objects inside the cylinder aren’t there--but everything remains in plain view.
DAVID DEAL
The announcement last November of an “invisibility shield,” created by David R. Smith of Duke University and colleagues, inevitably set the media buzzing with talk of H. G. Wells’s invisible man and Star Trek’s Romulans. Using rings of printed circuit boards, the researchers managed to divert microwaves around a kind of “hole in space”; even when a metal cylinder was placed at the center of the hole, the microwaves behaved as though nothing were there.

It was arguably the most dramatic demonstration so far of what can be achieved with metamaterials, composites made up of precisely arranged patterns of two or more distinct materials. These structures can manipulate electro­magnetic radiation, including light, in ways not readily observed in nature. For example, photonic crystals–arrays of identical microscopic blocks separated by voids–can reflect or even inhibit the propagation of certain wavelengths of light; assemblies of small wire circuits, like those Smith used in his invisibility shield, can bend light in strange ways.

But can we really use such materials to make objects seem to vanish? Philip Ball spoke with Smith, who explains why metamaterials are literally changing the way we view the world.

Technology Review: How do metamaterials let you make things invisible?

David R. Smith: It’s a somewhat complicated procedure but can be very simple to visualize. Picture a fabric formed from interwoven threads, in which light is constrained to travel along the threads. Well, if you now take a pin and push it through the fabric, the threads are distorted, making a hole in the fabric. Light, forced to follow the threads, is routed around the hole. John Pendry at Imperial College in London calculated what would be required of a meta­material that would accomplish exactly this. The waves are transmitted around the hole and combined on the other side. So you can put an object in the hole, and the waves won’t “see” it–it’s as if they’d crossed a region of empty space.

TR: And then you made it?

DRS: Yes–once we had the prescription, we set about using the techniques we’d developed over the past few years to make the material. We did the experiment at microwave frequencies because the techniques are very well established there and we knew we would be able to produce a demonstration quickly. We printed millimeter­-scale metal wires and split rings, shaped like the letter C, onto fiberglass circuit boards. The shield consisted of about 10 concentric cylinders made up of these split-ring building blocks, each with a slightly different pattern.

TR: So an object inside the shield is actually invisible?

DRS: More or less, but when we talk about invisibility in these structures, it’s not about making things vanish before our eyes–at least, not yet. We can hide them from microwaves, but the shield is plain enough to see. This isn’t like stealth shielding on military aircraft, where you just try to eliminate reflection–the microwaves seem literally to pass through the object inside the shield. If this could work with visible light, then you really would see the object vanish.

TR: Could you hide a large object, like an airplane, from radar by covering its surface with the right metamaterial?

DRS: I’m not sure we can do that. If you look at stealth technology today, it’s generally interested in hiding objects from detection over a large radar bandwidth. But the invisibility bandwidth is inherently limited in our approach. The same is true for hiding objects from all wavelengths of visible light–that would certainly be a stretch.

TR: How else might we use metamaterials?

DRS: Well, this is really an entirely new approach to optics. There’s a huge amount of freedom for design, and as is usual with new technology, the best uses probably haven’t been thought of yet.

One of the most provocative and controversial predictions came from John Pendry, who predicted that a material with a negative refractive index could focus light more finely than any conventional lens material. The refractive index measures how much light bends when it passes through a material–that’s what makes a pole dipped in water look as though it bends. A negative refractive index means the material bends light the “wrong” way. So far, we and others have been working not with visible light but with microwaves, which are also electro­magnetic radiation, but with a longer wavelength. This means the components of the metamaterial must be correspondingly bigger, and so they’re much easier to make. Pendry’s suggestion was confirmed in 2005 by a group from the University of California, Berkeley, who made a negative­-­­­­­­­refractive-index meta­material for microwaves.

Making a negative-index material that works for visible light is more difficult, because the building blocks have to be much smaller–no bigger than 10 to 20 nanometers. That’s now very possible to achieve, however, and several groups are working on it. If it can be done, these metamaterials could be used to increase the amount of information stored on CDs and DVDs or to speed up transmission and reduce power consumption in fiber-optic telecommunications.

We can also concentrate electro­magnetic fields–the exact opposite of what the cloak does–which might be valuable in energy-harvesting applications. With a suitable metamaterial, we could concentrate light coming from any direction–you wouldn’t need direct sunlight. Right now we’re trying to design structures like this. If we could achieve that for visible light, it could make solar power more efficient.

by Philip Ball",1-10%,Positive,Software,concept
Digital Imaging Reimagined,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Digital Imaging, Reimagined
Richard Baraniuk and Kevin Kelly believe compressive sensing could help devices such as cameras and medical scanners capture images more efficiently.

JOHN MACNEIL
Richard Baraniuk and Kevin Kelly have a new vision for digital imaging: they believe an overhaul of both hardware and software could make cameras smaller and faster and let them take incredi­bly high-resolution pictures.

Today’s digital cameras closely mimic film cameras, which makes them grossly inefficient. When a standard four-megapixel digital camera snaps a shot, each of its four million image sensors characterizes the light striking it with a single number; together, the numbers describe a picture. Then the camera’s onboard computer compresses the picture, throwing out most of those numbers. This process needlessly chews through the camera’s battery.

Baraniuk and Kelly, both professors of electrical and computer engineering at Rice University, have developed a camera that doesn’t need to compress images. Instead, it uses a single image sensor to collect just enough information to let a novel algorithm reconstruct a high-resolution image.

At the heart of this camera is a new technique called compressive sensing. A camera using the technique needs only a small percentage of the data that today’s digital cameras must collect in order to build a comparable picture. Baraniuk and Kelly’s algorithm turns visual data into a handful of numbers that it randomly inserts into a giant grid. There are just enough numbers to enable the algorithm to fill in the blanks, as we do when we solve a Sudoku puzzle. When the computer solves this puzzle, it has effectively re-created the complete picture from incomplete information.

Compressive sensing began as a mathematical theory whose first proofs were published in 2004; the Rice group has produced an advanced demonstration in a relatively short time, says Dave Brady of Duke University. “They’ve really pushed the applications of the theory,” he says.

Kelly suspects that we could see the first practical applications of compressive sensing within two years, in MRI systems that capture images up to 10 times as quickly as today’s scanners do. In five to ten years, he says, the technology could find its way into consumer products, allowing tiny mobile-phone cameras to produce high-quality, poster-size images. As our world becomes increasingly digital, compressive sensing is set to improve virtually any imaging system, providing an efficient and elegant way to get the picture.

by Kate Greene",2-20%,Positive,Software,Concept
Personalized Medical Monitors,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Personalized Medical Monitors
John Guttag says using computers to automate some diagnostics could make medicine more personal.

John Guttag believes that computers can improve diagnostic tests and make medicine more personal by automating the interpretation of complex medical data such as the brain wave tracings shown above, or electrocardiogram readings from heart patients.
MAX AGUILERA-HELLWEG
In late spring 2000, John Guttag came home from surgery. It had been a simple procedure to repair a torn liga­ment in his knee, and he had no plans to revisit the hospital anytime soon. But that same day his son, then a junior in high school, complained of chest pains. Guttag’s wife promptly got back in the car and returned to the hospital, where their son was diagnosed with a collapsed lung and immediately admitted. Over the next year, Guttag and his wife spent weeks at a time in and out of the hospital with their son, who underwent multiple surgeries and treatments for a series of recurrences.

During that time, Guttag witnessed what became a familiar scenario. “The doctors would come in, take a stethoscope, listen to his lungs, and make a pronouncement like ‘He’s 10 percent better than yesterday,’ and I wanted to say, ‘I don’t believe that,’” he says. “You can’t possibly sit there and listen with your ears and tell me you can hear a 10 percent difference. Surely there’s a way to do this more precisely.”

It was an observation that any concerned parent might make, but for Guttag, who was then head of MIT’s Department of Electrical Engineering and Computer Science, it was a personal challenge. “Health care just seemed like an area that was tremendously in need of our expertise,” he says.

The ripest challenge, Guttag says, is analyzing the huge amounts of data generated by medical tests. Today’s physicians are bombarded with physio­logical information–temperature and blood pressure readings, MRI scans, electrocardiogram (EKG) readouts, and x-rays, to name a few. Wading through a single patient’s record to determine signs of, say, a heart attack or stroke can be difficult and time consuming. Guttag­ believes computers can help doctors efficiently interpret these ever-­growing masses of data. By quickly perceiving patterns that might otherwise be buried, he says, software may provide the key to more precise and personalized medicine. “People aren’t good at spotting trends unless they’re very obvious,” says Guttag. “It dawned on me that doctors were doing things that a computer could do better.”

For instance, making sense of the body’s electrical signals seemed, to Guttag­, to be a natural fit for computer science. Some of his earlier work on computer networks caught the attention of physicians at Children’s Hospital Boston. The doctors and the engineer set out to improve the detection of epileptic seizures; ultimately, Guttag and graduate student Ali Shoeb designed personalized seizure detectors. In 2004, the team examined recordings of the brain waves of more than 30 children with epilepsy, before, during, and after seizures. They used the data to train a “classification algorithm” to distinguish between seizure and nonseizure waveforms. With the help of the algorithm, the researchers identified seizure patterns specific to each patient.

The team is now working on a way to make that type of information useful to people with epilepsy. Today, many patients can control their seizures with an implant that stimulates the vagus nerve. The implant typically works in one of two ways: either it turns on every few minutes, regardless of a patient’s brain activity, or patients sweep a magnet over it, activating it when they sense a seizure coming on. Both methods have their drawbacks, so Guttag is designing a noninvasive, software-driven sensor programmed to measure the wearer’s brain waves and determine what patterns–specific to him or her–signify the onset of a seizure. Once those patterns are detected, a device can automatically activate an implant, stopping the seizure in its tracks.

Guttag plans to test the sensor, essentially a bathing cap of electrodes that fits over the scalp, on a handful of patients at Beth Israel Deaconess Medical Center this spring. Down the line, such a sensor could also help people without implants, simply warning them to sit down, pull over, or get to a safe place before a seizure begins. “Just a warning could be enormously life changing,” says Guttag. “It’s all the collateral damage that people really fear.”

Now he’s turned his attention to patterns of the heart. Like the brain, cardiac activity is governed by electrical signals, so moving into cardiology is a natural transition for Guttag.

He began by looking for areas where large-scale cardiac-data analysis was needed. Today, many patients who have suffered heart attacks go home with Holter monitors that record heart activity. After a day or so, a cardiologist reviews the monitor’s readings for worri­some signs. But it can be easy to miss an abnormal pattern in thousands of minutes of dense waveforms.

That’s where Guttag hopes computers­ can step in. Working with ­Collin Stultz, a cardiologist and assistant professor of electrical engineering and computer science at MIT, and graduate student Zeeshan Syed, Guttag is devising algorithms to analyze EKG readings for statistically meaningful patterns. In the coming months, the team will compare EKG records from hundreds of heart attack patients, some of whose attacks were fatal. The immediate goal is to pick out key similarities and differences between those who survived and those who didn’t. There are known “danger patterns” that physicians can spot on an EKG readout, but the Guttag­ group is leaving it up to the computer to find significant patterns, rather than telling it what to look for. If the computer’s search isn’t influenced by existing medical knowledge, Guttag­ reasons, it may uncover un­expected relationships.

Joseph Kannry, director of the Center for Medical Informatics at the Mount Sinai School of Medicine, calls Guttag’s work a solid step toward developing more accurate automated medical readings. “It’s promising. The challenge is going to be in convincing a clinician to use it,” says Kannry.

Still, Guttag feels he is well on his way toward integrating computing into medical diagnostics. “People have very different reactions when you tell them computers are going to make decisions for you,” he says. “But we’ve gotten to the point where computers fly our airplanes for us, so there’s every reason to be optimistic.”

by Jennifer Chu",0.1% to 1%,Positive,Biotech,product
A New Focus for Light,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"A New Focus for Light
Kenneth Crozier and Federico Capasso have created light-focusing optical antennas that could lead to DVDs that hold hundreds of movies.

JOHN HERSEY
Researchers trying to make high-capacity DVDs, as well as more-powerful computer chips and higher-resolution optical microscopes, have for years run up against the “diffraction limit.” The laws of physics dictate that the lenses used to direct light beams cannot focus them onto a spot whose diameter is less than half the light’s wavelength. Physicists have been able to get around the diffraction limit in the lab–but the systems they’ve devised have been too fragile and complicated for practical use. Now Harvard University electrical engineers led by Kenneth Crozier and Federico Capasso have discovered a simple process that could bring the benefits of tightly focused light beams to commercial applications. By adding nanoscale “optical antennas” to a commercially available laser, Crozier­ and Capasso have focused infrared light onto a spot just 40 nanometers wide–one-­twentieth the light’s wavelength. Such optical antennas could one day make possible DVD-like discs that store 3.6 terabytes of data–the equivalent of more than 750 of today’s 4.7-gigabyte recordable DVDs.

Crozier and Capasso build their device by first depositing an insulating layer onto the light-emitting edge of the laser. Then they add a layer of gold. They carve away most of the gold, leaving two rectangles of only 130 by 50 nano­meters, with a 30-­nanometer gap between them. These form an antenna. When light from the laser strikes the rectangles, the antenna has what Capasso calls a “lightning­-rod effect”: an intense electrical field forms in the gap, concentrating the laser’s light onto a spot the same width as the gap.

“The antenna doesn’t impose design constraints on the laser,” Capasso says, because it can be added to off-the-shelf semiconductor lasers, commonly used in CD drives. The team has already demonstrated the antennas with several types of lasers, each producing a different wavelength of light. The researchers­ have discussed the technology with storage-device companies Seagate and Hitachi Global Storage Technologies.

Another application could be in photo­lithography, says ­Gordon Kino, professor emeritus of electrical engineering at Stanford University. This is the method typically used to make silicon chips, but the lasers that carve out ever-smaller features on silicon are also constrained by the diffraction limit. Electron-beam lithography, the technique that currently allows for the smallest chip features, requires a large machine that costs millions of dollars and is too slow to be used in mass production. “This is a hell of a lot simpler,” says Kino of Crozier and Capasso’s technique, which relies on a laser that costs about $50.

But before the antennas can be used for lithography, the engineers will need to make them even smaller: the size of the antennas must be tailored to the wavelength of the light they focus. Crozier­ and Capasso’s experiments have used infrared lasers, and photo­lithography relies on shorter-wavelength ultraviolet light. In order to inscribe circuitry on microchips, the researchers must create antennas just 50 nanometers long.

Capasso and Crozier’s optical antennas could have far-reaching and un­predictable implications, from superdense optical storage to ­superhigh-resolution optical microscopes. Enabling engineers to simply and cheaply break the diffraction limit has made the many applications that rely on light shine that much brighter.

by Katherine Bourzac",0.05-0.5%,Positive,Nanotech,Project
Single-Cell Analysis,2007.0,https://www.technologyreview.com/10-breakthrough-technologies/2007/,,"Single-Cell Analysis
Norman Dovichi believes that detecting minute differences between individual cells could improve medical tests and treatments.

Analyzing individual cells allows researchers to distinguish between a uniform population of cells (above left) and a group of cells with members having, say, different protein content (above right). The ability to recognize such differences could be essential to understanding diseases such as cancer or diabetes.
ELAINE KURIE
We all know that focusing on the characteristics of a group can obscure the differences between the individuals in it. Yet when it comes to biological cells, scientists typically derive information about their behavior, status, and health from the collective activity of thousands or millions of them. A more precise understanding of differences between individual cells could lead to better treatments for cancer and diabetes, just for starters.

The past few decades have seen the advent of methods that allow astonishingly detailed views of single cells–each of which can produce thousands of different proteins, lipids, hormones, and metabolites. But most of those methods have a stark limitation: they rely on “affinity reagents,” such as anti­bodies that attach to specific proteins. As a result, researchers can use them to study only what’s known to exist. “The unexpected is invisible,” says Norman Dovichi, an analytical chemist at the University of Washington, Seattle. And most every cell is stuffed with mysterious components. So Dovichi­ has helped pioneer ultrasensitive­ techniques to isolate cells and reveal molecules inside them that no one even knew were there.

Dovichi’s lab–one of a rapidly growing number of groups that focus on single cells–has had particular success at identifying differences in the amounts of dozens of distinct proteins produced by individual cancer cells. “Ten years ago, I would have thought it would have been almost impossible to do that,” says Robert Kennedy, an analytical chemist at the University of Michigan-Ann Arbor, who analyzes insulin secretion from single cells to uncover the causes of the most common type of diabetes.

And Dovichi has a provocative hypothesis: he thinks that as a cancer progresses, cells of the same type diverge more and more widely in their protein content. If this proves true, then vast dissimilarities between cells would indicate a disease that is more likely to spread. Dovichi is working with clinicians to develop better prognostics for esophageal and breast cancer based on this idea. Ultimately, such tests could let doctors quickly decide on proper treatment, a key to defeating many cancers.

A yellow, diamond-shaped sign in Dovichi’s office warns that a “laser jock” is present. Dovichi helped develop the laser-based DNA sequencers that became the foundation of the Human Genome Project, and his new analyzers rely on much of the same technology to probe single cells for components that are much harder to detect than DNA: proteins, lipids, and carbohydrates.

For proteins, the machines mix reagents with a single cell inside an ultrathin capillary tube. A chemical reaction causes lysine, an amino acid recurring frequently in proteins, to fluoresce. The proteins, prodded by an electric charge, migrate out of the tube at different rates, depending on their size. Finally, a laser detector records the intensity of the fluor­escence. This leads to a graphic that displays the various amounts of the different­-­sized proteins inside the cell.

Although the technique reveals differences between cells, it does not identify the specific proteins. Still, the analyzer has an unprecedented sensitivity and makes visible potentially critical differences. “For our cancer prognosis projects, we don’t need to know the identity of the components,” Dovichi says.

Dovichi is both excited about the possibilities of single-cell biology and sober about its limitations. Right now, he says, analyses take too much time and effort. “This is way early-stage,” says Dovichi. “But hopefully, in 10, 20, or 30 years, people will look back and say those were interesting baby steps.”

by Jon Cohen",0.05% - 0.5%,Positive,Biotech,methodology
Atomic Magnetometers,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Atomic Magnetometers
John Kitching’s tiny magnetic-field sensors will take MRI where it’s never gone before.

Shrinking sensors: A completed magnetometer built by NIST physicists is shown above. It consists of a small infrared laser (glued to a gold-coated plate), the cesium-filled cell, and a light detector.
JIM YOST; COURTESY OF JOHN KITCHING
Magnetic fields are everywhere, from the human body to the metal in a buried land mine. Even molecules such as proteins generate their own distinctive magnetic fields. Both magnetic resonance imaging (MRI), which yields stunningly detailed images of the body, and nuclear magnetic resonance spectroscopy (NMR), which is used to study proteins and other compounds such as petroleum, rely on magnetic information. But the sensors currently used to detect these faint but significant magnetic fields all have disadvan­tages. Some are portable and cheap but not very sensitive; other­s are highly sensitive but stationary, expensive, and power-hungry.

Now John Kitching, a physicist at the National Institute of Standards and Technology in Boulder, CO, is developing tiny, low-power magnetic sensors almost as sensitive as their big, expensive counterparts. About the size of a fat grain of rice, the sensors are called atomic magnetometers. Kitching hopes that they will one day be incorporated into everything from portable MRI machines to faster and cheaper detectors for unexploded bombs.

The tiny sensors have three key components, stacked vertically on top of a silicon chip. An off-the-shelf infrared laser and an infrared photodetector sandwich a glass-and-silicon cube filled with vaporized cesium atoms. In the absence of a magnetic field, the laser light passes straight through the cesium atoms. In the presenc­e of even very weak magnetic fields, though, the atoms’ alignment changes, allowing them to absorb an amount of light proportional to the strength of the field. This change is picked up by the photodetector. “It’s a simple configuration with extremely good sensitivity,” Kitching says.

Atomic magnetometers have been around for about 50 years; most have large, sensitive vapor cells, about the size of soda cans, made using glassblowing techniques. The most sensitive of these can detect fields on the order of a femtotesla–about one-fifty-billionth the strength of Earth’s magnetic field. Kitching’s innovation was to shrink the vapor cell to a volume of only a few cubic millimeters, decreasing power usage while keeping performance comparable.

Working with five other physicists, Kitching makes the vapor cells using micromachining techniques. They begin by using a combination of lithography and chemical etching to punch square holes three millimeters across into a silicon wafer. Then they clamp the silicon to a slip of glass and create a bond using high heat and a voltage, turning the square hole into a topless box with a glass bottom.

Inside a vacuum chamber, they use a tiny glass syringe to fill the box with vaporized cesium atoms; then they seal the box with another slip of glass at high heat. (This must be done in a vacuum because cesium reacts vigorously with water and oxygen.) Next, the physicists mount the finished vapor cell on a chip, along with the infrared laser and the photodetector. They pass a current through thin conductive films on the top and bottom of the cell to produce heat, which keeps the cesium atoms vaporized.

Kitching currently builds magnetometers a few at a time in the lab, but he has designed them with bulk manufacturing in mind. Many copies of each component are carved out simultaneously from a single silicon wafer. Several wafers, each containing multiple copies of a different component, could be layered one on top of the other. Then the stack could be sliced into multiple magnetometers.

Made in this inexpensive way, the low-power sensors could be set into portable, battery-power­ed imaging arrays. Such arrays could easily map out the strength and extent of magnetic fields; the more sensors in an array, the more information it can provide about an object’s location and shape. Soldiers, for example, could use such arrays to find unexploded bombs and improvised explosive devices more quickly and cheaply.

The tiny sensors could also revolutionize MRI and NMR. Both technologies rely on powerful, cumbersome, expensive magnets that require costly cooling systems. Because Kitching’s­ sensors can detect very weak magnetic fields, MRI and NMR machines incorporating them might be able to get good pictures using a magnet that’s much weaker–and therefore smaller and cheaper.

As a result, MRI could become more widely available. And for the first time, doctors could use it to examine patients with pace­makers or other metallic implants that can’t be exposed to powerful magnets. Portable systems might even be developed for use in ambulances or on battlefields. And NMR could move from the lab into the field, where it could help oil and mining companies assess promising underground deposits.

Kitching and his colleagues recently showed that the sensors can measure NMR signals produced by water. Much remains to be done, Kitching says, before the devices can resolve faint signals from multiple chemical structures–distinguishing, say, between several possible trace contaminants in a water sample. Likewise, portable MRI machines will take some work. But with Kitching’s miniaturized magnetometers, the challenge will shift from gathering magnetic information to interpreting it.

by Katherine Bourzac",0.01% - 0.1%,Positive,Hardware,Product
Cellulolytic Enzymes,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Cellulolytic Enzymes
Frances Arnold is designing better enzymes for making biofuels from cellulose.

Frances Arnold
GREGG SEGAL
In December, President Bush signed the Energy Independence and Security Act of 2007, which calls for U.S. production of renewable fuels to reach 36 billion gallons a year–nearly five times current levels–by 2022. Of that total, cellulosic biofuels derived from sources such as agricultural waste, wood chips, and prairie grasses are supposed to account for 16 billion gallons. If the mandates are met, gasoline consumption should decline significantly, reducing both greenhouse-gas emissions and imports of foreign oil.

The ambitious plan faces a significant hurdle, however: no one has yet demonstrated a cost-competitive industrial process for making cellulosic biofuels. Today, nearly all the ethanol produced in the United States is made from the starch in corn kernels, which is easily broken down into the sugars that are fermented to make fuel. Making ethanol from cheaper sources will require an efficient way to free sugar molecules packed together to form crystalline chains of cellulose, the key structural component of plants. That’s “the most expensive limiting step right now for the large-scale commercialization of [cellulosic] biofuels,” says protein engineer Frances Arnold, a professor of chemical engineering and biochemistry at Caltech.

The key to more efficiently and cheaply breaking down cellulose, Arnold and many others believe, is better enzymes. And Arnold, who has spent the last two decades designing enzymes for use in everything from drugs to stain removers, is confident that she’s well on her way to finding them.

Cellulosic biofuels have many advantages over both gasoline and corn ethanol. Burning cellulosic ethanol rather than gasoline, for instance, could cut cars’ greenhouse-gas emissions by 87 percent; corn ethanol achieves reductions of just 18 to 28 percent. And cellulose is the most abundant organic material on earth.

But whereas converting cornstarch into sugar requires a single enzyme, breaking down cellulose involves a complex array of enzymes, called cellulases, that work together. In the past, cellulases found in fungi have been recruited to do the job, but they have proved too slow and unstable. Efforts to improve their performance by combining them in new ways or tweaking their constituent amino acids have been only moderately successful. Researchers have reduced the cost of industrial cellulolytic enzymes to 20 to 50 cents per gallon of ethanol produced. But the cost will have to fall to three or four cents per gallon for cellulosic ethanol to compete with corn ethanol.

Ultimately, Arnold wants to do more than just make cheaper, more efficient enzymes for breaking down cellulose. She wants to design cellulases that can be produced by the same microörganisms that ferment sugars into biofuel. Long a goal of researchers, “superbugs” that can both metabolize cellulose and create fuel could greatly lower the cost of producing cellulosic biofuels. “If you consolidate these two steps, then you get synergies that lower the cost of the overall process,” Arnold says.

Consolidating those steps will require cellulases that work in the robust organisms used in industrial fermentation processes–such as yeast and bacteria. The cellulases will need to be stable and highly active, and they’ll have to tolerate high sugar levels and function in the presence of contaminants. Moreover, researchers will have to be able to produce the organisms in sufficient quantities. This might seem like a tall order, but over the years, Arnold has developed a number of new tools for making novel proteins. She pioneered a technique, called directed evolution, that involves creating many variations of genes that code for specific proteins. The mutated genes are inserted into microörganisms that churn out the new proteins, which are then screened for particular characteristics.

Her latest strategy is a computational approach that can rapidly identify thousands of new protein sequences for screening. This approach generates many more sequence variants than other methods do, greatly increasing the chances of creating functional molecules with useful new properties.

Arnold is using the technique to build libraries containing thousands of new cellulase genes. She and her colleagues will then screen the cellulases to see how they act as part of a mixture of enzymes. “If you test them simply by themselves, you really don’t know how they work as a group,” she says.

To fulfill her ultimate goal of a superbug able to feed on cellulose and produce biofuels, Arnold is working with James Liao, a professor of chemical engineering at the University of California, Los Angeles. Liao recently engineered E. coli that can efficiently convert sugar into butanol, a higher-energy biofuel than ethanol. Arnold hopes to be able to incorporate her new enzymes into Liao’s ­butanol-­producing microbes. Gevo, a startup cofounded by Arnold and based in Denver, CO, has licensed Liao’s technology for use in the large-scale production of advanced biofuels, including butanol.

Overcoming cellulose’s natural resistance to being broken down is “one of the most challenging protein-engineering problems around,” says Arnold. Solving it will help determine whether low-­emission biofuels will ever be a viable substitute for fossil fuels.

by Alexandra M. Goho",0.1-1%,positive,Biotech,Methodology
Connectomics,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Connectomics
Jeff Lichtman hopes to elucidate brain development and disease with new technologies that illuminate the web of neural circuits.

BrainBows: Genetically engineering mice so that their brain cells express different combinations of fluorescent colors reveals the brain’s complicated anatomy. In the image round green neurons are interspersed with diffuse support cells called astrocytes.
JEAN LIVET
Displayed on Jeff Lichtman’s computer screen in his office at Harvard University is what appears to be an elegant drawing of a tree. Thin multicolored lines snake upward in parallel, then branch out in twos and threes, their tips capped by tiny leaves. Lichtman is a neuroscientist, and the image is the first comprehensive wiring diagram of part of the mammalian nervous system. The lines denote axons, the long, hairlike extensions of nerve cells that transmit signals from one neuron to the next; the leaves are synapses, the connections that the axons make with other neurons or muscle cells.

The diagram is the fruit of an emerging field called “connectomics,” which attempts to physically map the ­tangle of neural circuits that collect, ­process, and archive information in the nervous system. Such maps could ultimately shed light on the early development of the human brain and on diseases that may be linked to faulty wiring, such as autism and schizophrenia. “The brain is ­essentially a computer that wires itself up during development and can rewire itself,” says ­Sebastian Seung, a computational neuroscientist at MIT, who is working with Lichtman. “If we have a wiring diagram of the brain, that could help us understand how it works.”

Although researchers have been studying neural connectivity for decades, existing tools don’t offer the resolution needed to reveal how the brain works. In particular, scientists haven’t been able to generate a detailed picture of the hundreds of millions of neurons in the brain, or of the connections between them.

Lichtman’s technology–developed in collaboration with Jean Livet, a former postdoc in his lab, and Joshua Sanes, director of the Center for Brain Science at Harvard–paints nerve cells in nearly 100 colors, allowing scientists to see at a glance where each axon leads. Understanding this wiring should shed light on how information is processed and transferred between different brain areas.

To create their broad palette, ­Lichtman and his colleagues genetically engineered mice to carry multiple copies of genes for three proteins that fluoresce in different colors–yellow, red, or cyan. The mice also carry DNA encoding an enzyme that randomly rearranges these genes so that individual nerve cells produce an arbitrary combination of the fluorescent proteins, creating a rainbow of hues. Then the researchers use fluorescence microscopy to visualize the cells.

“This will be an incredibly powerful tool,” says Elly Nedivi, a neuroscientist at MIT who is not involved in the research. “It will open up huge opportunities in terms of looking at neural connectivity.”

Lichtman and others hope that the ability to study multiple neural circuits simultaneously and in depth will provide unprecedented insight into how the wiring of the nervous system can go awry. “There’s a whole class of disorders of the nervous system that people suspect are due to defects in the connections between nerve cells, but we don’t have real tools to trace the connections,” says Lichtman. “It would be very useful to look at wiring in animal models of autism-spectrum disorders or psychiatric illness.”

In experiments so far, Lichtman’s group has used the technology to trace all the connections in a small slice of the cerebellum, the part of the brain that controls balance and movement. Other scientists have already expressed interest in using the technology to study neural connections in the retina, the cortex, and the olfactory bulb, as well as in non-neural cell types.

Generating maps of even a small chunk of the brain will be a huge challenge: the human brain consists of an estimated 100 billion neurons, with trillions of synapses. Scientists will need to find ways to store, annotate, and mine the volumes of data they create, and to meld information about connectivity with findings about the molecular and physiological characteristics of neurons in the circuits. But now, at least, they have a key tool with which to begin the massive effort of creating a wiring diagram of the brain.

by Emily Singer",0.01% - 0.1%,Positive,Biotech,methodology
Graphene Transistors,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Graphene Transistors
A new form of carbon being pioneered by Walter de Heer of Georgia Tech could lead to speedy, compact computer processors.

MAXWELL GUBERMAN, GEORGIA TECH
The remarkable increases in computer speed over the last few decades could be approaching an end, in part because silicon is reaching its physical limits. But this past December, in a small Washington, DC, conference room packed to overflowing with an audience drawn largely from the semiconductor industry, Georgia Tech physic­s professor Walter de Heer described his latest work on a surprising alternative to silicon that could be far faster. The material: graphene, a seemingly unimpressive substance found in ordinary pencil lead.

Theoretical models had previously predicted that graphene, a form of carbon consisting of layers one atom thick, could be made into transistors more than a hundred times as fast as today’s silicon transistors. In his talk, de Heer reported making arrays of hundreds of graphene transistors on a single chip. Though the transistors still fall far short of the material’s ultimate promise, the arrays, which were fabricated in collaboration with MIT’s Lincoln Laboratory, offer strong evidence that graphene could be practical for future generations of electronics.

Today’s silicon-based computer processors can perform only a certain number of operations per second without overheating. But electrons move through graphene with almost no resistance, generating little heat. What’s more, graphene is itself a good thermal conductor, allowing heat to dissipate quickly. Because of these and other factors, graphene-based electronics could operate at much higher speeds. “There’s an ultimate limit to the speed of silicon–you can only go so far, and you cannot increase its speed any more,” de Heer says. Right now silicon is stuck in the gigahertz range. But with graphene, de Heer says, “I believe we can do a terahertz–a factor of a thousand over a gigahertz. And if we can go beyond, it will be very interesting.”

Besides making computers faster, graphene electronics could be useful for communications and imaging technolo­gies that require ultrafast transistors. Indeed, graphene is likely to find its first use in high-frequency applications such as terahertz-wave imaging, which can be used to detect hidden weapons. And speed isn’t graphene’s only advantage. Silicon can’t be carved into pieces smaller than about 10 nanometers without losing its attractive electronic properties. But the basic physics of graphene remain the same–and in some ways its electronic properties actually improve–in pieces smaller than a single nanometer.

Interest in graphene was sparked by research into carbon nanotubes as potential successors to silicon. Carbon nanotubes, which are essentially sheets of graphene rolled up into cylinders, also have excellent electronic properties that could lead to ultrahigh-­performance electronics. But nanotubes have to be carefully sorted and positioned in order to produce complex circuits, and good ways to do this haven’t been developed. ­Graphene is far easier to work with.

In fact, the devices that de Heer announced in December were carved into graphene using techniques very much like those used to manufacture silicon chips today. “That’s why industry people are looking at what we’re doing,” he says. “We can pattern graphene using basically the same methods we pattern silicon with. It doesn’t look like a science project. It looks like technology to them.”

Graphene hasn’t always looked like a promising electronic material. For one thing, it doesn’t naturally exhibit the type of switching behavior required for computing. Semiconductors such as silicon can conduct electrons in one state, but they can also be switched to a state of very low conductivity, where they’re essentially turned off. By contrast, graphene’s conductivity can be changed slightly, but it can’t be turned off. That’s okay in certain applications, such as high-frequency transistors for imaging and communications. But such transistors would be too inefficient for use in computer processors.

In 2001, however, de Heer used a computer model to show that if graphene could be fashioned into very narrow ribbons, it would begin to behave like a semiconductor. (Other researchers, he learned later, had already made similar observations.) In practice, de Heer has not yet been able to fabricate graphene ribbons narrow enough to behave as predicted. But two other methods have been shown to have similar promise: chemically modifying graphene and putting a layer of graphene on top of certain other substrates. In his presentation in Washington, de Heer described how modifying graphene ribbons with oxygen can induce semiconducting behavior. Combining these different techniques, he believes, could produce the switching behavior needed for transistors in computer processors.

Meanwhile, the promise of graphene electronics has caught the semiconductor industry’s attention. Hewlett-­Packard, IBM, and Intel (which has funded de Heer’s work) have all started to investigate the use of graphene in future products.

by Kevin Bullis",0.01% - 0.1%,Positive,Nanotech,methodology
Modeling Surprise,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Modeling Surprise
Combining massive quantities of data, insights into human psychology, and machine learning can help manage surprising events, says Eric Horvitz.

PHOTO: BETTMAN/CORBIS; GRAPHICS: JOHN HERSEY
Much of modern life depends on forecasts: where the next hurricane will make landfall, how the stock market will react to falling home prices, who will win the next primary. While existing computer models predict many things fairly accurately, surprises still crop up, and we probably can’t eliminate them. But Eric Horvitz, head of the Adaptive Systems and Interaction group at Microsoft Research, thinks we can at least minimize them, using a technique he calls “surprise modeling.”

Horvitz stresses that surprise modeling is not about building a technological crystal ball to predict what the stock market will do tomorrow, or what al-Qaeda might do next month. But, he says, “We think we can apply these methodologies to look at the kinds of things that have surprised us in the past and then model the kinds of things that may surprise us in the future.” The result could be enormously useful for decision makers in fields that range from health care to military strategy, politics to financial markets.

Granted, says Horvitz, it’s a far-out vision. But it’s given rise to a real-world application: SmartPhlow, a traffic-forecasting­ service that Horvitz’s group has been developing and testing at Microsoft since 2003.

SmartPhlow works on both desktop computers and Microsoft PocketPC devices. It depicts traffic conditions in Seattle, using a city map on which backed-up highways appear red and those with smoothly flowing traffic appear green. But that’s just the beginning. After all, Horvit­z says, “most people in Seattle already know that such-and-such a highway is a bad idea in rush hour.” And a machine that constantly tells you what you already know is just irritating. So Horvitz and his team added software that alerts users only to surprises–the times when the traffic develops a bottleneck that most people wouldn’t expect, say, or when a chronic choke point becomes magically unclogged.

But how? To monitor surprises effectively, says Horvitz, the machine has to have both knowledge–a good cognitive model of what humans find surprising–and foresight: some way to predict a surprising event in time for the user to do something about it.

Horvitz’s group began with several years of data on the dynamics and status of traffic all through Seattle and added information about anything that could affect such patterns: accidents, weather, holidays, sporting events, even visits by high-profile officials. Then, he says, for dozens of sections of a given road, “we divided the day into 15-minute segments and used the data to compute a probability distribution for the traffic in each situation.”

That distribution provided a pretty good model of what knowledgeable drivers expect from the region’s traffic, he says. “So then we went back through the data looking for things that people wouldn’t expect–the places where the data shows a significant deviation from the averaged model.” The result was a large database of surprising traffic fluctuations.

Once the researchers spotted a statistical anomaly, they backtracked 30 minutes, to where the traffic seemed to be moving as expected, and ran machine-­learning algorithms to find subtleties in the pattern that would allow them to predict the surprise. The algorithms are based on ­Bayesian modeling techniques, which calculate the probability, based on prior experience, that something will happen and allow researchers to subjectively weight the relevance of contributing events (see TR10: “Bayesian Machine Learning,” February 2004).

The resulting model works remarkably well, Horvitz says. When its parameters are set so that its false-positive rate shrinks to 5 percent, it still predicts about half of the surprises in Seattle’s traffic system. If that doesn’t sound impressive, consider that it tips drivers off to 50 percent more surprises than they would other­wise know about. Today, more than 5,000 Microsoft employees have this “surprise machine” loaded on their smart phones, and many have customized it to reflect their own preferences.

Horvitz’s group is working with Microsoft’s traffic and routing team on the possibility of commercializing aspects of ­SmartPhlow. And in 2005 Microsoft announced that it had licensed the core technology to Inrix of Kirklan­d, WA, which launched the Inrix Traffic application for Windows Mobile devices last March. The service offers traffic predictions, several minutes to five days in advance, for markets across the United States and England.

Although none of the technologies involved in SmartPhlow is entirely new, notes Daphne Koller, a probabilistic-modeling and machine-learning expert at Stanford University, their combination and application are unusual. “There has been a fair amount of work on anomaly detectio­n in large data sets to detect things like credit card fraud or bio­terrorism,” she says. But that work emphasizes the detection of present anomalies, she says, not the prediction of events that may occur in the near future. Additionally, most predictive model­s dis­regard statistical outliers; H­orvitz’s specifically tracks them. The thing that makes his approach unique, though, is his focus on the human factor, Koller says: “He’s explicitly trying to model the human cognitive process.”

The question is how wide a range of human activities can be modeled this way. While the algorithms used in SmartPhlow are, of necessity, domain specific, Horvit­z is convinced that the overall approach could be generalized to many other areas. He has already talked with political scientists about using surprise modeling to predict, say, un­expected conflicts. He is also optimistic that it could predict, for example, when an expert would be surprised by changes in housing prices in certain markets, in the Dow Jones Industrial Average, or in the exchange rate of a currency. It could even predict business trends. “Over the past few decades, companies have died because they didn’t foresee the rise of technologies that would lead to a major shift in the competitive landscape,” he says.

Most such applications are a long way off, Horvitz concedes. “This is a longer-term vision. But it’s very important, because it’s at the foundation of what we call wisdom: understanding what we don’t know.”

by M. Mitchell Waldrop",1-10%,Positive,Software,concept
NanoRadio,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"NanoRadio
Alex Zettl’s tiny radios, built from nanotubes, could improve everything from cell phones to medical diagnostics.

Tiny tunes: A nanoradio is a carbon nanotube anchored to an electrode, with a second electrode just beyond its free end.
JOHN HERSEY
If you own a sleek iPod Nano, you’ve got nothing on Alex Zettl. The physicist at the University of California, Berkeley, and his colleagues have come up with a nanoscale radio, in which the key circuitry consists of a single carbon nanotube.

Any wireless device, from cell phones to environmental sensors, could benefit from nanoradios. Smaller electronic component­s, such as tuners, would reduce power consumption and extend battery life. Nanoradios could also steer wireless communications into entirely new realms, including tiny devices that navigate the bloodstream to release drugs on command.

Miniaturizing radios has been a goal ever since RCA began marketing its pocket-sized transistor radios in 1955. More recently, electronics manufacturers have made microscale radios, creating new products such as radio frequency identification (RFID) tags. About five years ago, Zettl’s group decided to try to make radios even smaller, working at the molecular scale as part of an effort to create cheap wireless environmental sensors.

Zettl’s team set out to minia­turize individual components of a radio receiver, such as the antenna and the tuner, which selects one frequency to convert into a stream of electrical pulses that get sent to a speaker. But integrating separate nanoscale components proved difficult. About a year ago, however, Zettl and his students had a eureka moment. “We realized that, by golly, one nanotube can do it all,” Zettl says. “Within a matter of days, we had a functioning radio.” The first two transmissions it received were “Layla” by Derek and the Dominos and “Good Vibrations” by the Beach Boys.

The Beach Boys song was an apt choice. Zettl’s nano receiver works by translating the electromagnetic oscillations of a radio wave into the mechanical vibrations of a nanotube, which are in turn converted into a stream of electrical pulses that reproduce the original radio signal. Zettl’s team anchored a nanotube to a metal electrode, which is wired to a battery. Just beyond the nanotube’s free end is a second metal electrode. When a voltage is applied between the electrodes, electrons flow from the battery through the first electrode and the nanotube and then jump from the nanotube’s tip across the tiny gap to the second electrode. The nanotube–now negatively charged–is able to “feel” the oscillations of a passing radio wave, which (like all electro­magnetic waves) has both an electrical and a magnetic component.

Those oscillations successively attract and repel the tip of the tube, making the tube vibrate in sync with the radio wave. As the tube is vibrating, electrons continue to spray out of its tip. When the tip is farther from the second electrode, as when the tube bends to one side, fewer electrons make the jump across the gap. The fluctuating electrical signal that results reproduces the audio information encoded onto the radio wave, and it can be sent to a speaker.

The next step for Zettl and his colleagues is to make their nanoradios send out information in addition to receiving it. But Zettl says that won’t be hard, since a transmitter is essentially a receiver run in reverse.

Nano transmitters could open the door to other applications as well. For instance, Zettl suggests that nanoradios attached to tiny chemical sensors could be implanted in the blood vessels of patients with diabetes or other diseases. If the sensors detect an abnormal level of insulin or some other target compound, the transmitter could then relay the information to a detector, or perhaps even to an implanted drug reservoir that could release insulin or another therapeutic on cue. In fact, Zettl says that since his paper on the nanotube radio came out in the journal Nano Letters, he’s received several calls from researchers working on radio-based drug delivery vehicles. “It’s not just fantasy,” he says. “It’s active research going on right now.”

by Robert F. Service",0.01% - 0.1%,Positive,Nanotech,product
Offline Web Applications,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Offline Web Applications
Adobe’s Kevin Lynch believes that computing applications will become more powerful when they take advantage of the browser and the desktop.

Kevin Lynch
TOBY BURDITT
Web-based computer programs, unlike their desktop counterparts, are always up to date and are instantly available, no matter where the user is or what operating system she’s running. That’s why cloud computing–so called because it involves software that resides in the “clouds” of the Internet–has caused a “tidal shift in how people are actually creating software,” says Kevin Lynch, chief software architect at Adobe Systems. (For a review of ­Nicholas Carr’s new book on cloud computing, see “The Digital Utility.”) But cloud computing has drawbacks: users give up the ability to save data to their own hard drives, to drag and drop items between applications, and to receive notifications, such as appointment reminders, when the browser window is closed.

So while many companies have rushed to send users to the clouds, Lynch and his team have been planning the return trip. With the system they’re developing, the Adobe Integrated Runtime (AIR), programmers can use Web technologies to build desktop applications that people can run online or off.

The project is rooted in Lynch’s recognition of both the bene­fits and the limitations of the move from desktop to Web. He envisioned hybrid applications that would allow users to take simultaneous advantage of the Interne­t and their own machines’ ­capa­bil­i­ties. Lynch’s team started work on the concept in 2002 and launched AIR in beta last June.

AIR is a “runtime environment,” an extra layer of software that allows the same program to run on different operating systems and hardware. (Java is another example.) With AIR, developers can use Web technologies such as HTML and Flash to write software for the desktop. Users won’t have to seek out AIR to enjoy its bene­fits; they’ll be prompted to download it along with the first AIR applications they want to use.

The Adobe team chose to base the system on HTML and Flash for several reasons, Lynch says. First, it makes it easy for desktop applications to swap content with websites: for example, information from a website can be pulled into an AIR application with its formatting intact. Second, it should simplify development, encouraging a broader range of applications. Programmers can easily rebuild existing Web applications to work on the desktop. And in the same way that Web-based applications can be built once and will then run on any device with a browser, an application built on AIR will run on any machine that has AIR installed. (Adobe currently offers versions for Windows and ­Macintosh and is developing versions for Linux and mobile devices.)

Adobe is already working with partners to demonstrate AIR’s capabilities. One example: the popular auction site eBay has released a beta AIR-based application called eBay Desktop. Designed to improve the customer’s bidding experience, the application itself retrieves and displays content about eBay auctions rather than relying on a browser. It also takes advantage of the processing power of the user’s computer to provide search tools more powerful than those on the website. For example, it can scan search results for related keywords–a process that product manager Alan Lewis says works better on the desktop because the applicatio­n can store and quickly access lots of relevant information on the user’s computer. The program also uses desktop alerts to notify users when someone bids on auctions they are following. AIR enabled the company to create a customized user interface, without constraints imposed by the browser’s design and controls.

Lynch says that AIR was a response to the Web’s evolution into a more interactive medium. The browser, he notes, was created for “the Web of pages”; while developers have stretched what can be done with it, Lynch sees the need for an interface more appropriate to the Web of software that people use today. AIR, he hopes, will be just that.

by Erica Naone",1% - 10%,Positive,Software,Platform
Probabilistic Chips,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Probabilistic Chips
Krishna Palem thinks a little uncertainty in chips could extend battery life in mobile devices–and maybe the duration of Moore’s Law, too.
Krishna Palem
Krishna Palem
BRENT HUMPHREYS
Krishna Palem is a heretic. In the world of microchips, precision and perfection have always been imperative. Every step of the fabrication process involves testing and retesting and is aimed at ensuring that every chip calculates the exact answer every time. But Palem, a professor of computing at Rice ­University, believes that a little error can be a good thing.

Palem has developed a way for chips to use significantly less power in exchange for a small loss of precision. His concept carrie­s the daunting moniker “probabilistic complementary metal-oxide semi­conductor technology”–PCMOS for short. Palem’s premise is that for many applications–in particular those like audio or video processing, where the final result isn’t a number–maximum precision is ­unnecessary. Instead, chips could be designed to produce the correct answer sometimes, but only come close the rest of the time. Because the errors would be small, so would their effects: in essence, Palem believes that in computing, close enough is often good enough.

Every calculation done by a microchip depends on its transistors’ registering either a 1 or a 0 as electrons flow through them in response to an applied voltage. But electron­s move constantly, producing electrical “noise.” In order to overcome noise and ensure that their transistors register the correct values, most chips run at a relatively high voltage. Palem’s idea is to lower the operating voltage of parts of a chip–specifically, the logic circuits that calculate the least significant bits, such as the 3 in the number 21,693. The resulting decrease in signal-to-noise ratio means those circuits would occasionally arrive at the wrong answer, but engineers can calculate the probability of getting the right answer for any specific voltage. “Relaxing the probability of correctness even a little bit can produce significant savings in energy,” Palem says.

Within a few years, chips using such designs could boost battery life in mobile devices such as music players and cell phones. But in a decade or so, Palem’s ideas could have a much larger impact. By then, silicon transistors will be so small that engineers won’t be able to precisely control their behavior: the transistors will be inherently probabilistic. Palem’s techniques could then become important to the continuation of Moore’s Law, the exponential increase in transistor density–and thus in computing power–that has persisted for four decades.

When Palem began working on the idea around 2002, skepticism about the prin­ciples behind PCMOS was “pretty universal,” he says. That changed in 2006. He and his students simulated a PCMOS circuit that would be part of a chip for processing video, such as streaming video in a cell phone, and compared it with the performance of existing chips. They presented the work at a technical conference, and in a show of hands, much of the audience couldn’t discern any difference in picture quality.

Applications where the limits of human perception reduce the need for precision are perfectly suited to PCMOS designs, Palem says. In cell phones, laptop computers, and other mobile devices, graphics and sound processing consume a significant proportion of the battery power; Palem believes that PCMOS chips might increase battery life as much as tenfold without compromising the user’s experience.

PCMOS also has obvious applications in fields that employ probabilistic approaches, such as cryptography and machine learning. Algorithms used in these fields are typically designed to arrive quickly at an approximate answer. Since PCMOS chips do the same thing, they could achieve in hardware what must be done with software today–with a significant gain in both energy efficiency and speed. Palem envisions devices that use one or more PCMOS coprocessors to handle specialized tasks, such as encryption, while a traditional chip assists with other computing chores.

Palem and his team have already built and started testing a cryptography engine. They are also designing a graphics engine and a chip that people could use to adjust the power consumption and performance of their cell phones: consumers might choose high video or call quality and consume more power or choose lower quality and save the battery. Palem is discussing plans for one or more startup companies to commercialize such PCMOS chips. Companies could launch as early as next year, and products might be available in three or four years.

As silicon transistors become smaller, basic physics means they will become less reliable, says Shekhar Borkar, director of Intel’s Microprocessor Tech­nology Lab. “So what you’re looking at is having a probability of getting the result you wanted,” he says. In addition to developing hardware designs, Palem has created a probabilistic analogue to the Boolean algebra that is at the core of computational logic circuits; it is this probabilistic logic that Borkar believes could keep Moore’s Law on track. Though he says that much work remains to be done, Borkar says Palem’s research “has a very vast applicability in any digital electronics.”

If Palem’s research plays out the way the optimists believe it will, he may have the rebel’s ultimate satisfaction: seeing his heresy become dogma.

by Erika Jonietz",0.05-0.5%,Positive.,Hardware,concept
Reality Mining,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Reality Mining
Sandy Pentland is using data gathered by cell phones to learn about human behavior.

JULIEN PACAUD
Every time you use your cell phone, you leave behind a few bits of information. The phone pings the nearest cell-phone towers, revealing its location. Your service provider records the duration of your call and the number dialed.

Some people are nervous about trailing digital bread crumbs behind them. Sandy ­Pentland, however, revels in it. In fact, the MIT professor of media arts and sciences would like to see phones collect even more information about their users, recording everything from their physical activity to their conversational cadences. With the aid of some algorithms, he posits, that information could help us identify things to do or new people to meet. It could also make devices easier to use–for instance, by automatically determining security settings. More significant, cell-phone data could shed light on workplace dynamics and on the well-being of communities. It could even help project the course of disease outbreaks and provide clues about individuals’ health. Pentland, who has been sifting data gleaned from mobile devices for a decade, calls the practice “reality mining.”

Reality mining, he says, “is all about paying attention to patterns in life and using that information to help [with] things like setting privacy patterns, sharing things with people, notifying people–basically, to help you live your life.”

Researchers have been mining data from the physical world for years, says Alex Kass, a researcher who leads reality-mining projects at Accenture, a consulting and technology services firm. Sensors in manufacturing plants tell operators when equipment is faulty, and cameras on highways monitor traffic flow. But now, he says, “­reality mining is getting personal.”

Within the next few years, Pentland predicts, reality mining will become more common, thanks in part to the proliferation and increasing sophistication of cell phones. Many handheld devices now have the processing power of low-end desktop computers, and they can also collect more varied data, thanks to devices such as GPS chips that track location. And researchers such as Pentland are getting better at making sense of all that information.

To create an accurate model of a person’s social network, for example, Pentland’s team combines a phone’s call logs with information about its proximity to other people’s devices, which is continuously collected by Bluetooth sensors. With the help of factor analysis, a statistical technique commonly used in the social sciences to explain correlations among multiple variables, the team identifies patterns in the data and translates them into maps of social relationships. Such maps could be used, for instance, to accurately categorize the people in your address book as friends, family members, acquaintances, or coworkers. In turn, this information could be used to automatically establish privacy settings–for instance, allowing only your family to view your schedule. With location data added in, the phone could predict when you would be near someone in your network. In a paper published last May, ­Pentland and his group showed that cell-phone data enabled them to accurately model the social networks of about 100 MIT students and professors. They could also precisely predict where subjects would meet with members of their networks on any given day of the week.

This relationship information could have much broader implications. Earlier this year, Nathan Eagle, a former MIT grad student who had led the reality-­mining research in Pentland’s lab, moved to the Santa Fe Institute in New Mexico. There, he plans to use cell-phone data to improve existing computational models of how diseases like SARS spread. Most epidemiology models don’t back up their predictions with detailed data on where and with whom people spend their time, Eagle says. The addition of relationship and proximity data would make these models more accurate. “What’s interesting is that you can see that a disease spreads based on who is infected first,” Eagle says.

Taking advantage of other sensors in cell phones, such as the microphone or the accelerometers built into newer devices like Apple’s iPhone, could even extend the benefits of reality mining into personal health care, Pentland says. For example, clues to diagnosing depression could be found in the way a person talks: depressed people may speak more slowly, a change that speech analysis software on a phone might recognize more readily than friends or ­family do. Monitoring a phone’s motion sensors might reveal slight changes in gait, which could be an early indicator of ailments such as Parkinson’s disease.

While the promise of ­reality mining is great, the idea of collecting so much personal information naturally raises many questions about privacy, Pentland admits. He says it’s crucial that behavior-logging technology not be forced on anyone. But legal statutes are lagging behind our data collection abilities, he says, which makes it all the more important to begin discussing how the technology will be used.

For now, though, Pentland is excited about the potential of reality mining to simplify people’s lives. “All of the devices that we have are completely ignorant of the things that matter most,” he says. “They may know all sorts of stuff about Web pages and phone numbers. But at the end of the day, we live to interact with other people. Now, with reality mining, you can see how that happens … it’s an interesting God’s-eye view.”

by Kate Greene",1-10%,Negative,Software,methodology
Wireless Power,2008.0,https://www.technologyreview.com/10-breakthrough-technologies/2008/,,"Wireless Power
Physicist Marin Soljacic is working toward a world of wireless electricity.

Wireless light: Marin Soljacic (top) and colleagues used magnetic resonance coupling to power a 60-watt light bulb. Tuned to the same frequency, two 60-centimeter copper coils can transmit electricity over a distance of two meters, through the air and around an obstacle (bottom).
AMPS/MIT LIBRARIES (TOP); BRYAN CHRISTIE DESIGN (BOTTOM)
In the late 19th century, the realization that electricity could be coaxed to light up a bulb prompted a mad dash to determine the best way to distribute it. At the head of the pack was inventor Nikola Tesla, who had a grand scheme to beam elec­tricity around the world. Having difficulty imagining a vast infrastructure of wires extending into every city, building, and room, Tesla figured that wireless was the way to go. He drew up plans for a tower, about 57 meters tall, that he claimed would transmit power to points kilometers away, and even started to build one on Long Island. Though his team did some tests, funding ran out before the tower was completed. The promise of airborne power faded rapidly as the industrial world proved willing to wire up.

Then, a few years ago, Marin Soljačić, an assistant professor of physics at MIT, was dragged out of bed by the insistent beeping of a cell phone. “This one didn’t want to stop until you plugged it in for charging,” says Soljačić. In his exhausted state, he wished the phone would just begin charging itself as soon as it was brought into the house.

So Soljačić started searching for ways to transmit power wirelessly. Instead of pursuing a long-distance scheme like Tesla’s, he decided to look for midrange power transmission methods that could charge–or even power–portabl­e devices such as cell phones, PDAs, and laptops. He considered using radio waves, which effectively send information through the air, but found that most of their energy would be lost in space. More-targeted methods like lasers require a clear line of sight–and could have harmful effects on anything in their way. So Soljačić sought a method that was both efficient–able to directly power receivers without dissipating energy to the surrounding­s–and safe.

He eventually landed on the phenome­non of resonant coupling, in which two objects tuned to the same frequency exchange energy strongly but interact only weakly with other objects. A classic example is a set of wine glasses, each filled to a different level so that it vibrates at a different sound frequency. If a singer hits a pitch that matches the frequency of one glass, the glass might absorb so much acoustic energy that it will shatter; the other glasses remain unaffected.

Soljačić found magnetic resonance a promising means of electricity transfer because magnetic fields travel freely through air yet have little effect on the environment or, at the appropriate frequencies, on living beings. Working with MIT physics professors John Joannopoulos and Peter Fisher and three students, he devised a simple setup that wirelessly powered a 60-watt light bulb.

The researchers built two resonant copper coils and hung them from the ceiling, about two meters apart. When they plugged one coil into the wall, alternating current flowed through it, creating a magnetic field. The second coil, tuned to the same frequency and hooked to a light bulb, reso­nated with the magnetic field, generating an electric current that lit up the bulb–even with a thin wall between the coils.

So far, the most effective setup consists of 60-centimeter copper coils and a 10-megahertz magnetic field; this transfers power over a distance of two meters with about 50 percent efficiency. The team is looking at silver and other materials to decrease coil size and boost efficiency. “While ideally it would be nice to have efficiencies at 100 percent, realistically, 70 to 80 percent could be possible for a typical application,” says Soljačić.

Other means of recharging batteries without cords are emerging. Startups such as Powercast, Fulton Innovation, and WildCharge have begun marketing adapters and pads that allow consumers to wirelessly recharge cell phones, MP3 players, and other devices at home or, in some cases, in the car. But Soljačić’s technique differs from these approaches in that it might one day enable devices to recharge automatically, without the use of pads, whenever they come within range of a wireless transmitter.

The MIT work has attracted the attention of consumer-electronics companies and the auto industry. The U.S. Department of Defense, which is funding the research, hopes it will also give soldiers a way to automatically recharge batteries. However, Soljačić remains tight-lipped about possible industry collaborations.

“In today’s battery-operated world, there are so many potential applications where this might be useful,” he says. “It’s a powerful concept.”

by Jennifer Chu",0.05% - 0.5%,Positive,Hardware,Concept
Biological Machines,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Biological Machines
Michel Maharbiz’s novel interfaces between machines and living systems could give rise to a new generation of cyborg devices.

Cyborg beetle: By equipping a giant flower beetle with a processor and implanting electrodes that deliver electrical jolts to its brain and to its wing muscles, scientists have created a living machine whose flight can be wirelessly controlled.
JOHN BURGOYNE
A giant flower beetle flies about, veering up and down, left and right. But the insect isn’t a pest, and it isn’t steering its own path. An implanted receiver, microcontroller, microbattery, and six carefully placed electrodes–a payload smaller than a dime and weighing less than a stick of gum–allow an engineer to control the bug wirelessly. By remotely delivering jolts of electricity to its brain and wing muscles, the engineer can make the cyborg beetle take off, turn, or stop midflight.

The beetle’s creator, Michel ­Maharbiz, hopes that his bugs will one day carry sensors or other devices to locations not easily accessible to humans or the terrestrial robots used in search-and-rescue missions. The devices are cheap: materials cost as little as five dollars, and the electronics are easy to build with mostly off-the-shelf components. “They can fly into tiny cracks and could be fitted with heat sensors designed to find injured survivors,” says Maharbiz, an assistant professor at the University of California, Berkeley. “You cannot do that now with completely synthetic systems.”

Maharbiz’s specialty is designing interfaces between machines and living systems, from individual cells to entire organisms. His goal is to create novel “biological machines” that take advantage of living cells’ capacity for extremely low-energy yet exquisitely precise movement, communication, and computation. Maharbi­z envisions devices that can collect, manipulate, store, and act on information from their environments. Tissue for replacing damaged organs might be an example, or tables that can repair themselves or reconfigure their shapes on the basis of environmental cues. In 100 years, Maharbiz says, “I bet this kind of machine will be everywhere, derived from cells but completely engineered.”

The remote-controlled beetles are an early success story. Beetles integrate visual, mechanical, and chemical information to control flight, all using a modicum of energy–a feat that’s almost impossible to reproduce from scratch. In order to deploy a beetle as a useful and sophisticated tool like a search-and-rescue “robot,” Maharbiz’s team had to create input and output mechanisms that could efficiently communicate with and control the insect’s nervous system. Such interfaces are now possible thanks to advances in microfabrication techniques, the availability of ever smaller power sources, and the growing sophistication of microelectromechanical systems (MEMS)–tiny mechanical devices that can be assembled to make things like radios and microcontrollers.

Stuck to the beetle’s back is a commercial radio receiver atop a ­custom-made circuit board. Six electrode stimulators snake from the circuit board into the insect’s optic lobes, brain, and left and right basilar flight muscles. A transmitter attached to a laptop running custom software sends messages to the receiver, delivering small electric pulses to the optic lobes to initiate flight and to the left or right flight muscle to trigger a turn. Because the receiver sends very high-level instructions to the beetle’s nervous system, it can simply signal the beginning and end of a flight, rather than sending continuous messages to keep the beetle flying.

Others have created interfaces that make it possible to remotely control the movements of rats and other animals. But insects are much smaller, and thus more challenging. Maharbiz is one of the few scientists with a sufficiently deep knowledge of both biology and engineering to successfully mesh an animal’s nervous system with MEMS technologies. His team previously modified ­beetles during the pupal stage, so that their implants are invisible in adulthood–a valuable property if they are to be used in covert missions. The researchers are now working on novel microstimulators and MEMS radio receivers that will allow for more precise neural targeting and even smaller systems.

The cyborg beetle is just one of an array of new technologies incubating in ­Maharbiz’s lab, including microflui­dic chips that can deliver controlled amounts of oxygen and other chemicals–even DNA–to individual cells. This kind of system could be used to precisely control the development of cell populations. Ultimately, Maharbiz wants to develop programmable cell-based materials, like those required for the fantastical self-healing table. For now, his team focuses on finding the best ways to manipulate devices such as the beetles. “We want to find out,” says Maharbiz, “what are the limits of control?”

by Emily Singer",1-10%,Positive,Biotech,Concept
$100 Genome (Nanofluidic Chip),2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"$100 Genome
Han Cao’s nanofluidic chip could cut DNA sequencing costs dramatically.

Nanoscale sorting: A tiny nanofluidic chip is the key to BioNanomatrix’s effort to sequence a human genome for just $100.
BIONANOMATRIX
In the corner of the small lab is a locked door with a colorful sign taped to the front: “$100 Genome Room–Authorized Persons Only.” BioNanomatrix, the startup that runs the lab, is pursuing what many believe to be the key to personalized medicine: sequencing technology so fast and cheap that an entire human genome can be read in eight hours for $100 or less. With the aid of such a powerful tool, medical treatment could be tailored to a patient’s distinct genetic profile.

Despite many experts’ doubt that whole-genome sequencing could be done for $1,000, let alone a 10th that much, BioNanomatrix believes it can reach the $100 target in five years. The reason for its optimism: company founder Han Cao has created a chip that uses nanofluidics and a series of branching, ever-narrowin­g channels to allow researchers, for the first time, to isolate and image very long strands of individual DNA molecules.

If the company succeeds, a physician could biopsy a cancer patient’s tumor, sequence all its DNA, and use that information to determine a prognosis and prescribe treatment– all for less than the cost of a chest x-ray. If the ailment is lung cancer, for instance, the doctor could determine the particular genetic changes in the tumor cells and order the chemo­therapy best suited to that variant.

Cao’s chip, which neatly aligns DNA, is essential to cheaper sequencing because double-stranded DNA, when left to its own devices, winds itself up into tight balls that are impossible to analyze. To sequence even the smallest chromosomes, researchers have had to chop the DNA up into millions of smaller pieces, anywhere from 100 to 1,000 base pairs long. These shorter strands can be sequenced easily, but the data must be pieced back together like a jigsaw puzzle. The approach is expensive and time consuming. What’s more, it becomes problematic when the puzzle is as large as the human genome, which consists of about three billion pairs of nucleo­tides. Even with the most elegant algorithms, some pieces get counted multiple times, while others are omitted completely. The resulting sequence may not include the data most relevant to a particular disease.

In contrast, Cao’s chip untangles stretches of delicate double-stranded DNA molecules up to 1,000,000 base pairs long–a feat that researchers had previously thought impossible. The series of branching channels gently prompts the molecules to relax a bit more at each fork, while also acting as a floodgate to help distribute them evenly. A mild electrical charge drives them through the chip, ultimately coaxing them into spaces that are less than 100 nanometers wide. With tens of thousands of channels side by side, the chip allows an entire human genome to flow through in about 10 minutes. The data must still be pieced together, but the puzzle is much smaller (imagine a jigsaw puzzle of roughly 100 pieces versus 10,000), leaving far less room for error.


Sequencing DNA: Thousands of branching channels just nanometers wide (left) untangle very long DNA strands; bright fluorescent labels allow researchers to easily visualize these molecules (right).
The chip meets only half the $100-genome challenge: it unravels DNA but does not sequence it. To achieve that, the company is working with Silicon Valley-based Complete Genomics, which has developed bright, fluorescently labeled probes that bind to the 4,096 possible combinations of six-letter DNA “words.” Along with ­BioNanomatrix’s chip, the probes could achieve the lightning-fast sequencing necessary for the $100 genome. But the probes can’t stick to double-stranded DNA, so Complete Genomics will need to figure out how to open up small sections of DNA without uncoupling the entire molecule.

BioNanomatrix is keeping its options open. “At this point, we don’t have any exclusive ties to any sequencing chemistry,” says Gary Zweiger, the company’s vice president of development. “We want to make our chip available to sequencers, and we feel that it is an essential component to driving the costs down to the $100 level. We can’t do it alone, but we feel that they can’t do it without this critical component.”

Whether or not BioNanomatrix reaches its goal of $100 sequencing in eight hours, its technology could play an important role in medicine. Because the chips can process long pieces of DNA, the molecules retain information about gene location; they can thus be used to quickly identify new viruses or bacteria causing an outbreak, or to map new genes linked to specific diseases. And as researchers learn more about the genetic variations implicated in different diseases, it might be possible to biopsy tissue and sequence only those genes with variants known to cause disease, says Colin Collins, a professor at the Prostate Center at Vancouver General Hospital, who plans to use BioNanomatrix chips in his lab. “Suddenly,” Collins says, “you can sequence extremely rapidly and very, very inexpensively, and provide the patient with diagnosis and prognosis and, hopefully, a drug.”

by Lauren Gravitz",0.01% - 0.1%,Positive,Biotech,product
Paper Diagnostics,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Paper Diagnostics
George Whitesides has created a cheap, easy-to-use diagnostic test out of paper.

Color change: Paper tests, such as those shown here, could make it possible to diagnose a range of diseases quickly and cheaply. A small drop of liquid, such as blood or urine, wicks in through the corner or back of the paper and passes through channels to special testing zones. Substances in these zones react with specific chemicals in the sample to indicate different conditions; results show up as varying colors. These tests are small, simple, and inexpensive.
BRUCE PETERSON
Diagnostic tools that are cheap to make, simple to use, and rugged enough for rural areas could save thousands of lives in poor parts of the world. To make such devices, Harvard University professor George Whitesides is coupling advanced microfluidics with one of humankind’s oldest technologies: paper. The result is a versatile, disposable test that can check a tiny amount of urine or blood for evidence of infectious diseases or chronic conditions.

The finished devices are squares of paper roughly the size of postage stamps. The edge of a square is dipped into a urine sample or pressed against a drop of blood, and the liquid moves through channels into testing wells. Depending on the chemicals present, different reactions occur in the wells, turning the paper blue, red, yellow, or green. A reference key is used to interpret the results.

The squares take advantage of paper’s natural ability to rapidly soak up liquid­s, thus circumventing the need for pumps and other mechanical components common in microfluidic devices. The first step in building the devices is to creat­e tiny channels, about a millimete­r in width, that direct the fluid to the test wells. Whiteside­s and his coworkers soak the paper with a light-­sensitive photo­resist; ultraviolet light causes polymers in the photoresist to cross-link and harden, creating long, waterproof walls wherever the light hits it. The researchers can even create the desired channels and wells by simply drawing on the paper with a black marker and laying it in sunlight. “What we do is structure the flow of fluid in a sheet, taking advantage of the fact that if the paper is the right kind, fluid wicks and hence pulls itself along the channels,” says Whiteside­s. Each well is then brushed with a different solution that reacts with specific molecules in blood or urine to trigger a color change.

Paper is ­easily incinerated, making it easy to safely dispose of used tests. And while paper-based diagnostics (such as pregnancy tests) already exist, Whitesides­’s device has an important advantage: a single square can perform many reactions, giving it the potential to diagnose a range of conditions. Meanwhile, its small size means that blood tests require only a tiny sample, allowing a user to simply prick a finger.

Currently, Whitesides is developing a test to diagnose liver failure, which is indicated by elevated levels of certain enzymes in blood. In countries with advanced health care, people who take certain medications undergo regular blood tests to screen for liver problems that the drugs can cause. But people without consistent access to health care do not have that luxury; a paper-based test could give them the same safety margin. ­Whitesides also wants to develop tests for infectious diseases such as tuberculosis.

To disseminate the technology, ­Whitesides cofounded the nonprofit Diagnostics for All in Brookline, MA, in 2007. It plans to deploy the liver function tests in an African country around the end of this year. The team hopes that eventually, people with little medical training can administer the tests and photograph the results with a cell phone. Whitesides envisions a center where technicians and doctors can evaluate the images and send back treatment recommendations.

“This is one of the most deployable devices I have seen,” says Albert Folch, an associate professor of bioengineering at the University of Washington, who works with microfluidics. “What is so incredibly clever is that they were able to create photoresist structures embedded inside paper. At the same time, the porosity of the paper acts as the cheapest pump on the planet.”

Recently, the Harvard researchers have made the paper chips into a three-­dimensional diagnostic device by layering them with punctured pieces of waterproof tape. A drop of liquid can move across channels and into wells on the first sheet, diffuse down through the holes in the tape, and react in test wells on the second paper layer. The ability to perform many more tests and even carry out two-step reactions with a single sample will enable the device to detect diseases (like malaria or HIV) that require more complicated assays, such as those that use antibodies. Results appear after five minutes to half an hour, depending on the test.

The researchers hope the advanced version of the test can eventually be mass produced using the same printing tech­nology that churns out newspapers. Cost for the materials should be three to five cents. At that price, says Folch, the tests “will have a big impact on health care in areas where transportation and energy access is difficult.”

by Kristina Grifantini",0.01% - 0.1%,positive,Biotech,product
Traveling-Wave Reactor,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Traveling-Wave Reactor
A new reactor design could make nuclear power safer and cheaper, says John Gilleland.

Wave of the future: Unlike today’s reactors, a traveling-wave reactor requires very little enriched uranium, reducing the risk of weapons proliferation. (Click here for a larger diagram, also on page 3). The reactor uses depleted-uranium fuel packed inside hundreds of hexagonal pillars (shown in black and green). In a “wave” that moves through the core at only a centimeter per year, this fuel is transformed (or bred) into plutonium, which then undergoes fission. The reaction requires a small amount of enriched uranium (not shown) to get started and could run for decades without refueling. The reactor uses liquid sodium as a coolant; core temperatures are extremely hot--about 550 ºC, versus the 330 ºC typical of conventional reactors.
BRYAN CHRISTIE DESIGN
Enriching the uranium for reactor fuel and opening the reactor periodically to refuel it are among the most cumbersome and expensive steps in running a nuclear plant. And after spent fuel is removed from the reactor, reprocessing it to recover usable materials has the same drawbacks, plus two more: the risks of nuclear-weapons proliferation and environmental pollution.

These problems are mostly accepted as a given, but not by a group of researcher­s at Intellectual Ventures, an invention and investment company in Bellevue, WA. The scientists there have come up with a preliminary design for a reactor that requires only a small amount of enriched fuel–that is, the kind whose atoms can easily be split in a chain reaction. It’s called a traveling­-wave reactor. And while government researchers intermittently bring out new reactor designs, the traveling-wave reactor is noteworthy for having come from something that barely exists in the nuclear industry: a privately funded research company.

As it runs, the core in a traveling-­wave reactor gradually converts nonfissile material into the fuel it needs. Nuclear reactors based on such designs “theoretically could run for a couple of hundred years” without refueling, says John G­illeland, manager of nuclear programs at Intellectual Ventures.

Gilleland’s aim is to run a nuclear reactor on what is now waste. ­Conventional reactors use uranium-235, which splits easily to carry on a chain reaction but is scarce and expensive; it must be separated from the more common, nonfissile uranium-238 in special enrichment plants. Every 18 to 24 months, the reactor must be opened, hundreds of fuel bundles removed, hundreds added, and the remainder reshuffled to supply all the fissile uranium needed for the next run. This raises proliferation concerns, since an enrichment plant designed to make low-enriched uranium for a power reactor differs trivially from one that makes highly enriched material for a bomb.

But the traveling-wave reactor needs only a thin layer of enriched U-235. Most of the core is U-238, millions of pounds of which are stockpiled around the world as leftovers from natural uranium after the U-235 has been scavenged. The design provides “the simplest possible fuel cycle,” says Charles W. Forsberg, executive director of the Nuclear Fuel Cycle Project at MIT, “and it requires only one uranium enrichment plant per planet.”

The trick is that the reactor itself will convert the uranium-238 into a usable fuel, plutonium-239. Conventional reactors also produce P-239, but using it requires removing the spent fuel, chopping it up, and chemically extracting the plutonium–a dirty, expensive process that is also a major step toward building an atomic bomb. The traveling-wave reactor produces plutonium and uses it at once, eliminating the possibility of its being diverted for weapons. An active region less than a meter thick moves along the reactor core, breeding new plutonium in front of it.

The traveling-wave idea dates to the early 1990s. However, Gilleland’s team is the first to develop a practical design. Intellectual Ventures has patented the technology; the company says it is in licensing discussions with reactor manufacturers but won’t name them. Although there are still some basic design issues to be worked out–for instance, precise models of how the reactor would behave under accident conditions–Gilleland thinks a commercial unit could be running by the early 2020s.

While Intellectual Ventures has caught the attention of academics, the commercial industry–hoping to stimulate interest in an energy source that doesn’t contribute to global warming–is focused on selling its first reactors in the U.S. in 30 years. The designs it’s proposing, however, are essentially updates on the models operating today. Intellectual Ventures thinks that the traveling-wave design will have more appeal a bit further down the road, when a nuclear renaissance is fully under way and fuel supplies look tight.

“We need a little excitement in the nuclear field,” says Forsber­g. “We have too many people working on 1/10th of 1 percent change.”


A. Coolant pumps B. Expansion area for fission gases C. Fuel (depleted uranium) inside the hexagonal pillars; green represents unused fuel, black spent fuel D. Fission wave (red) E. Breeding wave (yellow) F. Liquid sodium coolant
by Matthew L. Wald",<0.1% to <1%,Positive,Nanotech,Concept
Racetrack Memory,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Racetrack Memory
Stuart Parkin is using nanowires to create an ultradense memory chip.

ARTHUR MOUNT; SOURCE: IBM
When IBM sold its hard-drive business to Hitachi in April 2002, IBM fellow Stuart Parkin wondered what to do next. He had spent his career studying the fundamental physics of magnetic materials, making a series of discoveries that gave hard-disk drives thousands of times more storage capacity. So Parkin set out to develop an entirely new way to store information: a memory chip with the huge storage capacity of a magnetic hard drive, the durability of electronic flash memory, and speed superior to both. He dubbed the new technology “racetrack memory.”

Both magnetic disk drives and existing solid-state memory technologies are essentially two-dimensional, Parkin says, relying on a single layer of either magnetic bits or transistors. “Both of these technologies have evolved over the last 50 years, but they’ve done it by scaling the devices smaller and smaller or developing new means of accessing bits,” he says. Parki­n sees both technologies reaching their size limits in the coming decades. “Our idea is totally different from any memory that’s ever been made,” he says, “because it’s three-dimensional.”

The key is an array of U-shaped magnetic nanowires, arranged vertically like trees in a forest. The nanowires have regions with different magnetic polarities, and the boundaries between the regions represent 1s or 0s, depending on the polarities of the regions on either side. When a spin-polarized current (one in which the electrons’ quantum-mechanica­l “spin” is oriented in a specific direction) passes through the nanowire, the whole magnetic pattern is effectively pushed along, like cars speeding down a racetrack. At the base of the U, the magnetic boundaries encounter a pair of tiny devices that read and write the data.

This simple design has the potential to combine the best qualities of other ­memory technologies while avoiding their drawbacks. Because racetrack memory stores data in vertical nanowires, it can theoretically pack 100 times as much data into the same area as a flash-chip transistor, and at the same cost. There are no mechanical parts, so it could prove more reliable than a hard drive. Racetrack memory is fast, like the dynamic random-access memory (DRAM) used to hold frequently accessed data in computers, yet it can store information even when the power is off. This is because no atoms are moved in the process of reading and writing data, eliminating wear on the wire.

Just as flash memory ushered in ultra­small devices that can hold thousands of songs, pictures, and other types of data, racetrack promises to lead to whole new categories of electronics. “An even denser, smaller memory could make computers more compact and more energy efficient,” Parkin says. Moreover, chips with huge data capacity could be shrunk to the size of a speck of dust and sprinkled about the environment in tiny sensors or implanted in patients to log vital signs.

When Parkin first proposed racetrack memory, in 2003, “people thought it was a great idea that would never work,” he says. Before last April, no one had been able to shift the magnetic domains along the wire without disturbing their orientations. However, in a paper published that month in Science, Parkin’s team showed that a spin-polarized current would preserve the original magnetic pattern.

The Science paper proved that the concept of racetrack memory is sound, although at the time, the researchers had moved only three bits of data down a nanowire. Last December, Parkin’s team successfully moved six bits along the wire. He hopes to reach 10 bits soon, which he says would make racetrack memory competitive with flash storage. If his team can manage 100 bits, racetrack could replace hard drives.

Parkin has already found that the trick to increasing the number of bits a wire can handle is to precisely control its diameter: the narrower and more uniform the wire, the more bits it can hold. Another challenge will be to find the best material for the job: it needs to be one that can survive the manufacturing process and one that allows the magnetic domains to move quickly along the wire, with the least amount of electrical current possible.

If the design proves successful, racetrack memory could replace all other forms of memory, and Parkin will bolster his status as a ­magnetic-memory genius. After all, his work on giant magnetoresistance, which led to today’s high-capacity hard drives, transformed the computing industry. With racetrack memory, ­Parkin could revamp computing once more.


ARTHUR MOUNT; SOURCE: IBM
by Kate Greene",0.01% to 0.1%,Positive,Hardware,product
Liquid Battery,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Liquid Battery
Donald Sadoway conceived of a novel battery that could allow cities to run on solar power at night.

Conventional battery: Ordinary batteries use at least one solid active material. In the lead-acid battery shown here, the electrodes are solid plates immersed in a liquid electrolyte. Solid materials limit the conductivity of batteries and therefore the amount of current that can flow through them. They’re also vulnerable to cracking, disintegrating, and otherwise degrading over time, which reduces their useful lifetimes.
ARTHUR MOUNT
Without a good way to store electricity on a large scale, solar power is useless at night. One promising storage option is a new kind of battery made with all-liquid active materials. Prototypes suggest that these liquid batteries will cost less than a third as much as today’s best batteries and could last significantly longer.

The battery is unlike any other. The electrodes are molten metals, and the electrolyte that conducts current between them is a molten salt. This results in an unusually resilient device that can quickly absorb large amounts of electricity. The electrodes can operate at electrical currents “tens of times higher than any [battery] that’s ever been measured,” says Donald Sadow­ay, a materials chemistry professor at MIT and one of the battery’s inventors. What’s more, the materials are cheap, and the design allows for simple manufacturing.

The first prototype consists of a container surrounded by insulating material. The researchers add molten raw materials: antimony on the bottom, an electrolyte such as sodium sulfide in the middle, and magnesium at the top. Since each material has a different density, they naturally remain in distinct layers, which simplifies manufacturing. The container doubles as a current collector, delivering electrons from a power supply, such as solar panels, or carrying them away to the electrical grid to supply electricity to homes and businesses.


Discharged, charging, charged: The molten active components (colored bands: blue, magnesium; green, electrolyte; yellow, antimony) of a new grid-scale storage battery are held in a container that delivers and collects electrical current (left). Here, the battery is ready to be charged, with positive magnesium and negative antimony ions dissolved in the electrolyte. As electric current flows into the cell (center), the magnesium ions in the electrolyte gain electrons and form magnesium metal, which joins the molten magnesium electrode. At the same time, the antimony ions give up electrons to form metal atoms at the opposite electrode. As metal forms, the electrolyte shrinks and the electrodes grow (right), an unusual property for batteries. During discharge, the process is reversed, and the metal atoms become ions again.
ARTHUR MOUNT
As power flows into the battery, magnesium and antimony metal are generated from magnesium antimonide dissolved in the electrolyte. When the cell discharges, the metals of the two electrodes dissolve to again form magnesium antimonide, which dissolves in the electrolyte, causing the electrolyte to grow larger and the electrodes to shrink (see above).

Sadoway envisions wiring together large cells to form enormous battery packs. One big enough to meet the peak electricity demand in New York City–about 13,000 megawatts–would fill nearly 60,000 square meters. Charging it would require solar farms of unprecedented size, generating not only enough electricity to meet daytime power needs but enough excess power to charge the batteries for nighttime demand. The first systems will probably store energy produced during periods of low electricity demand for use during peak demand, thus reducing the need for new power plants and transmission lines.

Many other ways of storing energy from intermittent power sources have been proposed, and some have been put to limited use. These range from stacks of lead-acid batteries to systems that pump water uphill during the day and let it flow back to spin generators at night. The liquid battery has the advantage of being cheap, long-lasting, and (unlike options such as pumping water) useful in a wide range of places. “No one had been able to get their arms around the problem of energy storage on a massive scale for the power grid,” says Sadoway. “We’re literally looking at a battery capable of storing the grid.”

Since creating the initial prototypes, the researchers have switched the metals and salts used; it wasn’t possible to dissolve magnesium antimonide in the electrolyte at high concentrations, so the first prototypes were too big to be practical. (Sadowa­y won’t identify the new materials but says they work along the same principles.) The team hopes that a commercial version of the battery will be available in five years.

by Kevin Bullis",0.01% - 0.1%,Positive,"Energy Storage is a type of ""hardware"" but more specifically related to climate tech and batteries so: 
Climate Tech",product
Intelligent Software Assistant,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Intelligent Software Assistant
Adam Cheyer is leading the design of powerful software that acts as a personal aide.

Weekend plans: Adam Cheyer participates in a conversation with the software. (Go to the next page to read the dialogue and an explanation of the artificial-intelligence behind it.)
HOWARD CAO
Search is the gateway to the Internet for most people; for many of us, it has become second nature to distill a task into a set of keywords that will lead to the required tools and information. But Adam Cheyer, cofounder of Silicon Valley startup Siri, envisions a new way for people to interact with the services available on the Internet: a “do engine” rather than a search engine. Siri is working on virtual personal-assistant software, which would help users complete tasks rather than just collect information.

Cheyer, Siri’s vice president of engineering, says that the software takes the user’s context into account, making it highly useful and flexible. “In order to get a system that can act and reason, you need to get a system that can interact and understand,” he says.

Siri traces its origins to a military-funded artificial-intelligence project called CALO, for “cognitive assistant that learns and organizes,” that is based at the research institute SRI International. The project’s leaders–including Cheyer–combined traditionally isolated approaches to artificial intelligence to try to create a personal-assistant program that improves by interacting with its user. Cheyer, while still at SRI, took a team of engineers aside and built a sample consumer version; colleagues finally persuaded him to start a company based on the prototype. Siri licenses its core technology from SRI.

Mindful of the sometimes spectacular failure of previous attempts to create a virtual personal assistant, Siri’s founders have set their sights conservatively. The initial version, to be released this year, will be aimed at mobile users and will perform only specific types of functions, such as helping make reservations at restaurants, check flight status, or plan weekend activities. Users can type or speak commands in casual sentences, and the software deciphers their intent from the context. Siri is connected to multiple online services, so a quick interaction with it can accomplish several small tasks that would normally require visits to a number of websites. For example, a user can ask Siri to find a midpriced Chinese restaurant in a specific part of town and make a reservation there.

Recent improvements in computer processor power have been essential in bringing this level of sophistication to a consumer product, Cheyer says. Many of CALO’s abilities still can’t be crammed into such products. But the growing power of mobile phones and the increasing speed of networks make it poss­ible to handle some of the processing at Siri’s headquarters and pipe the results back to users, allowing the software to take on tasks that just couldn’t be done before.

“Search does what search does very well, and that’s not going anywhere anytime soon,” says Dag Kittlaus, Siri’s cofounder and CEO. “[But] we believe that in five years, everyone’s going to have a virtual assistant to which they delegate a lot of the menial tasks.”

While the software will be intelligent and useful, the company has no aspiration to make it seem human. “We think that we can create an incredible experience that will help you be more efficient in your life, in solving problems and the tasks that you do,” Cheyer says. But Siri is always going to be just a tool, not a rival to human intelligence: “We’re very practical minded.”


Weekend Plans Siri cofounder Tom Gruber volunteered Adam Cheyer to participate in a conversation with the software (shown above). Gruber explains the artificial-intelligence tasks behind its responses.
1. “The user can ask a broad question like this because Siri has information that gives clues about what the user intends. For example, the software might store data about the user’s location, schedule, and past activities. Siri can deal with open-ended questions within specific areas, such as entertainment or travel.”

2. “Siri pulls information rele­vant to the user’s question from a variety of Web services and tools. In this case, it checks the weather, event listings, and directories of local attractions and uses machine learning to select certain options based on the user’s past preferences. Siri can connect to various Web applications and then integrate the results into a single response.”

3. “Siri interprets this reply in the context of the existing conversation, using it to refine the user’s request.”

4. “The software offers specific suggestions based on the user’s personal preferences and its ability to categorize. Because Siri is task-oriented, rather than a search engine, it offers to buy tickets that the user selects.”

5. “By now, the conversation has narrowed enough that all the user has to do is click on his choice.”

6. “Siri compiles information about the event, such as band members, directions, and prices, and structures it in a logical way. It also handles the task of finding out what’s available and getting the tickets.”

by Erica Naone",1% - 10%,Positive,Software,Product
Nanopiezoelectronics,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Nanopiezoelectronics
Zhong Lin Wang thinks piezoelectric nanowires could power implantable medical devices and serve as tiny sensors.

BRYAN CHRISTIE DESIGN
Nanoscale sensors are exquisitely sensitive, very frugal with power, and, of course, tiny. They could be useful in detecting molecular signs of disease in the blood, minute amounts of poisonous gases in the air, and trace contaminants in food. But the batteries and integrated circuits necessary to drive these devices make them difficult to fully miniaturize. The goal of Zhong Lin Wang, a materials scientist at Georgia Tech, is to bring power to the nano world with minuscule generators that take advantage of piezoelectricity. If he succeeds, biological and chemical nano sensors will be able to power themselves.

The piezoelectric effect–in which crystalline materials under mechanical stress produce an electrical potential–has been known of for more than a century. But in 2005, Wang was the first to demonstrate it at the nanoscale by bending zinc oxide nanowires with the probe of an atomic-force microscope. As the wires flex and return to their original shape, the potential produced by the zinc and oxide ions drives an electrical current. The current that Wang coaxed from the wires in his initial experiments was tiny; the electrical potential peaked at a few millivolts. But Wang rightly suspected that with enough engineering, he could design a practical nanoscale power source by harnessing the tiny vibrations all around us–sound waves, the wind, even the turbulence of blood flow over an implanted device. These subtle movements would bend nanowires, generating electricity.


Piezoelectric wires: The mechanical stress produced by bending a zinc oxide nanowire creates an electrical potential across the wire. This drives current through a circuit. The conversion of mechanical energy to electrical energy is called the piezoelectric effect. It’s harnessed in the devices on the next page, which might be made from the nanowires.
BRYAN CHRISTIE DESIGN
Last November, Wang embedded zinc oxide nanowires in a layer of polymer; the resulting sheets put out 50 millivolts when flexed. This is a major step forward in powering tiny sensors.

And Wang hopes that these generators could eventually be woven into fabric; the rustling of a shirt could generate enough power to charge the batteries of devices like iPods. For now, the nanogenerator’s output is too low for that. “We need to get to 200 millivolts or more,” says Wang. He’ll get there by layering the wires, he says, though it might take five to ten more years of careful engineering.

Meanwhile, Wang has demonstrated the first components for a new class of nanoscale sensors. Nanopiezotronics, as he calls this technology, exploit the fact that zinc oxide nanowires not only exhibit the piezoelectric effect but are semiconductors. The first property lets them act as mechanical sensors, because they produce an electrical response to mechanical stress. The second means that they can be used to make the basic components of integrated circuits, including transistors and diodes. Unlike traditional electronic components, nanopiezotronics don’t need an external source of electricity. They generate their own when exposed to the same kinds of mechanical stresses that power nanogenerators.


Nanogenerator: (Left, clockwise) Arrays of zinc oxide nanowires packaged in a thin polymer film generate electrical current when flexed. The nanogenerator could be embedded in clothing and used to convert the rustling of fabric into current to power portable devices such as cell phones. Hearing aid: An array of vertically aligned piezoelectric nanowires could serve as a hearing aid. When sound waves hit them, the wires bend, generating an electrical potential. The electrical signal can then be amplified and sent directly to the auditory nerve. Signature verification: A grid of piezoelectric wires underneath a signature pad would record the pattern of pressure applied by each person signing. Combined with a database of such patterns, the system could authenticate signatures. Bone-loss monitor: A mesh of piezoelectric nanowires could monitor mechanical strain indicative of bone loss. Dangerous stress to the bone would generate an electrical current in the wires; this would cause the device to beam an alert signal outside the body. The sensor could be implanted in a minimally invasive procedure.
BYRAN CHRISTIE DESIGN
Freeing nanoelectronics from outside power sources opens up all sorts of possibilities. A nano­piezotronic hearing aid integrated with a nanogenerator might use an array of nanowires, each tuned to vibrate at a different frequency over a large range of sounds. The nanowires would convert sounds into electrical signals and process them so that they could be conveyed directly to neurons in the brain. Not only would such implanted neural prosthetics be more compact and more sensitive than traditional hearing aids, but they wouldn’t need to be removed so their batteries could be changed. Nanopiezotronic sensors might also be used to detect mechanical stresses in an airplane engine; just a few nanowire components could monitor stress, process the information, and then communicate the relevant data to an airplane’s computer. Whether in the body or in the air, nano devices would at last be set loose in the world all around us.

by Katherine Bourzac",0.01% - 0.1%,Positive,Nanotech,concept
HashCache,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"HashCache
Vivek Pai’s new method for storing Web content could make Internet access more affordable around the world.

Closing the divide: Students surf the Web at Ghana’s Kokrobitey Institute, a conference center with an Internet connection only about four times as fast as dial-up. The link is enhanced by Princeton’s low-cost, low-power HashCache technology, which stores frequently accessed Web content.
OLIVIER ASSELIN/WPN
Throughout the developing world, scarce Internet access is a more conspicuous and stubborn aspect of the digital divide than a dearth of computers. “In most places, networking is more expensive–not only in relative terms but even in absolute terms–than it is in United States,” says Vivek Pai, a computer scientist at Princeton University. Often, even universities in poor countries can afford only low-bandwidth connections; individual users receive the equivalent of a fraction of a dial-up connection. To boost the utility of these connections, Pai and his group created HashCache, a highly efficient method of caching–that is, storing frequently accessed Web content on a local hard drive instead of using precious bandwidth to retrieve the same information repeatedly.

Despite the Web’s protean nature, a surprising amount of its content doesn’t change often or by very much. But current caching technologies require not only large hard disks to hold data but also lots of random-access memory (RAM) to store an index that contains the “address” of each piece of content on the disk. RAM is expensive relative to hard-disk capacity, and it works only when supplied with electricity–which, like bandwidth, is often both expensive and scarce in the developing world.

HashCache abolishes the index, slashing RAM and electricity requirements by roughly a factor of 10. It starts by transforming the URL of each stored Web “object”–an image, graphic, or block of text on a Web page–into a shorter number, using a bit of math called a hash function. While most other caching systems do this, they also store each hash number in a RAM-hogging table that correlates it with a hard-disk memory address. Pai’s technology can skip this step because it uses a novel hash function: the number that the function produces defines the spot on the disk where the corresponding Web object can be found. “By using the hash to directly compute the location, we can get rid of the index entirely,” Pai says.

To be sure, some RAM is still needed, but only enough to run the hash function and to actually retrieve a specific Web object, Pai says. Though still at a very early stage of development, HashCache is being field-tested at the Kokrobitey Institute in Ghana and Obafemi Awolowo University in Nigeria.

The technology ends a long drought in fundamental caching advances, says Jim Gettys, a coauthor of the HTTP specification that serves as the basis of Internet communication. While it’s increasingly feasible for a school in a poor country to buy hundreds of gigabytes of hard-disk memory, Gettys says, those same schools–if they use today’s best available software–can typically afford only enough RAM to support tens of gigabytes of cached content. With HashCache, a classroom equipped with pretty much any kind of computers, even castoff PCs, could store and cheaply access one terabyte of Web data. That’s enough to store all of Wikipedia’s content, for example, or all the coursework freely available from colleges such as Rice University and MT.

Even with new fiber-optic cables connecting East Africa to the Internet, thousands of students at some African universities share connections that have roughly the same speed as a home DSL line, says Ethan Zuckerman, a fellow at the Berkman Center for Internet and Society at Harvard University. “These universities are extremely bandwidth constrained,” he says. “All their students want to have computers but almost never have sufficient bandwidth. This innovation makes it significantly cheaper to run a very large caching server.”

Pai plans to license HashCache in a way that makes it free for nonprofits but leaves the door open to future commercialization. And that means that it could democratize Internet access in wealthy countries, too.

by David Talbot",0.001% - 0.01%,Positive,Software,methodology
Software-Defined Networking,2009.0,https://www.technologyreview.com/10-breakthrough-technologies/2009/,,"Software-Defined Networking
Nick McKeown believes that remotely controlling network hardware with software can bring the Internet up to speed.

SUPERBROTHERS
For years, computer scientists have dreamed up ways to improve networks’ speed, reliability, energy efficiency, and security. But their schemes have generally remained lab projects, because it’s been impossible to test them on a large enough scale to see if they’d work: the routers and switches at the core of the Internet are locked down, their software the intellectual property of companies such as Cisco and Hewlett-Packard.

Frustrated by this inability to fiddle with Internet routing in the real world, Stanfor­d computer scientist Nick McKeow­n and colleagues developed a standard called OpenFlow that essentially opens up the Internet to researchers, allowing them to define data flows using software–a sort of “software-defined networking.” Installing a small piece of OpenFlow firmware (software embedded in hardware) gives engineers access to flow tables, rules that tell switches and routers how to direct network traffic. Yet it protects the proprietary routing instructions that differentiate one company’s hardware from another.

With OpenFlow installed on routers and switches, researchers can use software on their computers to tap into flow tables and essentially control a network’s layout and traffic flow with the click of a mouse. This software-based access allows computer scientists to inexpensively and easily test new switching and routing protocols. “Today, security, routing, and energy management are dictated by the box, and it’s very hard to change,” says McKeown. “That’s why the infrastructure hasn’t changed for 40 years.”

Normally, when a data packet arrives at a switch, firmware checks the packet’s destination and forwards it according to predefined rules over which network operators have no control. All packets going to the same place are routed along the same path and treated the same way.

On a network running OpenFlow, computer scientists can add to, subtract from, and otherwise meddle with these rules. This means that researchers could, say, give video priority over e-mail, reducing the annoying stops and starts that sometimes plague streaming video. They could set up rules for traffic coming from or going to a certain destination, allowing them to quarantine traffic from a computer suspected of harboring viruses.

And OpenFlow can be used to improve cellular networks as well. Mobile-service providers have begun to expand their networks using commodity hardware built for the Internet. But such hardware is horrible at maintaining connections when a user is moving: just think about the less-than-seamless way that a laptop’s data connection is transferred from one wireless base station to another. OpenFlow, says McKeown, offers service providers a way to try out new solutions to the mobility problem.

McKeown’s group receives funding and equipment from networking companies such as Cisco, Juniper, HP, and NEC, as well as cellular providers including T-Mobile, Ericsson, and NTT DoCoMo. Ideas tested on switches running ­OpenFlow could be incorporated into the firmware of new routers, or they could be added to old ones through firmware updates. McKeown expects that within the year, one or more of these companies will ship products with OpenFlow built in.

by Kate Greene",0.1% - 1.0%,Positive,Software,platform
Real-Time Search,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Real-Time Search
Social networking is changing the way we find information.

The real-time man: Google's Amit Singhal is mining social networks ­to generate up-to-the-second search results.
TOBY BURDITT
How do you parse a tweet? Five years ago, that question would have been gibberish. Today, it’s perfectly sensible, and it’s at the front of Amit ­Singhal’s mind. Singhal is leading Google’s quest to incorporate new data into search results in real time by tracking and ranking updates to online content–particularly the thousands of messages that course through social networks every second.

Real-time search is a response to a fundamental shift in the way people use the Web. People used to visit a page, click a link, and visit another page. Now they spend a lot of time monitoring streams of data–tweets, status updates, headlines–from services like Facebook and Twitter, as well as from blogs and news outlets.

Ephemeral info-nuggets are the Web’s new currency, and sifting through them for useful information is a challenge for search engines. Its most daunting aspect, according to Singhal, is not collecting the data. Facebook and Twitter are happy to sell access to their data feeds–or “fire hoses,” as they call them–directly to search providers; the information pours straight into Google’s computers.

What’s really hard about real-time search is figuring out the meaning and value of those fleeting bits of information. The challenge goes beyond filtering out spam, though that’s an important part of it. People who search real-time data want the same quality, authority, and relevance that they expect when they perform traditional Web searches. Nobody wants to drink straight from a fire hose.

Google dominates traditional search by meticulously tracking links to a page and other signals of its value as they accumulate over time. But for real-time search, this doesn’t work. Social-networking messages can lose their value within minutes of being written. Google has to gauge their worth in seconds, or even microseconds.

Google is notoriously tight-lipped about its search algorithms, but Singhal explains a few of the variables the company uses to analyze what he calls “chatter.” Some are straightforward. A Twitter user who attracts many followers, and whose tweets are often “retweeted” by other users, can generally be assumed to have more authority. Similarly, Facebook users gain authority as their friends multiply, particularly if those friends also have many friends.

Other signals are more subtle. A sudden spike in the prevalence of a word in a message stream–earthquake, say–may indicate an important event. If a message on a commonly discussed topic includes unusual phrasing, that may signal new information or a fresh insight. Google, says Singhal, continuously scans for shifts in language and other deviations from predicted behavior.

The company is also working to connect message content to the geolocation data that’s transmitted by smart phones and other mobile computers, or broadcast through services like Foursquare. The location of someone sending a message can matter a great deal. If you know that a person tweeting about an earthquake is close to the epicenter, chances are those tweets will be more valuable than those of someone hundreds of miles away.

Singhal’s view of real-time search is very much in line with Google’s strategy: distilling from a welter of data the few pieces of content that are most relevant to an individual searcher at a particular point in time. Other search providers, including Google’s arch rival, Microsoft, are taking a more radical view.

Sean Suchter, who runs Microsoft’s Search Technology Center in Mountain View, CA, doesn’t like the term real-time search, which he considers too limiting. He thinks Microsoft’s Bing search engine should not just filter data flowing from social networks but become an extension of them.

Ultimately, says Suchter, one-on-one conversations will take place within Bing, triggered by the keywords people enter. Real-time search, he predicts, will be so different from what came before that it will erase Google’s long-standing advantages. “History doesn’t matter here,” he says. After a pause, he adds, “We’re going to wipe the floor with them.”

Amit Singhal has heard such threats before, and so far they haven’t amounted to much. But even he admits that real-time search comes as close to marking “a radical break” in the history of search as anything he’s seen. Keeping Google on top in the age of chatter may prove to be Singhal’s toughest test.

by Nicholas Carr",2-20%,Negative,Software,product
Solar Fuel,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Solar Fuel
Designing the perfect renewable fuel.

Fuel for the future: Joule Biotechnologies' genetically engineered microörganisms can turn sunlight into ethanol or diesel.
BOB O'CONNOR
When Noubar Afeyan, the CEO of Flagship Ventures in Cambridge, MA, set out to invent the ideal renewable fuel, he decided to eliminate the middleman. Biofuels ultimately come from carbon dioxide and water, so why persist in making them from biomass–corn or switchgrass or algae? “What we wanted to know,” Afeyan says, “is could we engineer a system that could convert carbon dioxide directly into any fuel that we wanted?”

The answer seems to be yes, according to Joule Biotechnologies, the company that Afeyan founded (also in Cambridge) to design this new fuel. By manipulating and designing genes, Joule has created photosynthetic microörganisms that use sunlight to efficiently convert carbon dioxide into ethanol or diesel–the first time this has ever been done, the company says. Joule grows the microbes in photobioreactors that need no fresh water and occupy only a fraction of the land needed for biomass-based approaches. The creatures secrete fuel continuously, so it’s easy to collect. Lab tests and small trials lead Afeyan to estimate that the process will yield 100 times as much fuel per hectare as fermenting corn to produce ethanol, and 10 times as much as making it from sources such as agricultural waste. He says costs could be competitive with those of fossil fuels.

If Afeyan is right, biofuels could become an alternative to petroleum on a much broader scale than has ever seemed possible. The supply of conventional biofuels, such as those made from corn, is constrained by the vast amount of water and agricultural land needed to grow the plants they’re made from. And while advanced biofuels require less water and don’t need high-quality land, their potential is limited by the expensive, multistep processes needed to make them. As a result, the International Energy Agency estimates that in 2050, biodiesel and ethanol will meet only 26 percent of world demand for transportation fuel.

Joule’s bioengineers have equipped their microörganisms with a genetic switch that limits growth. The scientists allow them to multiply for only a couple of days before flipping that switch to divert the organisms’ energy from growth into fuel production. While other companies try to grow as much biomass as possible, Afeyan says, “I want to make as little biomass as I can.” In retrospect, the approach might seem obvious. Indeed, the startup Synthetic Genomics and an academic group at the BioTechnology Institute at the University of Minnesota are also working on making fuels directly from carbon dioxide. Joule hopes to succeed by developing both its organisms and its photobioreactor from scratch, so that they work perfectly together.


Oil farm: Arrays of bioreactors filled with Joule Biotechnologies' microörganisms absorb sunlight. Supplied with carbon dioxide and nutrients, the organisms use photosynthesis to produce diesel. As they secrete it, the diesel fraction circulates to a separator that extracts the fuel and sends it to storage tanks.
BROWN BIRD DESIGN
Still, it’s a risky strategy, since it departs from established processes. Usually, a startup sets out determined to do something novel, says James Collins, a professor of biomedical engineering at Boston University and a member of Joule’s scientific advisory board, “and it falls quickly back on trying to find something that works … an old thing that’s been well established.” Afeyan, however, has pushed the company to stay innovative. This summer, it will move beyond lab-scale tinkering; an outdoor pilot plant is currently under construction in Leander, TX.

As both a venture capitalist and a technologist–he received his PhD in chemical engineering from MIT in 1987–Afeyan is keenly aware of the challenges in demonstrating that a novel process can operate economically and make fuel in large volumes. To minimize the financial risks, he steered Joule toward a modular process that doesn’t require large and expensive demonstration plants.

“I’m not saying it’s easy or around the corner, because I’ve done this for a long time,” Afeyan says. But he does believe that Joule is onto something big: a renewable fuel that could compete with fossil fuels on both cost and scale. He says, “We have the elements of a potentially transformative technology.”

by Kevin Bullis",<0.01% - <0.1%,Positive,Climate tech,product
Mobile 3-D,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Mobile 3-D
Smart phones will take 3-D mainstream.

3-D on the go: Released in South Korea in March, Samsung's W960 mobile phone comes with 3-D video content, generated by Dynamic Digital Depth, that can be viewed without special glasses. Dynamic Digital Depth expects that its software to convert 2-D games to 3-D on the fly will be built into phones within the next two years.
BRYAN CHRISTIE DESIGN
The Samsung B710 phone looks like a typical smart phone, but something unexpected happens when the screen is moved from a vertical to a horizontal orientation: the image jumps from 2-D to 3-D. The technology that produces this perception of depth is the work of Julien Flack, CTO of Dynamic Digital Depth, who has spent more than a decade perfecting software that can convert 2-D content to 3-D in real time. It could help solve the biggest problem with 3-D: the need for special glasses that deliver a separate image to each eye.

Flack’s software synthesizes 3-D scenes from existing 2-D video by estimating the depth of objects using various cues; a band of sky at the top of a frame probably belongs in the far background, for example. It then creates pairs of slightly different images that the viewer’s brain combines to produce the sensation of depth.

The technology can be used with the much-hyped 3-D televisions announced in January (which require glasses), but its biggest impact will be as a way to create content for mobile devices with auto­stereoscopic 3-D displays, which work by directing light to deliver different versions of an image directly to each of a viewer’s eyes. The effect works best over a narrow range of viewing angles, so it is ill suited to television or cinema screens. But phones are generally used by one person at a time and are easily held at the optimum angle. That’s why mobile multimedia devices are likely to win the race to bring 3-D into the mainstream.

Powered by Flack’s software, Dynamic Digital Depth has become an early leader in mobile 3-D. The software was built into the B710, which Samsung released in South Korea in 2007, and Samsung has licensed 3-D content generated by Dynamic Digital Depth for its latest 3-D phone, the W960, released in March. Research firm DisplaySearch recently predicted that by 2018 there will be 71 million such devices worldwide.

The most exciting area for Flack right now is games. Hundreds of games actually simulate 3-D spaces internally to handle mechanics such as the path of a missile, and then convert those 3-D spaces into 2-D to display to the player. With his technology, he says, the 3-D geometry “available inside the game itself” can be made accessible to the display. DDD has already released software that converts games to 3-D on PCs and expects to have similar software running on mobile devices in the next year or two.

It’s applications like mobile games and video that will drive the widespread adoption of 3-D screens. And that, in turn, could lay the groundwork for a new generation of surprising interfaces and applications, just as large 2-D screens on mobile devices spawned developments such as touch-based interfaces and augmented reality.

by Annalee Newitz",0.5% - 5.0%,Positive,Hardware,product
Engineered Stem Cells,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Engineered Stem Cells
Mimicking human disease in a dish.

The cell factory: James Thomson (above) and Junying Yu first transformed adult cells into stem cells called iPS cells in 2007.
KEVIN MIYAZAKI
The small plastic vial in James ­Thomson’s hand contains more than 1.5 billion carefully coddled heart cells grown at Cellular Dynamics, a startup based in Madison, WI. They are derived from a new type of stem cell that ­Thomson, a cofounder of the company, hopes will improve our models of human diseases and transform the way drugs are developed and tested.

Thomson, director of regenerative biology at the Morgridge Institute at the University of Wisconsin, first isolated human embryonic stem cells in 1998. Isolating these cells, which are capable of maturing into any other type of cell, marked a landmark in biology–but a controversial one, since the process destroys a human embryo. A decade later, ­Thomson and Junying Yu, then a Wisconsin postdoc, reached another milestone: they developed a way to make stem cells from adult cells by adding just four genes that are normally active only in embryos. (Japanese researcher Shinya Yamanaka simultaneously published a similar approach.) Dubbed induced pluripotent stem cells (iPS cells), they have the two defining characteristics of embryonic stem cells: they can reproduce themselves many times over, and they can develop into any cell type in the human body. Because no human embryos are used to create them, iPS cells solve two problems that had long plagued researchers: political protest and shortages of material.

Much of the excitement over iPS cells, and stem cells in general, arises from the possibility that they could replace damaged or diseased tissue. But Thomson thinks their most important contribution will be to provide an unprecedented window on human development and disease. Scientists can create stem cells from the adult cells of people with different disorders, such as diabetes, and induce them to differentiate into the types of cells damaged by the disease. This could allow researchers to watch the disease as it unfolds and trace the molecular processes that have gone awry.

In the nearer term, iPS cells may revolutionize toxicity testing for drugs. The cells are “the first unlimited source of any type of human tissue,” says Thomson, who founded Cellular Dynamics to put stem cells to practical use. The company sells heart muscle cells derived from its iPS cells to pharmaceutical giants such as Roche, which are using them to screen experimental drugs for harmful side effects. Thomson hopes those cells will help uncover problems early in the drug development process, saving billions of dollars on research and testing. For instance, since the iPS-derived heart cells will beat in a dish, scientists should be able to detect which drugs alter the heart’s rhythm. Scientists can also use the cells to study how the heart functions at a molecular level. And the company is developing other cell types, including brain and liver cells. The latter are of particular interest to pharmaceutical researchers, since drug toxicity often shows up in the liver. “Having a model that would predict toxicity before going into humans is incredibly valuable,” says Chris Parker, vice president and chief commercial officer of Cellular Dynamics.

By generating iPS cells from people with diverse ethnic backgrounds and genetic conditions, and from those who have reacted poorly to certain drugs, scientists can also gain a better picture of how compounds will affect different people. Thomson and others have already created iPS cells from people with ALS, Down syndrome, and spinal muscular atrophy, among other disorders. While it’s not yet clear how well those cells reflect the specific diseases, early research is promising. If it succeeds, researchers hope to use iPS cells to study other disorders and develop drugs to treat them. “That’s the thing that would fundamentally change the way drug development happens,” says Kyle Kolaja, director of early safety and investigative toxicology at Roche, which has partnered with Cellular Dynamics.

The last decade brought many difficult years for Thomson. His work on embryonic stem cells was a breakthrough, but it also brought intense controversy and media attention, turning him somewhat reclusive. With the rise of iPS cells and Cellular Dynamics, Thomson is beginning to come back to the limelight. “I think the legacy of embryonic stem cells will be that they gave rise to iPS cells,” he says. “These cells will be used in creative ways we can’t even imagine.”

by Emily Singer",0.01% to 0.1%,Positive,Biotech,product
Light-Trapping Photovoltaics,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Light-Trapping Photovoltaics
Nanoparticles boost solar power’s prospects.

Solar spheres: By depositing nanoparticles made of silver on thin-film photovoltaic cells, Kylie Catchpole increases the cells' efficiency, which could make solar power more competitive.
MEGHAN PETERSEN
In 1995, finishing her undergraduate degree in physics, Kylie Catchpole decided to take a risk on a field that was nearly moribund: photovoltaics. “There was a sense that I might have difficulty ever being employed,” she recalls. But her gamble paid off. In 2006 Catchpole, then a postdoc, discovered something that opened the door to making thin-film solar cells significantly more efficient at converting light into electricity. It’s an advance that could help make solar power more competitive with fossil fuels.

Thin-film solar cells, which are made from semiconductor materials like amorphous silicon or cadmium telluride, are cheaper to produce than conventional solar cells, which are made from relatively thick and expensive crystalline wafers of silicon. But they are also less efficient, because if a cell is thinner than the wavelength of incoming light is long, that light is less likely to be absorbed and converted. At just a few micro­meters thick, thin-film cells only weakly absorb wavelengths in the near-infrared part of the spectrum; that energy is lost. The result is that thin-film photovoltaics convert 8 to 12 percent of incoming light to electricity, versus 14 to 19 percent for crystalline silicon. Thus, larger installations are required in order to produce the same amount of electricity, limiting the number of places the technology can be used.

Catchpole, who is now a research fellow at the Australian National University in Canberra, began work on this problem in 2002 at the University of New South Wales in Sydney. “It was a case of ‘start at the beginning: can you think of a completely different way to make a solar cell?’ ” she says. “One of the things I came across was plasmonics–looking at the strange optical properties of metals.”

Plasmons are a type of wave that moves through the electrons at the surface of a metal when they are excited by incident light. Others had tried harnessing plasmonic effects to make conventional silicon photovoltaics more efficient, but no one had tried it with thin-film solar cells. Catchpole found that nanoparticles of silver she deposited on the surface of a thin-film silicon solar cell did not reflect back light that fell directly onto them, as would happen with a mirror. Instead, plasmons that formed at the particles’ surface deflected the photons so that they bounced back and forth within the cell, allowing longer wavelengths to be absorbed.

Catchpole’s experimental devices produce 30 percent more electrical current than conventional thin-film silicon cells. If Catchpole can integrate her nanoparticle technology with the processes used to mass-produce thin films commercially, it could shift the balance of technology used in solar cells. Thin-film photovoltaics could not only gain market share (they currently have just 30 percent of the market in the United States) but sustain growth in the solar industry overall.

Thus far, silicon has been losing out to cadmium telluride as the material of choice for thin-film solar cells. (First Solar, the market leader, is planning gigawatt-scale solar farms that will use cadmium telluride thin-film technology to deliver as much electricity as conventional power stations.) But tellurium is a rare material, and experts question whether the supply will support such grand ambitions. “There just isn’t enough tellurium to make a substantial difference to the way the world’s energy is produced,” says Catchpole. “Silicon is the way to go.”

Catchpole has been approached by companies, but she wants to refine the technology further before commercializing it. Meanwhile, researchers at Swinburne University of Technology in Melbourne are collaborating with Suntech Power, one of the world’s largest manufacturers of silicon solar cells, on plasmonic thin-film silicon cells of their own. The company’s plasmonic photovoltaics are expected to be ready for production within four years.

by Bob Johnstone",0.01-0.1%,Positive,Climate Tech,concept
Social TV,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Social TV
Relying on relationships to rebuild TV audiences.

The viewership for live television broadcasts has generally been declining for years. But something surprising is happening: events such as the winter Olympics and the Grammys are drawing more viewers and more buzz. The rebound is happening at least in part because of new viewing habits: while people watch, they are using smart phones or laptops to swap texts, tweets, and status updates about celebrities, characters, and even commercials.

Marie-José Montpetit, an invited scientist at MIT’s Research Lab for Electronics, has been working for several years on social TV–a way to seamlessly combine the social networks that are boosting TV ratings with the more passive experience of traditional TV viewing. Her goal is to make watching television something that viewers in different places can share and discuss–and to make it easier to find something to watch.

Carriers, networks, and content producers hope that making it easier for viewers to link up with friends will help them hold on to their audiences rather than losing them to services like Hulu, which stream shows over the Internet. And opening TV to social networking could make it easier for companies to provide personalized programming.

Many developers are working on ways to let people share the viewing experience over broadband connections or through set-top boxes; indeed, cable companies and other broadband video providers have sponsored small trials of various interactive TV services around the world for more than 20 years. But most of the systems were even clumsier than the combination of laptop and large-screen TV that today’s viewers have kludged together. ­Montpetit wants to unite different communication systems–especially cellular and broadband services–to create an elegant user experience. She’s been sharing ideas about that sort of system with BT, which provides broadband connections to 15 million people in the United Kingdom and Ireland, including nearly a half-million digital-TV subscribers.

Though BT won’t comment on what form its social-TV system might take, Montpetit and her students at the MIT Media Lab demonstrated an intriguing prototype last year. A central database aggregates video from online sources like YouTube, shares user-specified data with social networks, delivers video to the user’s TV, and lets users and the people in their networks send comments and ratings back and forth via an iPhone app. It avoids using the TV screen for messages, something that has proved irritating to consumers who don’t want clunky text obscuring the pictures on their 52-inch HDTVs. The app also allows the user to tell the network what program to show on his or her set. For instance, if a friend suggests a show and the owner agrees, that show will pop up at the appointed time. In February, Montpetit and her students presented a refined version of this system to BT. Jeff Patmore, who works with Montpetit as head of strategic university research at BT, says such a system could be rolled out this year, although he declines to confirm any plans. But Montpetit anxiously awaits U.S. deployment of social TV: her daughter, with whom she watches certain shows, heads off to college next fall. Engineering and business issues aside, she wants social TV to help friends and family stay connected, even as they move apart.

by William M. Bulkeley",0.05% - 0.5%,Positive,Software,project
Green Concrete,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Green Concrete
Storing carbon dioxide in cement.

NATO WELTON
Making cement for concrete involves heating pulverized limestone, clay, and sand to 1,450 °C with a fuel such as coal or natural gas. The process generates a lot of carbon dioxide: making one metric ton of commonly used Portland cement releases 650 to 920 kilograms of it. The 2.8 billion metric tons of cement produced worldwide in 2009 contributed about 5 percent of all carbon dioxide emissions. Nikolaos Vlasopoulos, chief scientist at London-based startup Novacem, is trying to eliminate those emissions with a cement that absorbs more carbon dioxide than is released during its manufacture. It locks away as much as 100 kilograms of the greenhouse gas per ton.

Vlasopoulos discovered the recipe for Novacem’s cement as a grad student at Imperial College London. “I was investigating cements produced by mixing magnesium oxides with Portland cement,” he says. But when he added water to the magnesium compounds without any Portland in the mix, he found he could still make a solid-setting cement that didn’t rely on carbon-rich limestone. And as it hardened, atmospheric carbon dioxide reacted with the magnesium to make carbonates that strengthened the cement while trapping the gas. Novacem is now refining the formula so that the product’s mechanical performance will equal that of Portland cement. That work, says ­Vlasopoulos, should be done “within a year.”

Other startups are also trying to reduce cement’s carbon footprint, including Calera in Los Gatos, CA, which has received about $50 million in venture investment. However, Calera’s cements are currently intended to be additives to Portland cement rather than a replacement like Novacem’s, says Franz-Josef Ulm, director of the Concrete Sustainability Hub at MIT. Novacem could thus have the edge in reducing emissions, but all the startups face the challenge of scaling their technology up to industrial levels. Still, Ulm says, this doesn’t mean a company must displace billions of tons of Portland cement to be successful; it can begin by exploiting niche areas in specialized construction. If Novacem can produce 500,000 tons a year, ­Vlasopoulos believes, it can match the price of Portland cement.

Even getting that far will be tough. “They are introducing a very new material to a very conservative industry,” says Hamlin Jennings, a professor in the Department of Civil and Environmental Engineering at Northwestern University. “There will be questions.” Novacem will start trying to persuade the industry by working with Laing O’Rourke, the largest privately owned construction company in the U.K. In 2011, with $1.5 million in cash from the Royal Society and others, Novacem is scheduled to begin building a new pilot plant to make its newly formulated cement.

by David Bradley",0.01% - 0.1%,Positive,Climate Tech,product
Implantable Electronics,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Implantable Electronics­
Dissolvable devices make better medical implants.
Implanted under the skin, an array of light-emitting diodes could signal the concentration in the blood of biomarkers such as insulin. Over time, the array will dissolve away, eliminating the need for surgery to remove the implant. Flexible silicon electronics (inset) are held in place with a silk film. Incorporating antibodies or enzymes into the film will allow devices to detect biomarkers.
Implanted under the skin, an array of light-emitting diodes could signal the concentration in the blood of biomarkers such as insulin. Over time, the array will dissolve away, eliminating the need for surgery to remove the implant. Flexible silicon electronics (inset) are held in place with a silk film. Incorporating antibodies or enzymes into the film will allow devices to detect biomarkers.
BRYAN CHRISTIE DESIGN
The next generation of implantable medical devices will rely on a high-tech material forged not in the foundry but in the belly of a worm. Tufts University biomedical engineer Fiorenzo Omenetto is using silk as the basis for implantable optical and electronic devices that will act like a combination vital-sign monitor, blood test, imaging center, and pharmacy–and will safely break down when no longer needed.

Implanted electronics could provide a clearer picture of what’s going on inside the body to help monitor chronic diseases or progress after surgery, but biocompatibility issues restrict their use. Many materials commonly used in electronics cause immune reactions when implanted. And in most cases today’s implantable devices must be surgically replaced or removed at some point, so it’s only worth using an implant for critical devices such as pacemakers. Silk, however, is biodegradable and soft; it carries light like optical glass; and while it can’t be made into a transistor or an electrical wire, it can serve as a mechanical support for arrays of electrically active devices, allowing them to sit right on top of biological tissues without causing irritation. Depending on how it’s processed, silk can be made to break down inside the body almost instantly or to persist for years. And it can be used to store delicate molecules like enzymes for a long time.

Omenetto began working with silk three years ago, when David Kaplan, a biomedical engineer across the hall, asked for help making the material into complex scaffolds for growing new tissues. He boils silkworm cocoons and purifies the resulting solution to create his master ingredient, a water-based solution of the silk protein called fibroin. This solution can be poured into molds to make structures whose features are as small as 10 nanometers across. Omenetto has molded it into a wide variety of optical devices, such as lenses, mirrors, prisms, and optical fibers, all of which could be used to direct light to and from biosensors implanted in the body. Mixing antibodies or enzymes into the silk solution before molding it results in devices that could someday be used to sense low concentrations of just about any biological molecule, from glucose to tumor markers.

Collaborating with Kaplan and ma­te­ri­als scientist John Rogers at the University of Illinois at Urbana-Champaign, ­Omenetto has produced implants that combine silk with flexible silicon electronics. For instance, the group has used silk films to hold in place arrays of tiny silicon transistors and LEDs–a possible basis for implantable devices that will help identify the concentration of disease markers. The researchers have shown that the devices function fine in small animals, with no evidence of scarring or immune response. The silk dissolves, leaving behind a small amount of silicon and other materials used in the circuits.

Another device uses silk as a substrate for a metal electrode mesh designed to replace spike-like electrodes used on the surface of the brain to diagnose and treat conditions such as epilepsy. When doused with saline solution, the silk wraps the mesh around the surface of the brain (even tucking it into the creases), helping the electrodes measure neural activity more precisely. The silk-based electrodes will probably be the first of the group’s devices to be tested in people, perhaps in two to three years.

Omenetto sees other possibilities further in the future: for example, a silk optical fiber could transmit light from an LED array to an implanted silk sensor, which would change color to indicate that a cancer has come back. The device might then release a precisely calibrated dose of a drug. A second silk fiber could transmit that information to the surface of the patient’s skin, where the output might be read by a cell phone. All the components for making such things exist, Omenetto says. Once the pieces are brought together, a little silk will help save lives.

by Katherine Bourzac",0.05% - 0.5%,Positive,Hardware,product
Cloud Programming,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Cloud Programming
A new language will improve online applications.

Joseph Hellerstein wants cloud programmers to reach new heights.
TOBY BURDITT
Cloud computing offers the promise of virtually unlimited processing and storage power, courtesy of vast data centers run by companies like Amazon and Google. But programmers don’t know how best to exploit this power.

Today, many developers are converting existing programs to run on clouds, rather than creating new types of applications that could work nowhere else. And they are held back by difficulties in keeping track of data and getting reliable information about what’s going on across a cloud. If programmers could solve those problems, they could start to really take advantage of what’s possible with a cloud. For example, an online music retailer could monitor popular social-media feeds; if a singer suddenly became a hot topic, advertising and special offers across the retailer’s site could be instantly reconfigured to make the most of the spike in interest.

At the University of California, Berkeley, Joseph Hellerstein thinks he can make it much easier to write complex cloud applications by developing software that takes over the job of tracking data and keeping tabs on what’s happening. His big idea is to modify database programming languages so that they can be used to quickly build any sort of application in the cloud–social networks, communication tools, games, and more. Such languages have been refined over the years to hide the complexities of shuffling information in and out of large databases. If one could be made cloud-friendly, programmers could just think about the results they want, rather than micromanaging data.

The challenge is that these languages process data in static batches. They can’t process data that is constantly changing, such as readings from a network of sensors. The solution, ­Hellerstein explains, is to build into the language the notion that data can be dynamic, changing as it’s being processed. This sense of time enables a program to make provisions for data that might be arriving later–or never.

The result is called Bloom. So far, Hellerstein’s group has used the Bloom language and its predecessors to quickly rebuild and add major features to popular cloud tools such as Hadoop, a platform used to manipulate very large amounts of data. By lowering the complexity barrier, these languages should increase the number of developers willing to tackle cloud programming, resulting in a wave of ideas for new types of powerful applications.

Hellerstein’s group is getting Bloom ready for a release in late 2010. They and others are also working on demonstrating how the techniques can be used for real-time applications such as online multiplayer games, or to watch for the warning signs of an earthquake or tsunami.

by Erica Naone",4% - 12%,Positive,Software,methodology
Dual-Action Antibodies,2010.0,https://www.technologyreview.com/10-breakthrough-technologies/2010/,,"Dual-Action Antibodies
Fighting cancer more efficiently.

Two for the price of one: Antibody drugs work by binding to antigens such as extracellular proteins or receptors on a cell's surface, blocking their ability to function or targeting them for destruction by the immune system. Genentech's new antibodies can bind to two different antigens, potentially reducing the number of drugs required to treat diseases. The antibody colored yellow can bind only to a receptor on the surface of a cancer cell. The green-colored antibody can bind only to an extracellular protein that promotes tumor growth. The new antibody (red) can bind to both the surface receptor and the extracellular protein.
BRYAN CHRISTIE DESIGN
At Genentech’s sprawling headquarters south of San Francisco, senior scientist Germaine Fuh has been genetically redesigning two of the company’s most lucrative cancer drugs. One, Herceptin, is a monoclonal antibody that shuts down HER2, a growth accelerator in about 20 percent of breast tumors. The other, Avastin, is an antibody that blocks a protein that stimulates the formation of tumor-­feeding blood vessels. Last year the drugs had combined sales of $11 billion; a full course of Herceptin at wholesale costs about $43,000, while treating a breast cancer patient with a full course of Avastin costs about $55,000. Fuh’s goal: to show she can provide greater benefit for people fighting breast cancer by combining the action of the antibodies in one molecule. Last year, she and her coworkers showed that a modified version of the Herceptin antibody not only shut down the HER2 receptor in mice but also locked onto VEGF, Avastin’s target.

Designing such “dual-specific” antibodies could help solve a major problem with chemotherapy drugs: cancer cells can become resistant to them, mutating in ways that allow them to dodge the medication’s action. Doctors often mix various chemotherapy drugs in an effort to kill cancers before they can exploit this escape mechanism. Having a single drug that can hit the cancer from multiple directions would simplify treatment.

A single monoclonal antibody that could do the work of two is also attractive from a business perspective. It might cost half as much to manufacture as two separate antibodies, and the path to regulatory approval might also be shorter and less expensive, involving one set of clinical trials instead of multiple trials for two separate drugs in various dosage combinations. Genentech has started trials to determine whether Herceptin and Avastin together will fight breast cancer better than either used alone, but the cost of such studies is a big disincentive to doing them regularly.

Fuh’s research into whether one antibody drug could be redesigned to do the work of two began six years ago. An antibody, one of the immune system’s most robust weapons, is a Y-shaped protein about 10 nanometers long. At the tip of each branch is an active site, which grabs a specific molecule on an invading microbe or cancer cell. Swarms of antibodies disable the invader, marking it for destruction by white blood cells or other immune molecules.

Fuh notes that many mammalian antibodies have some ability to bind to a second antigen, but typically they do so weakly. Her goal was to exploit this ability while making both bonds tight and functional. Fuh’s team induced subtle mutations at the tips of Herceptin and screened 10 billion mutant clones for activity against VEGF. They netted several candidates–including one with active sites that could bind to both HER2 and VEGF strongly enough to limit tumor growth.

Genentech, now a wholly owned subsidiary of Swiss pharmaceutical giant Roche, is using this technique to develop another dual-specific drug, which may soon be ready for clinical trials. Fuh won’t disclose the details of what the drug is for and will only say, “Right now, we are very close.”

Meanwhile, her experiments have fueled interest in the overall potential of such drugs. “The two-for-one drug concept is important, especially for indications like cancer,” says Carlos Barbas III, a professor of molecular biology at the Scripps Research Institute in La Jolla, CA. Barbas is the founder of CovX, a company working on a different approach to making dual-specific antibodies (Pfizer acquired it in 2008). Despite the competition, he praises the accomplishment of the Genentech team as “a beautiful piece of antibody engineering.”

The implications of Fuh’s research are indeed far-reaching. If the concept proves successful, antibodies that stick to two targets might be used to treat infectious diseases as well as cancer–offering the promise of drugs that work better and cost less.

by Sabin Russell",0.01% - 1.00%,Positive,Biotech,Product
Social Indexing,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Social Indexing
Facebook remaps the Web to personalize online services
Results you’ll like: Bret Taylor, who wields Facebook’s “Like” button.
Results you’ll like: Bret Taylor, who wields Facebook’s “Like” button.
Bret Taylor wants to make online services more attuned to what you really want. Search sites could take your friends’ opinions into account when you look for restaurants. Newspaper sites could use their knowledge of what’s previously captured your attention online to display articles you are interested in. “Fundamentally, the Web would be better if it were more oriented around people,” says Taylor, who is Facebook’s chief technology officer. To bring this idea to fruition, he is creating a kind of social index of the most frequently visited chunks of the Web.

Many sites have tried to personalize what they offer by remembering your past behavior and showing information they presume will be relevant to you. But the social index could be much more powerful because it also mines your friends’ interests and collects information from multiple sites. As a result, the index can give websites a sense of what is likely to interest you even if you’ve never been there before.

This ambitious project gets much of its information from the simple “Like” button, a thumbs-up logo that adorns many Web pages and invites visitors to signal their appreciation for something—a news story, a recipe, a photo—with a click. Taylor created the concept in 2007 at FriendFeed, a social network that he cofounded, which was acquired by Facebook in 2009. Back then, the button was just a way to encourage people to express their interests, but in combination with Facebook’s user base of nearly 600 million people, it is becoming a potent data-collecting tool. The code behind the Like button is available to any site that wants to add it to its pages. If a user is logged in to Facebook and clicks the Like button anywhere on the Web, the link is shared with that person’s Facebook friends. Simultaneously, that thumbs-up vote is fed into Taylor’s Web-wide index.

That’s how the Wall Street Journal highlights articles that a person’s friends enjoyed on its site. This is what lets Microsoft’s Bing search engine promote pages liked by a person’s friends. And it’s how Pandora creates playlists based on songs or bands a person has appreciated on other sites.

This method of figuring out connections between pieces of content is fundamentally different from the one that has ruled for a decade. Google mathematically indexes the Web by scanning the hyperlinks between pages. Pages with many links from other sites rise to the top of search results on the assumption that such pages must be relatively useful or interesting. The social index isn’t going to be a complete replacement for Google, but for many types of activity—such as finding products, entertainment, or things to read—the new system’s personal touch could make it more useful.

Google itself acknowledges this: it recently rolled out a near-clone of the Like button, which it calls “+1.” It lets people signify for their friends which search results or Web pages they’ve found useful. Google is also using Twitter activity to augment its index. If you have connected your Twitter and Google accounts, Web links that your friends have shared on Twitter may come up higher in Google search results.

Another advantage of a social index is that it could be less vulnerable to manipulation: inflating Google rankings by creating extra links to a site is big business, but buying enough Facebook likes to make a difference is nearly impossible, says Chris Dixon, cofounder of Hunch, a Web startup that combines its own recommendation technology with tools from Facebook and Twitter. “Social activity provides a really authentic signal of what is authoritative and good,” says Dixon. That’s why Hunch and other services, including an entertainment recommendation site called GetGlue, are building their own social indexes, asking people to record their positive feelings about content from all over the Web. If you’re browsing for something on Amazon, a box from GetGlue can pop up to tell you which of your friends have liked that item.

A social index will be of less use to people who don’t have many online connections. And even Facebook’s map covers just a small fraction of the Web for now. But about 10,000 additional websites connect themselves to Facebook every day.

by Tom Simonite",2-20%,Negative,Software,Platform
Smart Transformers,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Smart Transformers
Controlling the flow of electricity to stabilize the grid
Powerful electronics: The smart transformer can handle AC and DC power and, thanks to semiconductors capable of handling high voltages, be programmed to redirect the flow of electricity in response to fluctuations in supply and demand.
A. High-voltage semiconductor-based AC rectifier.
B. High-voltage semiconductor-based DC converter.
C. High-frequency transformers.
D. Control circuitry.
Powerful electronics: The smart transformer can handle AC and DC power and, thanks to semiconductors capable of handling high voltages, be programmed to redirect the flow of electricity in response to fluctuations in supply and demand. A. High-voltage semiconductor-based AC rectifier. B. High-voltage semiconductor-based DC converter. C. High-frequency transformers. D. Control circuitry.
In a lab wired up to simulate a residential neighborhood, Alex Huang is working to revamp aging power grids into something more like the Internet—a network that might direct energy not just from centralized power stations to consumers but from any source to any destination, by whatever route makes the most sense. To that end, Huang, a professor of electrical engineering at North Carolina State University, is reinventing the transformers that currently reduce the voltage of the electricity distributed to neighborhoods so that it’s suitable for use in homes and offices.

His new transformer will make it easier for the grid to cope with things it was never designed for, like charging large numbers of electric vehicles and tapping surplus electricity from residential solar panels. Smart meters in homes and offices can help by providing fine-grained information about the flow of electricity, but precise control over that flow is needed too. Not only would this stabilize the grid, but it would better balance supply and demand, reducing spikes so that fewer power plants would be needed to guarantee the electricity supply.

“We need a radically new device to sit between homes and grid to provide a buffer, so that the grid will remain stable no matter what is going on in the homes,” Huang says. Conventional transformers handle only AC power and require manual adjustment or bulky electromechanical switches to redirect energy. What he wants is a compact transformer that can handle DC as well as AC and can be electronically controlled so that it will respond almost instantaneously to fluctuations in supply and demand. If one neighbor plugged an electric car into an AC charger, for example, it could respond by tapping otherwise unneeded DC power from another neighbor’s solar panels.

To build such a transformer, Huang started developing transistors and other semiconductor-based devices that can handle thousands of volts, founding the Future Renewable Electric Energy Delivery and Management Systems Center at NC State in 2008. His first transformer had silicon-based components, but silicon is too unreliable for large-scale use at high voltages. So Huang has pioneered the development of transformers with semiconductors based on compounds of silicon and carbon or gallium and nitrogen, which are more reliable in high-power applications. He expects to have a test version of the silicon-carbon transformer ready in two years and to have a device that utilities can test in five years.

Huang’s transformers would make connecting a solar panel or electric car to the grid as simple as connecting a digital camera or printer to a computer. That would reduce our reliance on fossil fuels by making it easier for small-scale sources of cleaner energy to contribute to the grid. He says, “The real benefit to society will come when there’s an aggregate effect from many, many small generators, which we hope will be renewable and sustainable energy sources.”

by David H. Freedman",0.05% - 0.5%,Positive,Hardware,product
Gestural Interfaces,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Gestural Interfaces
Controlling computers with our bodies
Determining depth: PrimeSense’s sensor determines depth by combining a number of techniques, including structured light, where an infrared pattern (red lines) is projected onto objects. How the pattern is distorted gives information about distances. The illustrated example here is an interactive airport information display (gray box), below is the depth sensor (blue box).
Determining depth: PrimeSense’s sensor determines depth by combining a number of techniques, including structured light, where an infrared pattern (red lines) is projected onto objects. How the pattern is distorted gives information about distances. The illustrated example here is an interactive airport information display (gray box), below is the depth sensor (blue box).
How do you issue complex commands to a computer without touching it? It’s a crucial issue now that televisions are connected to social networks and cars are fitted with computerized systems for communication, navigation, and entertainment. So Alexander Shpunt has designed a 3-D vision system that lets anyone control a computer just by gesturing in the air.

Shpunt spent five years developing the system at Tel Aviv-based PrimeSense, and Microsoft adopted the technology to power its popular Kinect controller for the Xbox 360 game console. Players can use it to direct characters with their bodies alone—no need for the wands, rings, gloves, or colored tags that previous gestural interfaces relied on to detect the user’s movements.

The key to dispensing with those props was getting the computer to see the world in three dimensions, rather than the two captured by normal cameras. Sensing depth makes it relatively easy to distinguish, say, an arm from a table in the background, and then track the arm’s movement.

Shpunt recalls that when he started developing his system there were a few ways to sense depth—primarily “time of flight” (determining distance from a sensor by measuring how long it takes light or sound to bounce off an object) and “structured light” (projecting patterns of light onto objects and analyzing how the patterns are distorted by the object’s surface). Although there was a lot of academic activity and a few companies built prototypes, there was “nothing really mature” that could be mass-produced, he says. Instead, he built his own system, cobbling together an approach that borrowed from those two techniques as well as stereoscopy—comparing images of the same scene from two different viewpoints.

The Kinect is only the beginning of what Shpunt believes will be a gestural-interface revolution. A small army of hackers, encouraged by PrimeSense, is already retooling the controller to other ends. Researchers at Louisiana State University have rigged a helmetless, gloveless virtual-reality system out of a Kinect unit and an off-the-shelf 3-D TV set. In Australia, a logistics software firm quickly put together a gesture-controlled system for monitoring air traffic. Further real-world applications are easy to imagine, says Shpunt: gaze-tracking heads-up controls for automobiles, touchless interactive displays for shopping malls and airports.

For now, Shpunt is working with computer maker Asus to build gestural controls for today’s increasingly complex and network-connected televisions—essentially turning a TV into a giant iPad that can be operated from the couch without a remote control.

by Julian Dibbell",0.05% - 0.5%,Positive,Software,Platform
Cancer Genomics,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Cancer Genomics
Deciphering the genetics behind the disease
Decoding cancer: Elaine Mardis uses sequencing to study the genomes of diseased cells.
Decoding cancer: Elaine Mardis uses sequencing to study the genomes of diseased cells.
In the fall of 2006, a new machine arrived at what’s now known as the Genome Institute at Washington University in St. Louis. It was capable of reading DNA a thousand times as quickly as the facility’s earlier machines, and at far less cost. Elaine Mardis, the center’s codirector, immediately began using it to sequence cancer tissues, scouring their DNA for mutations. Just five years later, Mardis and her collaborators have sequenced both cancerous and healthy tissue from several hundred patients and identified tens of thousands of mutations. Some of the findings have led to new approaches to treating cancer, while others have opened new avenues of research.

Cancer develops when cells accumulate genetic mistakes that allow them to grow and divide faster than healthy cells. Identifying the mutations that underlie this transformation can help predict a patient’s prognosis and identify which drugs are most likely to work for that patient. The information could also identify new targets for cancer drugs. “In a single patient, you have both the tumor genome and the normal genome,” Mardis says. “And you can get at answers much more quickly by comparing the two.”

In 2008, Mardis and her team became the first to publish the sequence of a cancer genome, derived by comparing the DNA of healthy and cancerous cells in a patient with a bone marrow cancer called AML. Further studies have suggested that patients with mutations in a particular gene may fare better with bone marrow transplants than with traditional chemotherapy, a less risky treatment that physicians usually try first. Mardis predicts that soon all AML patients will be genetically tested, allowing their physicians to make more informed decisions about treatment.

As the cost and speed of DNA sequencing have dropped—Mardis estimates that sequencing genomes from a patient’s cancerous and healthy tissue today costs about $30,000, compared with $1.6 million for the first AML genome—the technology is being applied to oncology more broadly. Research groups have now sequenced the genomes of multiple cancers, and in a handful of cases, they have used the results to guide treatment decisions for a patient (see “Cancer’s Genome,” January/February 2011). A few companies are now offering cancer genome analysis to researchers, and at least one is planning to offer the service to physicians and patients.

The decreasing cost of sequencing also means that Mardis can use the technology in drug development and testing. Her latest project is part of a clinical trial assessing hormone therapy for breast cancer. She has developed a preliminary genetic profile of cancers most likely to respond to a popular set of drugs called aromatase inhibitors, which are given to most breast cancer patients whose tumor cells have estrogen receptors on the surface. The goal is to identify the patients who will benefit from the drugs and those who won’t. (Preliminary evidence suggests that only about half the patients in the trial respond to the drugs.)

Understanding cancer genomes isn’t easy. Mardis’s team had to invent techniques to distinguish the rare cancer mutations from the mistakes that routinely occur when sequencing DNA. And scientists must figure out which mutations are actually driving the growth of the tumors and which are harmless. Then comes what might be the most challenging part: determining how the mutations trigger cancer. Mardis says she is leaving that challenge to the scientists around the world who are working to understand the mutations that she and others have identified. “It’s really gratifying to see others pick that up,” she says.

by Emily Singer",0.01% to 0.1%,Positive,Biotech,methodology
Solid-State Batteries,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Solid-State Batteries
High-energy cells for cheaper electric cars
Charged up: Sakti3’s Ann Marie Sastry is developing ways to mass-produce lighter and cheaper batteries.
Charged up: Sakti3’s Ann Marie Sastry is developing ways to mass-produce lighter and cheaper batteries.
Ann Marie Sastry wants to rid electric vehicles’ battery systems of most of the stuff that doesn’t store energy, such as cooling devices and supporting materials within the battery cells. It all adds up to more than half the bulk of typical lithium-ion-based systems, making them cumbersome and expensive. So in 2007, she founded a startup called Sakti3 to develop solid-state batteries that don’t require most of this added bulk. They save even more space by using materials that store more energy. The result could be battery systems half to a third the size of conventional ones.

Cutting the size of a battery system in half could cut its cost by as much as half, too. Since the battery system is the most expensive part of an electric car (often costing as much as $10,000), that would make electric cars far cheaper. Alternatively, manufacturers could keep the price constant and double the 100-mile range typical of electric cars.

The limitations of the lithium-ion batteries used in electric cars are well known. “Most liquid electrolytes are flammable. The cathode dissolves,” says Sastry. Keeping the electrolyte from bursting into flames requires safety systems. And to extend the electrode’s lifetime and prevent heat buildup, the battery must be cooled and prevented from ever fully charging or discharging, resulting in wasted capacity. All this adds bulk and cost. So Sastry wondered if she could make a battery that simply didn’t need this much management.

Sastry’s solid-state batteries are still based on lithium-ion technology, but they replace the liquid electrolyte with a thin layer of material that’s not flammable. Solid-state batteries are also resilient: some prototypes demonstrated by other groups can survive thousands of charge-discharge cycles. And they can withstand high temperatures, which will make it possible to use materials that can double or triple a battery’s energy density (the amount of energy stored in a given volume) but that are too dangerous or unreliable for use in a conventional lithium-ion battery.

To make solid-state batteries that are practical and inexpensive to produce, Sastry has written simulation software to identify combinations of materials and structures that will yield compact, reliable high-energy devices. She can simulate these materials and components precisely enough to accurately predict how they will behave when assembled together in a battery cell. She is also developing manufacturing techniques that lend themselves to mass production. “If your overall objective is to change the way people drive, your criteria can no longer only be the best energy density ever achieved or the greatest number of cycles,” she says. “The ultimate criterion is affordability, in a product that has the necessary performance.”

Although it may be several years before the batteries come to market, GM and other major automakers, such as Toyota, have already identified solid-state batteries as a potentially key component of future electric vehicles. There’s a limit to how much better conventional batteries can get, says Jon Lauckner, president of GM Ventures, which pumped over $3 million into Sakti3 last year. If electric vehicles are ever to make up more than a small fraction of cars on the road, “something fundamental has to change,” he says. He believes that Sakti3 is “working well beyond the limits of conventional electrochemical cells.”

Sastry is aware that success isn’t guaranteed. Her field is something of a technological battleground, with many different approaches competing to power a new generation of cars. “None of this is obvious,” she says.

by Kevin Bullis",0.01% - 0.1%,Positive,Hardware,product
Homomorphic Encryption,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Homomorphic Encryption
Making cloud computing more secure
Ciphering: Gentry’s system allows encrypted data to be analyzed in the cloud. In this example, we wish to add 1 and 2. The data is encrypted so that 1 becomes 33 and 2 becomes 54. The encrypted data is sent to the cloud and processed: the result (87) can be downloaded from the cloud and decrypted to provide the final answer (3).
Ciphering: Gentry’s system allows encrypted data to be analyzed in the cloud. In this example, we wish to add 1 and 2. The data is encrypted so that 1 becomes 33 and 2 becomes 54. The encrypted data is sent to the cloud and processed: the result (87) can be downloaded from the cloud and decrypted to provide the final answer (3).
Craig Gentry is creating an encryption system that could solve the problem keeping many organizations from using cloud computing to analyze and mine data: it’s too much of a security risk to give a public cloud provider such as Amazon or Google access to unencrypted data.

The problem is that while data can be sent to and from a cloud provider’s data center in encrypted form, the servers that power a cloud can’t do any work on it that way. Now Gentry, an IBM researcher, has shown that it is possible to analyze data without decrypting it. The key is to encrypt the data in such a way that performing a mathematical operation on the encrypted information and then decrypting the result produces the same answer as performing an analogous operation on the unencrypted data. The correspondence between the operations on unencrypted data and the operations to be performed on encrypted data is known as a homomorphism. “In principle,” says Gentry, “something like this could be used to secure operations over the Internet.”

With homomorphic encryption, a company could encrypt its entire database of e-mails and upload it to a cloud. Then it could use the cloud-stored data as desired—for example, to search the database to understand how its workers collaborate. The results would be downloaded and decrypted without ever exposing the details of a single e-mail.

Gentry began tackling homomorphic encryption in 2008. At first he was able to perform only a few basic operations on encrypted data before his system started producing garbage. Unfortunately, a task like finding a piece of text in an e-mail requires chaining together thousands of basic operations. His solution was to use a second layer of encryption, essentially to protect intermediate results when the system broke down and needed to be reset.

“The problem of how to create true homomorphic encryption has been debated for more than 30 years, and Craig was the first person who got it right and figured out how to make the math work,” says Paul Kocher, the president of the security firm Cryptography Research. However, Kocher warns, because Gentry’s scheme currently requires a huge amount of computation, there’s a long way to go before it will be widely usable.

Gentry acknowledges that the way he applied the double layer of encryption was “a bit of a hack” and that the system runs too slowly for practical use, but he is working on optimizing it for specific applications such as searching databases for records. He estimates that these applications could be ready for the market in five to 10 years.

by Erica Naone",0.01% - 0.1%,Positive,Software,methodology
Cloud Streaming,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Cloud Streaming
Bringing high-performance software to mobile devices
This computationally intensive 3-D animation software appears to be running on a tablet, but is actually running on OnLive’s remote servers.
This computationally intensive 3-D animation software appears to be running on a tablet, but is actually running on OnLive’s remote servers.
In the Silicon Valley conference room of OnLive, Steve Perlman touches the lifelike 3-D face of a computer-­generated woman displayed on his iPad. Swiping the screen with his fingers, Perlman rotates her head; her eyes move to compensate, so that she continues to stare at one spot. None of this computationally intensive animation and visualization is actually taking place on the iPad. The device isn’t powerful enough to run the program responsible—an expensive piece of software called Autodesk Maya. Rather, Perlman’s finger-swipe inputs are being sent to a data center running the software. The results are returned as a video stream that seems to respond instantaneously to his touch.

To make this work, Perlman has created a way of compressing a video stream that overcomes the problems marring previous attempts to use mobile devices as remote terminals for graphics-intensive applications. The technology could make applications such as sophisticated movie-editing or architectural-design tools accessible on hundreds of millions of Internet-­connected tablets, smart phones, and the like. And not only professional animators and architects would benefit. For consumers, it will allow streaming movies to be fast-forwarded and rewound in real time, as with a DVD player, while schools anywhere could gain easy access to software. “The long-term vision is actually to move all computing out to the cloud,” says Perlman, OnLive’s CEO.

Perlman’s biggest innovation is dispensing with the buffers that are typically used to store a few seconds or minutes of streaming video. Though buffers allow time for any lost or delayed data to be re-sent before it’s needed, they create a lag that makes it impossible to do real-time work. Instead, Perlman uses various strategies to fill in or hide missing details—in extreme cases even filling in entire frames by extrapolating from frames received earlier—so that the eye does not detect a problem should some data get lost or delayed. The system also continually checks the network connection’s quality, increasing the amount of video compression and decreasing bandwidth requirements as needed. To save precious milliseconds, Perlman has even negotiated with Internet carriers to ensure that data from his servers is carried directly on high-speed, high-capacity Internet backbones.

The goal is to respond to user inputs within 80 milliseconds, a key threshold for visual perception. Reaching that threshold is crucial for a broad range of applications, says Vivek Pai, a computer scientist at Princeton University: “If you see a delay between what you are doing and the result of what you are doing, your brain drifts off.”

Perlman founded OnLive in 2007 to commercialize his streaming technology, and last year he launched a subscription service offering cloud-based versions of popular action games, a particularly demanding application in terms of computing power and responsiveness. But games are just a start—OnLive’s investors include movie studio Warner Brothers and Autodesk, which, besides Maya, also makes CAD software for engineers and designers. Perlman believes that eventually, “any mobile device will be able to bring a huge level of computing power to any person in the world with as little as a cellular connection.”

by David Talbot",0.5% - 5%,Positive,Software,Platform
Crash-Proof Code,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Crash-Proof Code
Making critical software safer
Fail-safe: June Andronick uses mathematical analysis to create crash-proof software.
Fail-safe: June Andronick uses mathematical analysis to create crash-proof software.
When a computer controls critical systems in vehicles and medical devices, software bugs can be disastrous: “unnecessarily risky” programs could put lives in danger, says June Andronick, a researcher at NICTA, Australia’s national IT research center. As a result of one recently discovered software vulnerability, she notes by way of example, “a car could be controlled by an attack on its stereo system.” She is trying to reduce these risks by making the most important part of an operating system—the core, or kernel—in such a way that she can prove it will never crash.

The currently favored approach to creating reliable software is to test it under as many conditions as time and imagination allow. Andronick instead is adapting a technique known as formal verification, which microchip designers use to check their designs before making an integrated circuit: they create a mathematical representation of the chip’s subsystems that can be used to prove that the chip will behave as intended for all possible inputs. Until now, formal verification was considered impractical for large programs such as operating systems, because analyzing a representation of the program code would be too complicated. But in a computing and mathematical tour de force, Andronick and her colleagues, working in Gerwin Klein’s lab at NICTA, were able to formally verify the code that makes up most of the kernel of an operating system designed for processors often found embedded in smart phones, cars, and electronic devices such as portable medical equipment. Because this code is what ultimately passes software instructions from other parts of the system to hardware for execution, bulletproofing it has a major impact on the reliability of the entire system.

“The work is hugely significant,” says Lawrence Paulson, a computer science professor at the University of Cambridge. Beyond showing that there’s no bug in the kernel that could cause it to crash, he says, the verification guarantees that the kernel will perform, without error, every function it was programmed to perform.

The task was made a little easier by the choice to develop a so-called microkernel. Microkernels delegate as many functions as possible—such as handling input and output—to software modules outside the kernel. Consequently, they are relatively small—in this case, about 7,500 lines of C code and 600 lines of assembler. “That’s really small for a kernel, but really large for formal verification,” Andronick says. The analysis was targeted at the thousands of lines of C code; new software and mathematical tools had to be developed for the task. The kernel was released in February, and the team is working on another version designed for the popular line of x86 processor chips.

Andronick doesn’t expect that the technique will scale to much larger software, but she doesn’t think it has to. Using verified code in key subsystems would allow developers to make sure that bugs in less rigorously examined programs—such as those used to interface with a car stereo—can’t affect critical hardware. It could also prevent a computer from locking up if it encounters a problem. Andronick wants more software developers to embrace formal verification “in fields where safety and security really matter,” she says. “We show that it is possible.”

by William Bulkeley",0.01% - 0.1%,positive,Software,Methodology
Separating Chromosomes,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Separating Chromosomes
A more precise way to read DNA will change how we treat disease
Chromosome chip: This matchbox-size device uses tiny valves, channels, and chambers to separate the 23 pairs of chromosomes in the human genome so they can be analyzed individually.
Chromosome chip: This matchbox-size device uses tiny valves, channels, and chambers to separate the 23 pairs of chromosomes in the human genome so they can be analyzed individually.
The clear rubber chip sitting under a microscope in Stephen Quake’s lab is a complex maze of tiny channels, chambers, and pumps, hooked up to thin plastic tubes that supply reagents and control 650-plus minuscule valves. Using this microfluidic chip, Quake, a biophysicist at Stanford University, has engineered a way of obtaining data that’s missing from nearly all human genome sequences: which member of a pair of chromosomes a gene belongs to.

Technology that makes it easier to identify the variations between chromosomes could have a huge impact on fundamental genomic research and personalized medicine. “This is definitely the next frontier,” says Nicholas Schork, a statistical geneticist at the Scripps Research Institute. Right now, he says, “we’re missing out on all sorts of biological phenomena that occur as a result of humans’ having [paired chromosomes].”

When scientists sequence human genomes, they largely ignore the fact that chromosomes come in pairs, with one copy inherited from the mother and one from the father. (The Y chromosome, which determines gender, is the exception.) Standard techniques blend genetic data from the two chromosomes to yield a single sequence.

Quake’s alternative is to physically separate chromosomes before genomic analysis. Cells are piped into the chip; when Quake spots one that’s preparing to divide (a stage at which the chromosomes are easier to manipulate), he traps the cell in a chamber and bursts its membrane, causing the chromosomes to spill out. They are randomly distributed into 48 smaller chambers. While it is possible for more than one chromosome to end up in a single chamber, it’s very unlikely that a chromosome will end up with its pair. Using standard techniques, the chromosomes are then sequenced or screened for genetic variants.

Other groups have pursued different strategies to sequence individual chromosomes. But Quake thinks his has an advantage because it doesn’t rely on decoding and reconstructing chromosomes from a mixed pool of DNA fragments, as others do. “By the way we physically prepare the sample, we know [the result is] right,” he says.

If costs can come down enough, Quake’s technique will be widely used, says Meredith Yeager, a senior scientist at the National Cancer Institute’s Core Genotyping Facility. The ability to routinely tell where genetic variants lie on different chromosomes “really is a big deal,” Yeager says. “Context matters.”

For example, if testing detects two separate mutations in a disease-related gene, it’s now impossible to tell whether one chromosome has both mutations or each chromosome has one. A patient who has at least one good copy of the gene is much more likely to escape the disease or experience it in a relatively mild form. Whether the aim is to predict responses to an asthma drug or to find better matches for bone marrow transplants, the accuracy of personalized medicine could eventually hinge on understanding the variation between chromosomes.

Fluidigm, the South San Francisco company that Quake cofounded in 1999 to commercialize microfluidic chips, is now looking at ways to automate the chromosome separation chip so that it doesn’t require so much expertise to use. Quake hopes to discover “something really interesting” about human diversity or the region of the genome that defines immune system responses. This region has been difficult to understand because it has so much genetic variation, and scientists lacked a tool to study it carefully—until now.

by Ingfei Chen",0.05% to 0.5%,Positive,Biotech,product
Synthetic Cells,2011.0,https://www.technologyreview.com/10-breakthrough-technologies/2011/,,"Synthetic Cells
Designing new genomes could speed the creation of vaccines and biofuel-producing bacteria
1: Bacterial genomes take the form of rings of DNA. An artificial genome is designed on a computer, including a sequence that “watermarks” the genome (red arc) and one that confers resistance to antibiotics (yellow arc). The genome is then synthesized as 1,078 overlapping DNA fragments.
2: Yeast cells stitch together 10 sequential fragments at a time. The longer strands that are produced are in turn stitched together by other yeast cells, and the process is repeated until copies of the whole genome are assembled.
3: The synthetic genomes are added to a colony of bacteria. Some of the bacterial cells entering the process of division absorb the synthetic genomes alongside their own.
4: When the bacterial cells divide, each daughter inherits one genome. An antibiotic is used to kill cells with the natural genome, leaving a colony of bacteria with the synthetic genome.
1: Bacterial genomes take the form of rings of DNA. An artificial genome is designed on a computer, including a sequence that “watermarks” the genome (red arc) and one that confers resistance to antibiotics (yellow arc). The genome is then synthesized as 1,078 overlapping DNA fragments. 2: Yeast cells stitch together 10 sequential fragments at a time. The longer strands that are produced are in turn stitched together by other yeast cells, and the process is repeated until copies of the whole genome are assembled. 3: The synthetic genomes are added to a colony of bacteria. Some of the bacterial cells entering the process of division absorb the synthetic genomes alongside their own. 4: When the bacterial cells divide, each daughter inherits one genome. An antibiotic is used to kill cells with the natural genome, leaving a colony of bacteria with the synthetic genome.
The bacteria growing on stacks of petri dishes in Daniel Gibson’s lab are the first living creatures with a completely artificial genome. The microbes’ entire collection of genes was edited on a computer and assembled by machines that create genetic fragments from chemicals and by helper cells that pieced those fragments together. Gibson hopes that being able to design and create entire genomes, instead of just short lengths of DNA, will dramatically speed up the process of engineering microbes that can carry out tasks such as efficiently producing biofuels or vaccines.

Until last year, biologists hadn’t been able to make large enough pieces of DNA to create an entire genome; though living cells routinely make long stretches of DNA, a DNA synthesis machine can’t do the same. In May, ­Gibson and his colleagues at the J. Craig Venter Institute announced their solution to this problem. Gibson used yeast cells to stitch together thousands of fragments of DNA made by a machine, pooled the longer pieces, and repeated the process until the genome was complete. Next he inserted the genome into bacterial cells that were about to divide and grew the bacteria in a medium hostile to all cells except the ones harboring the synthetic genome.

“When we began in 2004,” he says, “assembling a complete bacterial genome didn’t seem like an easy thing to do”—even though the Venter Institute researchers started with one of the smallest bacterial genomes that have been sequenced, that of a mycoplasma. After finally overcoming the technical hurdles involved, Gibson says, creating the synthetic cell itself was exciting but almost anticlimactic. Going from computer screen to bacterial colony now seems easy.

Gibson has also developed a faster, yeast-free way to assemble large pieces of DNA in a bottle. His colleagues are using these methods to rapidly synthesize the viral DNA needed to speed up the production of influenza vaccines. The nonprofit Venter Institute is working with Synthetic Genomics, a company that commercializes work done at the institute, to develop products.

The creation of the synthetic cell is part of an effort to design a “minimal cell” containing only the most basic genome required for life. Gibson and his colleagues at the Venter Institute believe that synthetic biologists could use this minimal cell as the basis for cells that efficiently produce biofuels, drugs, and other industrial products.

Right now, Gibson’s technique for incorporating his synthetic genome into living cells works only with mycoplasmas, which are useful for experimentation but not for industrial purposes. If Gibson can adapt this system to work with a broader group of bacteria, it could be used to speed up the process of engineering microbes that make a wide variety of products. At least two major challenges remain: developing appropriate recipient cells for genome transplants, and finding ways of working with even larger pieces of DNA. “We’re still in the early stages,” he says, “and we don’t know what the limits are.”

by Katherine Bourzac",0.01% - 0.1%,Positive,Biotech,concept
Egg Stem Cells,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Egg Stem Cells
A recent discovery could increase older women’s chances of having babies.
Jonathan Tilly may have discovered a way to slow the ticking of women’s biological clocks. In a paper published in March, the Harvard University reproductive biologist and his colleagues reported that women carry egg stem cells in their ovaries into adulthood—a possible key to extending the age at which a woman might have a baby.
Jonathan Tilly may have discovered a way to slow the ticking of women’s biological clocks. In a paper published in March, the Harvard University reproductive biologist and his colleagues reported that women carry egg stem cells in their ovaries into adulthood—a possible key to extending the age at which a woman might have a baby.
Today, a woman’s fertility is limited  by her total supply of eggs and by the diminished quality of those eggs as she reaches her 40s. Tilly’s work with the stem cells—cells that can differentiate, or develop into other kinds of cells—could address both issues. For one thing, it’s possible that these newly discovered cells could be coaxed to develop into new eggs. And even if not, he says, they could be used to rejuvenate an older woman’s existing eggs.

Tilly first found egg stem cells in mice in 2004. Once he identified egg stem cells in ovarian tissue from adult women, he isolated the cells and injected them into human ovary tissue that was then transplanted into mice. There the cells differentiated into human oocytes, the immature egg cells that mature, one at a time, at ovulation. Tilly didn’t take these oocytes any further, but he says he has gotten egg stem cells from mice to generate functional mouse eggs that were fertilized and exhibited early embryonic development.

The research is still a long way from creating a crying human newborn. Nevertheless, the paper “changes what we understand” about fertility, says Tilly, who also directs a center for reproductive biology at Massachusetts General Hospital. Though some of Tilly’s peers remain dubious that the cells he’s found in women’s ovarian tissue are actually stem cells or could become functional egg cells, many find the research provocative. “I think this is a very intriguing leap,” says Elizabeth McGee, an associate professor and head of reproductive endocrinology and infertility at Virginia Commonwealth University. “However, I think there’s still a long way to go before this becomes a useful product for women.”

Boston-based OvaScience, which is commercializing Tilly’s work, hopes it won’t be too long. The company’s cofounders include venture capitalist Christoph Westphal and Harvard antiaging researcher David Sinclair, who founded Sirtris Pharmaceuticals and sold it to GlaxoSmithKline for $720 million in 2008. OvaScience has raised $43 million to pursue fertility treatments and other applications for the stem cells.

One of the more tantalizing implications is that this technology could be used to reclaim the youth of an older woman’s eggs. Tilly says he can do this by transferring mitochondria—the cell’s power source—from the stem-cell-derived cells into the existing eggs. Researchers who tried something similar in the 1990s, with the help of young donors, found that mitochondria from the donors’ egg cells could improve the viability of older eggs. But the nearly 30 children who resulted from this work ended up with DNA from two women as well as their father. (It’s not clear whether the children suffered any health consequences.) By being her own source for the younger mitochondria, a woman could avoid that potentially dangerous mix of DNA, Tilly says.

David Albertini, director of the Center for Reproductive Sciences at the University of Kansas Medical Center and a member of OvaScience’s advisory board, says he “can’t wait to get [his] hands on” Tilly’s cells for his own egg research. But he says it’s too soon to consider implanting them in women before much more testing is done in mice.

by Karen Weintraub",0.01% - 0.1%,Positive,Biotech,Concept
Ultra-Efficient Solar,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Ultra-Efficient Solar
Under the right circumstances, solar cells from Semprius could produce power more cheaply than fossil fuels.
Semprius’s solar panels use glass lenses to concentrate incoming light, maximizing the power production of tiny photovoltaic cells.
Semprius’s solar panels use glass lenses to concentrate incoming light, maximizing the power production of tiny photovoltaic cells.
This past winter, a startup called Semprius set an important record for solar energy: it showed that its solar panels can convert nearly 34 percent of the light that hits them into electricity. Semprius says its technology, once scaled up, is so efficient that in some places, it could soon make electricity cheaply enough to compete with power plants fueled by coal and natural gas.

Because solar installations have many fixed costs, including real estate for the arrays of panels, it is important to maximize the efficiency of each panel in order to bring down the price of solar energy. Companies are trying a variety of ways to do that, including using materials other than silicon, the most common semiconductor in solar panels today.

For example, a startup called Alta Devices (see the TR50, March/April 2012) makes flexible sheets of solar cells out of a highly efficient material called gallium arsenide. Semprius also uses gallium arsenide, which is better than silicon at turning light into electricity (the record efficiency measured in a silicon solar panel is about 23 percent). But gallium arsenide is also far more expensive, so Semprius is trying to make up for the cost in several ways.


A new mass-production process makes high-efficiency gallium arsenide a more cost-effective photovoltaic material.

One is by shrinking its solar cells, the individual light absorbers in a solar panel, to just 600 micrometers wide, 600 micrometers long, and 10 micrometers thick. Its manufacturing process is built on research by cofounder John Rogers, a professor of chemistry and engineering at the University of Illinois, who figured out a way to grow the small cells on a gallium arsenide wafer, lift them off quickly, and then reuse the wafer to make more cells. Once the cells are laid down, Semprius maximizes their power production by putting them under glass lenses that concentrate sunlight about 1,100 times.

Concentrating sunlight on solar panels is not new, but with larger silicon cells, a cooling system typically must be used to conduct away the heat that this generates. Semprius’s small cells produce so little heat that they don’t require cooling, which further brings down the cost. Scott Burroughs, Semprius’s vice president of technology, says utilities that use its system should be able to produce electricity at around eight cents per kilowatt-hour in a few years. That’s less than the U.S. average retail price for electricity, which was about 10 cents per kilowatt-hour in 2011, according to the U.S. Energy Information Administration.


The gallium arsenide is the black square on each cell. Using such small amounts of the expensive material keeps costs down.

Semprius’s advantages are tempered by the limitations of using lenses to concentrate light: the system works best when the cells receive direct sunlight under a cloudless sky, and energy production drops significantly under any other conditions. Even so, it could be suitable for large, utility-scale projects in places such as the American Southwest.

First, however, Semprius has to begin mass-producing its panels. The company, which has raised about $44 million from venture capital firms and Siemens (which builds solar power plants), plans this year to open a small factory in North Carolina that can make enough solar panels annually to deliver six megawatts of electricity. The company hopes to expand that to 30 megawatts by the end of 2013, but to do so it must raise an undisclosed amount of money in an atmosphere that is no longer kind to capital-­intensive energy startups.

All the while, Semprius will also have to reduce its manufacturing costs fast enough to compete with conventional silicon panels, whose prices fell by more than half in 2011 alone.

by Ucilia Wang",0.01% - 1.00%,Positive,Hardware,product
Light-Field Photography,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Light-Field Photography
Lytro reinvented the camera so that it can evolve faster.

This March brought the first major update to camera design since the dawn of cheap digital photography: a camera that lets you adjust the focus of an image after you’ve taken the picture. It is being sold for $399 and up by Lytro, a startup based in Silicon Valley that plans to use its technology to offer much more than the refocusing trick—options like making 3-D images at home.

All consumer cameras create images using a flat plate—whether chemical film or a digital sensor—to record the position, color, and intensity of the light that comes through a lens. Lytro’s camera does all that, but it also records the angle at which rays of light arrive (see graphic). The resulting files aren’t images but mini-databases capturing the three-dimensional pattern of light, called a light field, at a particular moment. Software can mine that database to produce many different possible photos and visual effects from one press of the shutter.


Lytro has wrapped its technology in a consumer-friendly package, making this new form of photography more likely to catch on.

Light-field cameras existed before, but they had limited industrial uses and were never cheap enough for consumers. Lytro founder Ren Ng, who worked on light-field technology for his PhD at Stanford University, made this one affordable by simplifying the design. Instead of using multiple lenses, which made previous light-field cameras expensive and delicate, Ng showed that laying a low-cost plastic film patterned with tiny microlenses on top of a regular digital sensor could enable it to detect the direction of incoming light.


Recording the entire light field entering the camera means that images can be focused after the fact: a user can choose near, far, or any focus in between.

Refocusing images after they are shot is just the beginning of what Lytro’s cameras will be able to do. A downloadable software update will soon enable them to capture everything in a photo in sharp focus regardless of its distance from the lens, which is practically impossible with a conventional camera. Another update scheduled for this year will use the data in a Lytro snapshot to create a 3-D image. Ng is also exploring a video camera that could be focused after shots were taken, potentially giving home movies a much-needed boost in production values.

Images from Lytro cameras can be shared on websites and Facebook in a way that allows other people to experiment with changing the focus to explore what the photographer captured. This kind of flexibility is so appealing, Ng says, that “in the future, all cameras will be light-field-based.”

by Tom Simonite",0.001% - 0.01%,Positive,Hardware,Platform
Solar Microgrids,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Solar Microgrids
Village-scale DC grids provide power for lighting and cell phones.
Rural Indians are replacing kerosene lamps with cheaper and cleaner LEDs.
Rural Indians are replacing kerosene lamps with cheaper and cleaner LEDs.
Nearly 400 million Indians, mostly those living in rural communities, lack access to grid power. For many of them, simply charging a cell phone requires a long trip to a town with a recharging kiosk, and their homes are dimly lit by sooty kerosene-fueled lamps.

To change that, Nikhil Jaisinghani and Brian Shaad cofounded Mera Gao Power. Taking advantage of the falling cost of solar panels and LEDs, the company aims to build and operate low-cost solar-powered microgrids that can provide clean light and charge phones. Microgrids distribute electricity in a limited area from a relatively small generation point. While alternative solutions, such as individual solar-powered lanterns, can also provide light and charge phones, the advantage of a microgrid is that the installation cost can be spread across a village. The system can also use more efficient, larger-scale generation and storage systems, lowering operational costs.

Mera Gao’s first commercial microgrid was deployed last summer, and eight more villages have been added since; there are plans to expand to another 40 villages this year with the help of a $300,000 grant from the U.S. Agency for International Development. The company has also encouraged others to enter the Indian market for off-grid renewable energy, which the World Resources Institute, a think tank based in Washington, DC, estimates at $2 billion per year.


A typical installation uses two banks of solar panels, located on different rooftops.

For a cost of $2,500, a hundred households, in groups of up to 15, can be wired up to two generation hubs, each consisting of a set of solar panels and a battery pack. The grid uses 24-volt DC power throughout, which permits the use of aluminum wiring rather than the more expensive copper wiring required for higher-voltage AC distribution systems. The village is carefully mapped before installation to ensure the most efficient arrangement of distribution lines. (Circuit breakers will trip if a freeloader tries to tap in.) “This mapping and design is our biggest innovation,” Jaisinghani says.

Each household gets 0.2 amps for seven hours a night—enough to power two LED lights and a mobile-phone charging point—for a prepaid monthly fee of 100 rupees ($2); kerosene and phone charging generally cost 100 to 150 rupees a month.

Jaisinghani says Mera Gao’s microgrid is not a replacement for grid power, but it’s what people want and can pay for right now. Currently the technology supports only lighting and phone charging, but the company is exploring ideas such as community entertainment centers where the costs of television, radio, cooling fans, and information services are spread across a group of homes rather than being paid by a single user.

by Seema Singh",0.1% - 1.0%,Positive,Hardware,product
3-D Transistors,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"3-D Transistors
Intel creates faster and more energy-efficient processors.

The new transistors (above) have vertical current-carrying channels. In older designs (inset), the channels lie flat under the gates.

In an effort to keep squeezing more components onto silicon chips, Intel has begun mass-producing processors based on 3-D transistors. The move not only extends the life of Moore’s Law (the prediction that the number of transistors per chip will double roughly every two years) but could help significantly increase the energy efficiency and speed of processors.

The on-and-off flow of current in conventional chips is controlled by an electric field generated by a so-called gate that sits on top of a wide, shallow conducting channel embedded in a silicon substrate. With the 3-D transistors, that current-carrying channel has been flipped upright, rising off the surface of the chip. The channel material can thus be in contact with the gate on both its sides and its top, leaving little of the channel exposed to interference from stray charges in the substrate below. In earlier transistors, these charges interfered with the gate’s ability to block current, resulting in a constant flow of leakage current.

With virtually no leakage current, a transistor can switch on and off more cleanly and quickly, and it can be run at lower power, since designers don’t have to worry that leakage current could be mistaken for an “on” signal.

Intel claims the new transistors can switch up to 37 percent faster than its previous transistors or consume as little as half as much power. Faster switching means faster chips. In addition, because of their smaller footprint, the transistors can be packed closer together. Signals thus take less time to travel between them, further speeding up the chip.

The first processors based on the technology will shortly appear in laptops. But the electronics industry is especially excited by the prospect of conserving power in handheld devices. That means designers can upgrade the performance of a device without requiring bulkier batteries, or reduce battery size without lowering performance. “Ten years ago everyone only cared about making chips faster,” says Mark Bohr, who heads process technology at Intel. “Today low-power operation is much more important.” He adds that the power savings and performance gains will be magnified in handheld devices because the smaller transistors will make it possible for a single chip to handle functions such memory, broadband communications, and GPS, each of which used to require its own chip. With fewer chips and smaller batteries, gadgets will be able to do more in tinier packages.

The new transistor design leaves room for enough further improvement to see the industry through the next five years. Intel’s previous chips could pack in 4.87 million transistors per square millimeter; the new chips have 8.75 million, and by 2017, about 30 million transistors per square millimeter should be possible. “This buys silicon another few generations,” says Bohr.

by David H. Freedman",1-10%,Positive,Hardware,product
A Faster Fourier Transform,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"A Faster Fourier Transform
A mathematical upgrade promises a speedier digital world.
Piotr Indyk, Dina Katabi, Eric Price, and Haitham Hassanieh (left to right) have created a faster way to break down complex signals into combinations of simple waves for processing.
Piotr Indyk, Dina Katabi, Eric Price, and Haitham Hassanieh (left to right) have created a faster way to break down complex signals into combinations of simple waves for processing.
In January, four MIT researchers showed off a replacement for one of the most important algorithms in computer science. Dina Katabi, Haitham Hassanieh, Piotr Indyk, and Eric Price have created a faster way to perform the Fourier transform, a mathematical technique for processing streams of data that underlies the operation of things such as digital medical imaging, Wi-Fi routers, and 4G cellular networks.

The principle of the Fourier transform, which dates back to the 19th century, is that any signal, such as a sound recording, can be represented as the sum of a collection of sine and cosine waves with different frequencies and amplitudes. This collection of waves can then be manipulated with relative ease—for example, allowing a recording to be compressed or noise to be suppressed. In the mid-1960s, a computer-friendly algorithm called the fast Fourier transform (FFT) was developed. Anyone who’s marveled at the tiny size of an MP3 file compared with the same recording in an uncompressed form has seen the power of the FFT at work.

With the new algorithm, called the sparse Fourier transform (SFT), streams of data can be processed 10 to 100 times faster than was possible with the FFT. The speedup can occur because the information we care about most has a great deal of structure: music is not random noise. These meaningful signals typically have only a fraction of the possible values that a signal could take; the technical term for this is that the information is “sparse.” Because the SFT algorithm isn’t intended to work with all possible streams of data, it can take certain shortcuts not otherwise available. In theory, an algorithm that can handle only sparse signals is much more limited than the FFT. But “sparsity is everywhere,” points out coinventor Katabi, a professor of electrical engineering and computer science. “It’s in nature; it’s in video signals; it’s in audio signals.”

A faster transform means that less computer power is required to process a given amount of information—a boon to energy-conscious mobile multimedia devices such as smart phones. Or with the same amount of power, engineers can contemplate doing things that the computing demands of the original FFT made impractical. For example, Internet backbones and routers today can actually read or process only a tiny trickle of the river of bits they pass between them. The SFT could allow researchers to study the flow of this traffic in much greater detail as bits shoot by billions of times a second.

by Mark Anderson",0.05% - 0.5%,Positive,Software,methodology
Nanopore Sequencing,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Nanopore Sequencing
Simple and direct analysis of DNA will make genetic testing routine in more situations.

Oxford Nanopore has demonstrated commercial machines that read DNA bases directly. The technology offers a way to make genome sequencing faster, cheaper, and potentially convenient enough to let doctors use sequencing as routinely as they order an MRI or a blood cell count, ushering in an era of personalized medicine.

The company’s devices, which eliminate the need to amplify DNA or use expensive reagents, work by passing a ­single strand of DNA through a protein pore created in a membrane. An electric current flows through the pore; different DNA bases disrupt the current in different ways, letting the machine electronically read out the sequence.

Rival sequencing technologies have gotten faster and cheaper in recent years as well. But most of them either use fluorescent reagents to identify bases or require chopping up the DNA molecule and amplifying the fragments. Nanopore’s technology is simpler, and it avoids errors that can creep in during these steps.

Being able to read DNA molecules directly also means that longer segments of a genome can be read at a time. This makes it easier for researchers to see large-scale patterns such as translocations, in which chunks of DNA are displaced from one part of the genome to another, and copy number variations, in which DNA sequences are repeated several times or more. (Translocations are thought to underlie various forms of cancer and other diseases, while copy number variations are linked to a range of neurological and developmental disorders.)

The company reports reading a stretch of DNA roughly 48,000 bases long. “That’s by far the longest piece of DNA that anyone’s claimed to read,” says ­Jeffery Schloss, program director for technology development at the National Human Genome Research Institute.

Oxford Nanopore’s new product line (which will begin shipping later this year) will include a miniaturized portable device, roughly the size of two decks of cards, that can plug directly into a computer’s USB port and is capable of sequencing small volumes of DNA. A larger desktop machine can handle larger volumes; clusters of these machines will be used for sequencing whole genomes. Although the company has not yet announced pricing for the desktop machine, the portable version could cost less than $900. This device will make it easier to read limited amounts of DNA in a host of settings, including remote clinics or food processing plants, where inspectors could monitor for contamination by dangerous strains of bacteria.

by Amanda Schaffer",0.01% - 0.1%,Positive,Biotech,product
Crowdfunding,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Crowdfunding
Kickstarter is funding the commercialization of new technologies.
Entrepreneurs can post videos and pictures on Kickstarter to attract pledges for projects. Some success stories (clockwise from top left):

Elevation iPhone dock
$1,460,000

Double Fine Adventure
(video game)
$3,330,000

Twine Wi-Fi sensors
$557,000

CloudFTP wireless
thumb drive hub
$262,000

PrintrBot 3-D printer
$831,000

The Order of the Stick
(comic book)
$1,250,000
Entrepreneurs can post videos and pictures on Kickstarter to attract pledges for projects. Some success stories (clockwise from top left): Elevation iPhone dock $1,460,000 Double Fine Adventure (video game) $3,330,000 Twine Wi-Fi sensors $557,000 CloudFTP wireless thumb drive hub $262,000 PrintrBot 3-D printer $831,000 The Order of the Stick (comic book) $1,250,000
Kickstarter, a New York City–based website originally founded to support creative projects, has become a force in financing technology startups. Entrepreneurs have used the site to raise hundreds of thousands of dollars at a time to develop and produce products, including a networked home sensing system and a kit that prints three-dimensional objects (see Hack).

This crowdfunding model offers an alternative to traditional means of raising startup funds for some types of businesses, such as Web or design firms. Startups keep their equity, maintain full control over strategy, and gain a committed community of early adopters to boot.

While most projects ask for relatively small amounts, several have exceeded the $1 million mark. Most notably, Double Fine Productions raised over $3 million to develop a video game. That’s well beyond the typical angel stake, which generally tops out at $600,000, and into the realm of the typical venture capital infusion.

Overall, Kickstarter users pledged nearly $99.3 million for projects last year—an amount roughly equivalent to 10 percent of all seed investment in the United States, which PricewaterhouseCoopers estimates at $920 million.

People seeking to raise money for a project set a funding target and solicit pledges. If a project fails to reach its target (as happened to about 54 percent of them in 2011), supporters pay nothing. For projects that do hit their target, donors receive a variety of rewards, including thank-you notes, products, or even elaborate packages that might include a visit to the creators’ place of work. Kickstarter, which was launched in 2009 by Yancey Strickler, Charles Adler, and Perry Chen, takes a 5 percent cut. Since the launch, the site has distributed over $150 million.

Kickstarter’s role could begin to shift with the end of the U.S. ban on private companies’ selling equity to small investors, which was lifted in April. Says Paul Kedrosky, a senior fellow at the Kauffman Foundation who focuses on risk capital: “If crowdfunding sites start offering equity shares, it will make a few dozen VC firms disappear.”

by Ted Greenwald",0.05% - 0.5%,Positive,Software,platform
High-Speed Materials Discovery,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"High-Speed Materials Discovery
A new way to identify battery materials suitable for mass production could revolutionize energy storage.
Wildcat starts with a wide range of precursor materials that may have potential for energy storage and other applications.
Wildcat starts with a wide range of precursor materials that may have potential for energy storage and other applications.
Electric cars could travel farther, and smart phones could have more powerful processors and better, brighter screens, thanks to batteries based on new materials being developed by San Diego–based Wildcat Discovery Technologies.

The company is accelerating the identification of valuable energy storage materials by testing thousands of substances at a time. In March of last year, it announced a lithium cobalt phosphate cathode that boosts energy density by nearly a third over current cathodes in popular lithium-ion phosphate batteries. The company also unveiled an electrolyte additive that allows batteries to work more reliably at higher voltages.


2/ Here, some precursor materials have been refined into powder suitable for making battery electrodes.

Choosing the optimal materials for batteries is a particularly tricky problem. The devices have three principal components: an anode, a cathode, and an electrolyte. Not only can each be formed from almost any blend of a huge number of compounds, but the three components have to work well together. That leaves many millions of promising combinations to explore.


3/ Cathodes, anodes, and electrolytes are assembled into small working batteries, which are tested by the thousand in this tower. Testing materials together allows dud combinations to be eliminated quickly.

To hunt down winning combinations, Wildcat has adopted a strategy originally developed by drug discovery labs: high-throughput combinatorial chemistry. Instead of testing one material at a time, Wildcat methodically runs through thousands of tests in parallel, synthesizing and testing some 3,000 new material combinations a week. “We’ve got materials in the pipeline that could triple energy density,” says CEO Mark Gresser.

Others have tried the combinatorial technique to find new battery materials, but they’ve run into a stumbling block. The easy way to test thousands of materials is to deposit a sample of each one in a thin film atop a substrate. This approach did allow previous researchers to turn up promising materials for battery components—but then candidates would typically prove unsuited to cost-effective large-scale production processes.


To process so many samples, Wildcat relies heavily on automation. This assay machine weighs and records vials of materials.

To avoid that time-wasting detour, Wildcat found ways to produce samples using miniaturized versions of large-scale production techniques. In effect, the candidate materials are being tested for ease of manufacturing at the same time as they’re being tested for performance. Wildcat also tests the materials wired together as actual batteries, and in a variety of potential operating conditions. “There are a lot of variables that affect battery performance, including temperature and voltage, and we examine all of them,” says Gresser. The result is that a material that performs well in a Wildcat test bed will probably perform well in field tests.

If Wildcat is successful, its efforts could lead to batteries that are smaller or more powerful than their present-day counterparts—improvements that will appeal to the makers of smart phones and electric vehicles alike.

by David H. Freedman",0.05% - 0.5%,Positive,Nanotech,Methodology
Facebook's Timeline,2012.0,https://www.technologyreview.com/10-breakthrough-technologies/2012/,,"Facebook’s Timeline
The social-networking company is collecting and analyzing consumer data on an unprecedented scale.

Facebook recently introduced its Timeline interface to its 850 million monthly active users. The interface is designed to make it easy to navigate much of the immense amount of information that the social network has gathered about each of its users—and to prompt them to add and share even more in a way that’s easy to analyze.

Facebook’s motivation is to better target the advertisements that are responsible for 85 percent of its revenue. In part, successful targeting is a numbers game. If reported trends have held steady, Facebook’s data warehouse was adding 625,000 terabytes of compressed data daily by last January. Timeline’s new features are bound to boost that number dramatically, potentially providing Facebook with more personal data than any other ad seller online can access.

In the past, much of the data that users contributed to Facebook was in the form of unstructured status updates. The addition of a “Like” button, and the ability to link that button to third-party websites (see “TR10: Social Indexing,” May/June 2011), provided somewhat more fine-grained information that could be used for targeting ads. Timeline goes well beyond that, prompting users to add an extensive array of metadata to their updates, which makes mining value much easier. And by design, it encourages users to revisit and add more information to old updates, or retroactively add completely new biographical information.

One way Timeline gets users to add marketable meaning is by asking them to categorize their updates under a broad collection of “Life Events,” which includes tags for actions like buying a home or vehicle. A user who notes a vehicle purchase is prompted to specify details such as the type, make, and model year of the car, along with when and where the purchase was made and whom the user was with at the time. Connecting the dots, Facebook may determine the gender, income bracket, educational level, and profession of the kind of person likely to buy a specific car.

This growing trove of data is a bonanza to marketers, but it’s also a challenge for Facebook, which must keep up with the flood of bits. Approximately 10 percent of Facebook’s revenue is devoted to R&D, including efforts to improve the speed, efficiency, and scalability of its infrastructure. If previous spending patterns hold true, much of the company’s 2012 capital-expense allocation—more than $1.6 billion—is likely to be devoted to servers and storage devices.

Timeline is making real the concept of the “permanent record,” in the form of a computer-assisted autobiography—a searchable multimedia diary of our lives that hovers in the cloud. But it may also have an unintended effect of calling users’ attention to just how much Facebook knows about them. Normally, “when people share information about themselves, they see a snapshot,” says Deirdre Mulligan, a professor at the UC Berkeley School of Information. “When people see Timeline, they become aware that all those bits and pieces are more than the sum of the parts. They suddenly understand the bigness of their own data.”

by Ted Greenwald",0.5% - 5.0%,Negative,Software,product
Smart Watches,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/smart-watches/,"Smart Watches
The designers of the Pebble watch realized that a mobile phone is more useful if you don’t have to take it out of your pocket.
by John Pavlus archive page
April 23, 2013
tr10-2013-smart watches
PETER BELANGER
Eric Migicovsky didn’t really want a “wearable computer.” When he first conceived of what would become the Pebble smart watch five years ago, as an industrial-design student at Delft University of Technology in the Nether­lands, he just wanted a way to use his smartphone without crashing his bicycle. “I thought of creating a watch that could grab information from my phone,” the 26-year-old Canadian says. “I ended up building a prototype in my dorm room.”

Now Migicovsky is shipping 85,000 Pebble watches to eager customers who don’t want to lug a glass slab out of their pocket just to check their e-mail or the weather forecast. Pebble uses Bluetooth to connect wirelessly to an iPhone or Android phone and displays notifications, messages, and other simple data of the user’s choosing on its small black-and-white LCD screen. In April 2012, using the online fund-raising platform Kickstarter, Migicovsky asked for $100,000 to help bring Pebble to market. Five weeks later, he had more than $10 million—making his the highest-grossing Kickstarter campaign yet. Suddenly smart watches are a real product category: Sony entered the market last year, Samsung is about to, and Apple seems likely to follow.

Although the $150 Pebble watch can be used to control a music playlist or run simple apps like RunKeeper, a cloud-based fitness tracker, Migicovsky and his team purposely designed the watch to do as little as possible, leaving more complicated apps for phones. This emphasis on making the watch “glanceable” informed nearly every aspect of the design. The black-and-white screen, for example, can be read in direct sunlight and displays content persistently without needing to “sleep” to conserve battery power, as color or touch-screen displays do.

These watches are coming to market a few months before Google Glass, which is another attempt to solve the problem Pebble addresses—namely, that “interacting with our phones has a certain overhead that doesn’t need to be there,” says Mark Rolston, chief creative officer of Frog Design. But Google Glass will try to replace the smartphone altogether by combining a computer and monitor into eyeglass frames so that wearers can “augment” their view of the world with data. That lines up with predictions about the advent of wearable computing, but it’s easy to see Pebble’s idea being much more popular. By making use of a watch—a classic accessory—Pebble is trying to fit in to long-standing social norms rather than create new ones.

by John Pavlus",0.1% - 1%,Positive,Hardware,product
Ultra-Efficient Solar Power,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/ultra-efficient-solar-power/,"Ultra-Efficient Solar Power
Doubling the efficiency of solar devices would completely change the economics of renewable energy. Here is a design that just might make it possible.
by Mike Orcutt archive page
April 23, 2013
tr10-2013-Ultra-Efficient Solar Power
JOHN MACNEILL
Harry Atwater thinks his lab can make an affordable device that produces more than twice the solar power generated by today’s panels. The feat is possible, says the Caltech professor of materials science and applied physics, because of recent advances in the ability to manipulate light at a very small scale.

Solar panels on the market today consist of cells made from a single semiconducting material, usually silicon. Since the material absorbs only a narrow band of the solar spectrum, much of sunlight’s energy is lost as heat: these panels typically convert less than 20 percent of that energy into electricity. But the device that ­Atwater and his colleagues have in mind would have an efficiency of at least 50 percent. It would use a design that efficiently splits sunlight, as a prism does, into six to eight component wavelengths—each one of which produces a different color of light. Each color would then be dispersed to a cell made of a semiconductor that can absorb it.

Atwater’s team is working on three designs. In one (see illustration), for which the group has made a prototype, sunlight is collected by a reflective metal trough and directed at a specific angle into a structure made of a transparent insulating material. Coating the outside of the transparent structure are multiple solar cells, each made from one of six to eight different semiconductors. Once light enters the material, it encounters a series of thin optical filters. Each one allows a single color to pass through to illuminate a cell that can absorb it; the remaining colors are reflected toward other filters designed to let them through.

Another design would employ nanoscale optical filters that could filter light coming from all angles. And a third would use a hologram instead of filters to split the spectrum. While the designs are different, the basic idea is the same: combine conventionally designed cells with optical techniques to efficiently harness sunlight’s broad spectrum and waste much less of its energy.

It’s not yet clear which design will offer the best performance, says Atwater. But the devices envisioned would be less complex than many electronics on the market today, he says, which makes him confident that once a compelling prototype is fabricated and optimized, it could be commercialized in a practical way.

Achieving ultrahigh efficiency in solar designs should be a primary goal of the industry, argues Atwater, since it’s now “the best lever we have” for reducing the cost of solar power. That’s because prices for solar panels have plummeted over the past few years, so continuing to focus on making them less expensive would have little impact on the overall cost of a solar power system; expenses related to things like wiring, land, permitting, and labor now make up the vast majority of that cost. Making modules more efficient would mean that fewer panels would be needed to produce the same amount of power, so the costs of hardware and installation could be greatly reduced. “Within a few years,” Atwater says, “there won’t be any point to working on technology that has efficiency that’s less than 20 percent.”

by Mike Orcutt",0.05% - 0.5%,Positive,Hardware,concept
Memory Implants,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/memory-implants/,"Memory Implants
A maverick neuroscientist believes he has deciphered the code by which the brain forms long-term memories.
by Jon Cohen archive page
April 23, 2013

DAN WINTERS
Theodore Berger, a biomedical engineer and neuroscientist at the University of Southern California in Los Angeles, envisions a day in the not too distant future when a patient with severe memory loss can get help from an electronic implant. In people whose brains have suffered damage from Alzheimer’s, stroke, or injury, disrupted neuronal networks often prevent long-term memories from forming. For more than two decades, Berger has designed silicon chips to mimic the signal processing that those neurons do when they’re functioning properly—the work that allows us to recall experiences and knowledge for more than a minute. Ultimately, Berger wants to restore the ability to create long-term memories by implanting chips like these in the brain.

The idea is so audacious and so far outside the mainstream of neuroscience that many of his colleagues, says Berger, think of him as being just this side of crazy. “They told me I was nuts a long time ago,” he says with a laugh, sitting in a conference room that abuts one of his labs. But given the success of recent experiments carried out by his group and several close collaborators, Berger is shedding the loony label and increasingly taking on the role of a visionary pioneer.

Berger and his research partners have yet to conduct human tests of their neural prostheses, but their experiments show how a silicon chip externally connected to rat and monkey brains by electrodes can process information just like actual neurons. “We’re not putting individual memories back into the brain,” he says. “We’re putting in the capacity to generate memories.” In an impressive experiment published last fall, Berger and his coworkers demonstrated that they could also help monkeys retrieve long-term memories from a part of the brain that stores them.

If a memory implant sounds farfetched, Berger points to other recent successes in neuroprosthetics. Cochlear implants now help more than 200,000 deaf people hear by converting sound into electrical signals and sending them to the auditory nerve. Meanwhile, early experiments have shown that implanted electrodes can allow paralyzed people to move robotic arms with their thoughts. Other researchers have had preliminary success with artificial retinas in blind people.

Still, restoring a form of cognition in the brain is far more difficult than any of those achievements. Berger has spent much of the past 35 years trying to understand fundamental questions about the behavior of neurons in the hippocampus, a part of the brain known to be involved in forming memory. “It’s very clear,” he says. “The hippocampus makes short-term memories into long-term memories.”

What has been anything but clear is how the hippocampus accomplishes this complicated feat. Berger has developed mathematical theorems that describe how electrical signals move through the neurons of the hippocampus to form a long-term memory, and he has proved that his equations match reality. “You don’t have to do everything the brain does, but can you mimic at least some of the things the real brain does?” he asks. “Can you model it and put it into a device? Can you get that device to work in any brain? It’s those three things that lead people to think I’m crazy. They just think it’s too hard.”

Cracking the Code

Berger often speaks in sentences that stretch to paragraph length and have many asides, footnotes, and complete diversions from the point. I ask him to define memory. “It’s a series of electrical pulses over time that are generated by a given number of neurons,” he says. “That’s important because you can reduce it to this and put it back into a framework. Not only can you understand it in terms of the biological events that happened; that means that you can poke it, you can deal with it, you can put an electrode in there, and you can record something that matches your definition of a memory. You can find the 2,147 neurons that are part of this memory. And what do they generate? They generate this series of pulses. It’s not bizarre. It’s something you can handle. It’s useful. It’s what happens.”

This is the conventional view of memory, but it only scratches the surface. And to Berger’s perpetual frustration, many colleagues who probe this mysterious realm of the brain haven’t attempted to go much deeper. Neuroscientists track electrical signals in the brain by monitoring action potentials, microvolt changes on the surfaces of neurons. But all too often, says Berger, their reports oversimplify what’s actually taking place. “They find an important event in the environment and count action potentials,” he says. “They say, ‘It went up from 1 to 200 after I did something. I’m finding something interesting.’ What are you finding? ‘Activity went up.’ But what are you finding? ‘Activity went up.’ So what? Is it coding something? Is it representing something that the next neuron cares about? Does it make the next neuron do something different? That’s what we’re supposed to be doing: explaining things, not just describing things.”

Berger takes a marker and fills a whiteboard from top to bottom with a line of circles that represent neurons. Next to each one, he draws a horizontal line that has a different pattern of blips on it. “This is you in my brain,” he says. “My hippocampus has already formed a long-term memory of you. I’ll remember you into next week. But how can I distinguish you from the next person? Let’s say there are 500,000 cells in the hippocampus that represent you, and there are all sorts of things that each cell is coding—like how your nose is relative to your eyebrow—and they code that with different patterns. So the reality of the nervous system is really complicated, which is why we’re still asking such basic, limited questions about it.”

If one neuron fires at a specific time and place, what exactly do the neighboring neurons do in response?

MISHA GRAVENOR
In graduate school at Harvard, ­Berger’s mentor was Richard Thompson, who studied localized, learning-induced changes in the brain. Thompson used a tone and a puff of air to condition rabbits to blink their eyes, aiming to determine where the memory he induced was stored. The idea was to find a specific place in the brain where the learning was localized, says Berger: “If the animal did learn and you removed it, the animal couldn’t remember.”

Thompson, with Berger’s help, managed to do just that; they published the results in 1976. To find the site in the rabbits, they equipped the animals’ brains with electrodes that could monitor the activity of a neuron. Neurons have gates on their membranes, which let electrically charged particles like sodium and potassium in and out. Thompson and Berger documented the electrical spikes seen in the hippocampus as rabbits developed a memory. Both the spikes’ amplitude (representing the action potential) and their spacing formed patterns. It can’t be an accident, Berger thought, that cells fire in a way that forms patterns with respect to time.

This led him to a central question that underlies his current work: as cells receive and send electrical signals, what pattern describes the quantitative relationship between the input and the output? That is, if one neuron fires at a specific time and place, what exactly do the neighboring neurons do in response? The answer could reveal the code that neurons use to form a long-term memory.

But it soon became clear that the answer was extremely complex. In the late 1980s, Berger, working at the University of Pittsburgh with Robert Sclabassi, became fascinated by a property of the neuronal network in the hippocampus. When they stimulated the hippocampus of a rabbit with electrical pulses (the input) and charted how signals moved through different populations of neurons (the output), the relationship they observed between the two wasn’t linear. “Let’s say you put in 1 and get 2,” says Berger. “That’s pretty easy. It’s a linear relation.” It turns out, however, that there’s “essentially no condition in the brain where you get linear activity, a linear summation,” he says. “It’s always nonlinear.” Signals overlap, with some suppressing an incoming pulse and some accentuating it.

By the early 1990s, his understanding—and computing hardware—had advanced to the point that he could work with his colleagues at the University of Southern California’s department of engineering to make computer chips that mimic the signal processing done in parts of the hippocampus. “It became obvious that if I could get this stuff to work in large numbers in hardware, you’ve got part of the brain,” he says. “Why not hook up to what’s existing in the brain? So I started thinking seriously about prosthetics long before anybody even considered it.”

A Brain Implant

Berger began working with Vasilis ­Marmarelis, a biomedical engineer at USC, to begin making a brain prosthesis (see “Regaining Lost Brain Function”). They first worked with hippocampal slices from rats. Knowing that neuronal signals move from one end of the hippocampus to the other, the researchers sent random pulses into the hippocampus, recorded the signals at various locales to see how they were transformed, and then derived mathematical equations describing the transformations. They implemented those equations in computer chips.

Next, to assess whether such a chip could serve as a prosthesis for a damage hippocampal region, the researchers investigated whether they could bypass a central component of the pathway in the brain slices. Electrodes placed in the region carried electrical pulses to an external chip, which performed the transformations normally done in the hippocampus. Other electrodes delivered the signals back to the slice of brain.

Then the researchers took a leap forward by trying this in live rats, showing that a computer could in fact serve as an artificial component of the hippocampus. They began by training the animals to push one of two levers to receive a treat, recording the series of pulses in the hippocampus as they chose the correct one. Using those data, Berger and his team modeled the way the signals were transformed as the lesson was converted into a long-term memory, and they captured the code believed to represent the memory itself. They proved that their device could generate this long-term memory code from input signals recorded in rats’ brains while they learned the task. Then they gave the rats a drug that interfered with their ability to form long-term memories, causing them to forget which lever produced the treat. When the researchers pulsed the drugged rats’ brains with the code, the animals were again able to choose the right lever.

“I never thought I’d see this go into humans, and now our discussions are about when and how. I never thought I’d live to see the day.”
Last year, the scientists published primate experiments involving the prefrontal cortex, a part of the brain that retrieves the long-term memories created by the hippocampus. They placed electrodes in the monkey brains to capture the code formed in the prefrontal cortex that they believed allowed the animals to remember an image they had been shown earlier. Then they drugged the monkeys with cocaine, which impairs that part of the brain. Using the implanted electrodes to send the correct code to the monkeys’ prefrontal cortex, the researchers significantly improved the animal’s performance on the image-identification task.

Within the next two years, Berger and his colleagues hope to implant an actual memory prosthesis in animals. They also want to show that their hippocampal chips can form long-term memories in many different behavioral situations. These chips, after all, rely on mathematical equations derived from the researchers’ own experiments. It could be that the researchers were simply figuring out the codes associated with those specific tasks. What if these codes are not generalizable, and different inputs are processed in various ways? In other words, it is possible that they haven’t cracked the code but have merely deciphered a few simple messages.

Berger allows that this may well be the case, and his chips may form long-term memories in only a limited number of situations. But he notes that the morphology and biophysics of the brain constrain what it can do: in practice, there are only so many ways that electrical signals in the hippocampus can be transformed. “I do think we’re going to find a model that’s pretty good for a lot of conditions and maybe most conditions,” he says. “The goal is to improve the quality of life for somebody who has a severe memory deficit. If I can give them the ability to form new long-term memories for half the conditions that most people live in, I’ll be happy as hell, and so will be most patients.”

Despite the uncertainties, Berger and his colleagues are planning human studies. He is collaborating with clinicians at his university who are testing the use of electrodes implanted on each side of the hippocampus to detect and prevent seizures in patients with severe epilepsy. If the project moves forward as envisioned, Berger’s group will piggyback on the trial to look for memory codes in those patients’ brains.

“I never thought I’d see this go into humans, and now our discussions are about when and how,” he says. “I never thought I’d live to see the day, but now I think I will.”

by Jon Cohen",-0.01% to -1%,"The article discusses the work of Theodore Berger, a biomedical engineer at the University of Southern California (USC), who is developing a brain prosthesis that can mimic the signal processing done in parts of the hippocampus. The hippocampus is responsible for forming new long-term memories.

Berger and his team have been working on this project for several years, starting with experiments on rat brains. They first recorded the electrical signals in the hippocampus as rats learned to perform a task, then developed mathematical equations that could model the transformations of those signals. These equations were implemented in computer chips, which were used to bypass damaged parts of the hippocampus and restore function.

The team has since moved on to experiments with live rats and primates, showing that their device can generate long-term memory codes from input signals recorded in the brain. They have also demonstrated that this device can be used to improve performance on a memory task after administering a drug that impairs memory formation.

Berger hopes to implant an actual memory prosthesis in animals within the next two years and is planning human studies. He is collaborating with clinicians at USC who are testing the use of electrodes implanted on each side of the hippocampus to detect and prevent seizures in patients with severe epilepsy. Berger's group will piggyback on this trial to look for memory codes in those patients' brains.

Berger acknowledges that there may be limitations to his device, such as its ability to form long-term memories only in specific situations. However, he believes that the brain's morphology and biophysics constrain what it can do, and that a generalizable model of memory formation is possible. He hopes that his work will improve the quality of life for individuals with severe memory deficits.

The article concludes by noting that Berger never thought he would see this technology move into humans, but now believes that it will happen in his lifetime.",Biotech,"The article discusses the work of Ted Berger, a biomedical engineer at the University of Southern California (USC), who is developing a brain prosthesis that can restore memory function in individuals with damaged or impaired brains. Berger and his team have been working on creating computer chips that mimic the signal processing done in parts of the hippocampus, a region of the brain responsible for forming long-term memories.

The researchers began by studying hippocampal slices from rats and derived mathematical equations describing how neuronal signals are transformed as they move through the brain. They then implemented these equations in computer chips and tested them in live rats, showing that a computer could serve as an artificial component of the hippocampus.

Berger's team has also conducted experiments with primates, using electrodes to capture the code formed in the prefrontal cortex that allows animals to remember images they had been shown earlier. They then drugged the monkeys with cocaine, which impairs this part of the brain, and used the implanted electrodes to send the correct code to the monkeys' prefrontal cortex, significantly improving their performance on an image-identification task.

Berger hopes to implant an actual memory prosthesis in animals within the next two years and plans to test it in humans soon after. He is collaborating with clinicians at USC who are testing the use of electrodes implanted on each side of the hippocampus to detect and prevent seizures in patients with severe epilepsy. Berger's group will piggyback on this trial to look for memory codes in those patients' brains.

While there are still uncertainties about whether Berger's chips can form long-term memories in a variety of situations, he is optimistic that his work will lead to significant improvements in the quality of life for individuals with severe memory deficits."
Prenatal DNA Sequencing,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/prenatal-dna-sequencing/,"Prenatal DNA Sequencing
Reading the DNA of fetuses is the next frontier of the genome revolution. Do you really want to know the genetic destiny of your unborn child?
by Antonio Regalado archive page
April 23, 2013
The Executive: Illumina CEO Jay Flatley is looking to pregnancy as a new market for DNA sequencing.
The Executive: Illumina CEO Jay Flatley is looking to pregnancy as a new market for DNA sequencing.
FRANK ROGOZIENSKI | WONDERFUL MACHINE
Earlier this year Illumina, the maker of the world’s most widely used DNA sequencing machines, agreed to pay nearly half a billion dollars for Verinata, a startup in Redwood City, California, that has hardly any revenues. What Verinata does have is technology that can do something as ethically fraught as it is inevitable: sequence the DNA of a human fetus before birth.

Verinata is one of four U.S. companies already involved in a rapidly expanding market for prenatal DNA testing using Illumina’s sequencers. Their existing tests, all launched in the last 18 months, can detect Down syndrome from traces of fetal DNA found in a syringeful of the mother’s blood. Until now, detecting Down syndrome has meant grabbing fetal cells from the placenta or the amniotic fluid, procedures that carry a small risk of miscarriage.

The noninvasive screen is so much safer and easier that it’s become one of the most quickly adopted tests ever and an important new medical application for Illumina’s DNA sequencing instruments, which have so far been used mainly in research labs. In January, Illumina’s CEO, Jay Flatley, told investors that he expects the tests will eventually be offered to as many as two million women a year in the United States, representing half of all pregnancies—up from around 250,000 mothers, mostly older, who now undergo the invasive tests. “It’s unprecedented in medical testing how fast this has gone from lab research to acceptance,” says Diana Bianchi, executive director of the Mother Infant Research Institute at Tufts University and the chief clinical advisor to Verinata. “It’s a huge impact for any technology in its first year.”

But this is likely to be just the start for prenatal DNA sequencing. The same labs and companies that launched the Down syndrome tests, like Verinata, have also figured out how they can get much more information from a mother’s bloodstream, including the complete genome sequence of her fetus. That’s a technical breakthrough, and maybe a commercial one, too. Pregnancy, with its hopes, anxieties, and frequent doctor’s visits, could be where genome sequencing finally finds a major consumer application.

“I think that we are going to sequence the genome of everyone—of every fetus— in the first trimester, at least in part,” says Arthur Beaudet, a pediatrician and head of human genetics at the Baylor College of Medicine, in Houston. Today some patients have their genomes sequenced to shed light on genetic diseases or illnesses like cancer, but one day people won’t wait until they’re sick. “We are already going to know the data at birth,” he says.

That won’t happen right away. For one thing, sorting out a fetus’s exact DNA code via its mother’s blood requires a huge amount of repeated sequencing, making it too expensive for routine use. (Illumina currently charges $9,500 to sequence the genome of an adult, and so far attempts to sequence fetal DNA have cost much more.) And there are still technical problems: the fetal genome results are still not accurate enough for making diagnoses. Ethically, too, the technology is a minefield. If we learn the genetic destiny of our children while they are still in the womb, what kinds of choices might we make?

“Technically, all this is possible before we’ve figured out whether we should be doing it,” says Jay Shendure, a genome scientist at the University of Washington. “You’ve got the whole genome—then what do you do with that? There are a lot of things that will have to get ironed out.” Shendure works with Ariosa, one of Verinata’s competitors. Last summer, his was one of two U.S. labs to demonstrate how the fetal genome might be revealed from a pregnant woman’s blood. He says the studies conducted so far on fetuses, including his own study, have been retrospective—they studied blood samples stored by hospitals. But Shendure says he is now working with doctors at Stanford to implement the technology during an actual pregnancy. In other words, as early as this year the first human whose complete genetic code is known in advance could be born.

Full Genome

In 1997, a Hong Kong scientist named Dennis Lo showed that a pregnant woman’s blood contains trillions of bits of DNA from her baby. The DNA comes from cells in the placenta that have died and ruptured. By Lo’s estimate, as much as 15 percent of the free-floating DNA in a mother’s bloodstream is the fetus’s. High-speed DNA sequencing can turn those fragments into a wealth of information.

To detect Down syndrome, which causes cognitive and physical impairments, geneticists have typically looked through a microscope to count the number of chromosomes in fetal cells captured in a procedure called amniocentesis. An extra copy of chromosome 21 means the fetus is affected, and about 65 percent of U.S. women confronted with that diagnosis choose an abortion.

Sequencing the DNA in the blood of a pregnant woman could reveal the full genetic code of a fetus.
To get the same information from a few milliliters of blood, scientists use a trick first suggested by Lo. They randomly sequence millions of those circulating DNA fragments, often only 50 to 500 DNA letters long. Then, using a computer program, they line up the sequences against a map of human chromosomes. After that, it’s a counting exercise: if more bits than expected match up with chromosome 21, that’s evidence of an extra copy somewhere, and the fetus probably has Down syndrome. The method is clever because it doesn’t matter that the mother’s DNA and that of the fetus are mixed together and are, in fact, partly identical. The same approach can find extra copies, or trisomies, of chromosomes 18 and 13, as well as missing or duplicate X chromosomes—all causes of birth defects in infants.


The Biophysicist: Stephen Quake showed how the genome of an unborn child could be revealed.
MAX WHITTAKER
Last July, the scientific founder of Verinata, Stanford University biophysicist Stephen Quake, showed how in addition to detecting extra chromosomes, sequencing the DNA in the blood of a pregnant woman could reveal the full genetic code of a fetus, letter for letter. Shendure’s lab did something similar, as have two teams in China.

Reconstructing the six billion chemical letters of a fetal genome from those DNA fragments isn’t easy. It requires lots of extra sequencing to see past the mother’s genes. Shendure says the bill came to $50,000, and Quake’s lab cut its experiment short after running up similar expenses. Yet the work showed that a genome readout might act as a kind of universal test not only for extra chromosomes but for common congenital diseases, too. Those are conditions, like cystic fibrosis or beta-thalassemia, that are caused when a person inherits two defective versions of a particular gene, one from each parent. There are about 3,000 such diseases whose precise genetic cause is known.

Some 200 other maladies, including some cases of autism, are caused by known duplications or deletions of larger swaths of DNA. A genome test would show all of them.

Quake says proving that a full genome readout is possible was the “logical extension” of the underlying technology. Yet what’s much less clear to Quake and others is whether a universal DNA test will ever become important or routine in medicine, as the more targeted test for Down syndrome has become. “We did it as an academic exercise, just for the hell of it,” he says. “But if you ask me, ‘Are we going to know the genomes of children at birth?’ I’d ask you, ‘Why?’ I get stuck on the why.” Quake says he’s now refining the technology so that it could be used to inexpensively pull out information on just the most medically important genes.

The problem is that it’s simply not clear whether doctors, or parents, really want so much information. That’s a challenge Illumina has already encountered in its Individual Genome Sequencing service, with which it first offered genome sequencing to medical patients in 2009. Yet the service hasn’t exactly taken off. Illumina now decodes about one genome a day for medical reasons (mostly of adults with cancer or young children with mystery ailments). What’s clear is that the ability to gather DNA data has outstripped the ability to understand that information, which means it has also outstripped the medical demand for it. “Showing the utility of the genome is the main challenge going forward,” says Mostafa Ronaghi, Illumina’s chief technical officer.

Why Worry?

Illumina’s Jay Flatley is the person who engineered the Verinata takeover. The 60-year-old chief executive has led his company to $1.15 billion in revenue by besting other makers of sequencing machines and last year also resisted a hostile takeover offer of $6.7 billion from Roche, the world’s largest diagnostics firm. Flatley convinced shareholders not to accept the deal, promising to make genomics a “routine” part of people’s lives, increasing Illumina’s profits.

Flatley has predicted for years that genome sequencing will become a reality in medicine—specifically, that every child will get its genome sequenced “at birth.” So does he now think it could happen even earlier, during pregnancy? In a field with a reputation for wild, unfulfilled promises, Flatley is known as a cool realist whose predictions often come true. “It’s not the technology that is limiting. It will be clearly possible to do this in two years,” he says. But a commercial market is much further away than that. “Most people would have an inherently negative reaction, and for good reason.”

Flatley has predicted for years that genome sequencing will become a reality in medicine—specifically, that every child will get its genome sequenced “at birth.” So does he now think it could happen even earlier, during pregnancy? In a field with a reputation for wild, unfulfilled promises, Flatley is known as a cool realist whose predictions often come true. “It’s not the technology that is limiting. It will be clearly possible to do this in two years,” he says. But a commercial market is much further away than that. “Most people would have an inherently negative reaction, and for good reason.”

While adults can decide whether to undergo genome sequencing, an unborn child can’t consent to knowing its genes.
The problem is that having more information about a fetus’s traits could present doctors and parents with a deluge of information they aren’t able to understand or act on. And if they do act, that could be controversial, too. “Whole-genome sequencing could open Pandora’s box,” says Henry Greely, a law professor who studies bioethics at Stanford. “You’d have the whole sequence, so you might be able to look for straight nose, curly hair. How many parents are going to abort a fetus because of male pattern baldness? I don’t think many. But it’s probably more than zero.” Greely says that because fetal DNA is detectable in the bloodstream so early in pregnancy—as early as six or eight weeks—the pregnancy could be ended relatively easily.


The Ethicist: Morris Foster wonders if we will treat children differently if we know their DNA.
SHANE BROWN
One doesn’t have to look far for a case that could stir concerns about eugenics. This year, with its chromosome-­counting test, Verinata began offering a screen for Klinefelter syndrome, in which males have an extra X chromosome. The condition—which causes reduced testosterone, feminine features, and often infertility—affects 1 in 1,000 men, so about as many American men have it as live in Pittsburgh. What’s more, the symptoms can be so mild that some of those affected don’t even realize it. Even so, about half of women choose to end a Klinefelter pregnancy. If Verinata’s test is widely applied, many more women will have to decide whether to make that choice.

Dennis Lo believes that as fetal DNA sequencing advances, test makers should restrict themselves to reporting just the 20 or so most common serious diseases. “We are going to face the challenge of what do you look for and how do you counsel women,” he says. “I think we must use the technology in an ethical fashion and should refrain from analyzing things that are not life-threatening. Like predisposition to diabetes when someone is 40 years old. We don’t even know what medicine would be in 40 years, so why worry the mother about that?”

Morris Foster, an anthropologist at the University of Oklahoma who heads a panel of ethics advisors that Illumina has hired, says he and Flatley have discussed whole-genome sequencing of the unborn. “It’s clearly something that is on the horizon,” he says. “My advice to Illumina is, ‘You are a lab receiving a physician order. You don’t second-guess the physician.’ The ethical advice I would give to a physician is much more complex and nuanced.”

Medical groups are still struggling to formulate rules for handling genomic data for adults. And Foster says prenatal tests would make the legal and ethical obligations facing a doctor that much more complicated. For one thing, he says, while adults can decide whether to undergo genome sequencing, an unborn child can’t consent to knowing its genes. And that knowledge could affect a person’s entire life. “The whole sequence invariably tells you more information than you can act on,” he says. “Yet because you can generate that data, it’s likely that we will. Instead of stopping people from knowing things about themselves, you’d want to use it in a way that doesn’t create anxiety or strain families and medical resources.”

Foster fears that, if anything, people will put too much stock in genes. “I think the greatest risk is the overinterpretation of genetic findings. That doctors will think a variant associated with diabetes means you are going to get diabetes. Or that the absence of it means you are not,” he says. For parents, such probabilities might seem like certainties, even if they aren’t really. “If they bring a child to term with a genetic-based risk, would it cause the parents to treat the child otherwise?”

Right now, Illumina’s medical genome lab takes orders only for adult DNA data, or for sick children. And its new subsidiary Verinata carries out only an improved version of fetal chromosome tests that are familiar to doctors. Even so, given the quick advance of prenatal DNA technology in the lab, Flatley thinks society may need some new laws. “What would help a lot is legislation that says you can’t do certain things,” he says. Partly, that argument is self-serving: a messy social debate is going to slow down genome sequencing. On the wall of the company cafeteria, next to a towering row of framed patents Illumina has won, hangs a newspaper article from 2009, in which Flatley is quoted as predicting that all newborns will have their genomes sequenced by 2019, six years from now. In it, the CEO struck a by now familiar note. The limits to the technology of DNA sequencing, and to his company’s prospects, “are sociological,” he said. The only constraints are “when and where people think it can be applied.”

by Antonio Regalado",0.01% - 0.1%,"The article discusses the rapid advancements in prenatal DNA testing, which allow for the analysis of a fetus's genetic information through a simple blood test from the mother. This technology has improved significantly, enabling the detection of chromosomal abnormalities and even whole-genome sequencing.

However, this raises ethical concerns about what to do with all the additional information that can be gathered from these tests. For example, would parents choose to abort a fetus based on non-life-threatening conditions such as male pattern baldness or a predisposition to diabetes later in life? Dennis Lo, a pioneer in the field of prenatal DNA testing, suggests limiting reports to only serious and life-threatening diseases.

The article also touches on the issue of informed consent. Since an unborn child cannot provide consent for genetic testing, it raises questions about who should make these decisions and how much information should be shared with parents.

Furthermore, there are concerns that doctors and patients may put too much stock in genetic findings, leading to overinterpretation and potentially causing anxiety or strain on families and medical resources. Morris Foster, an anthropologist and ethics advisor, fears that this could lead to the treatment of children differently based on their genetic profiles.

The article concludes by highlighting the need for new laws and regulations to govern the use of prenatal DNA testing. Jay Flatley, CEO of Illumina, suggests that legislation is needed to establish guidelines for what can and cannot be done with this technology. Ultimately, the constraints on the development and application of prenatal DNA testing are not technological but rather sociological, as society grapples with the implications of this powerful new tool.

Some potential discussion points based on this article include:

* What are the potential benefits and risks of prenatal DNA testing?
* How should doctors and parents handle the vast amount of information that can be gathered from these tests?
* Should there be limits on what genetic conditions can be tested for, and if so, who should decide those limits?
* How might prenatal DNA testing affect our societal attitudes towards genetic differences and disabilities?
* What role should government regulation play in governing the use of prenatal DNA testing?",Biotech,"The article discusses the rapid advancements in prenatal DNA testing and the potential implications for parents, doctors, and society as a whole. With new technologies emerging that allow for non-invasive fetal genome sequencing, the amount of information available to parents about their unborn child's traits is increasing exponentially.

One of the main concerns raised in the article is the potential for overinterpretation of genetic findings. Doctors may mislead parents into thinking that certain genetic variants are more significant than they actually are, leading to unnecessary anxiety and strain on families. Additionally, there is a risk that parents will treat their children differently based on their genetic makeup, potentially stigmatizing or favoring certain traits.

Another issue discussed in the article is the ethics of prenatal testing for non-life-threatening conditions. The example given is Klinefelter syndrome, where about half of women choose to end the pregnancy despite the condition being relatively mild and not necessarily debilitating. This raises questions about what constitutes a ""life-threatening"" condition and whether parents should have the right to terminate pregnancies based on genetic information.

The article also highlights the need for clear guidelines and regulations around prenatal genome sequencing. Morris Foster, an anthropologist and ethics advisor to Illumina, suggests that medical groups are still struggling to formulate rules for handling genomic data in adults, let alone in prenatal cases. He argues that new laws may be needed to restrict certain uses of genetic information.

Illumina's CEO, Jay Flatley, is quoted as saying that the limits to DNA sequencing technology are ""sociological,"" meaning that they depend on when and where people think it can be applied. This raises questions about how society will navigate these complex issues and whether we need new laws to govern the use of genetic information in prenatal testing.

Some potential implications of prenatal genome sequencing include:

* Overinterpretation of genetic findings leading to unnecessary anxiety and strain on families
* Stigmatization or favoritism based on genetic traits
* Increased pressure to terminate pregnancies for non-life-threatening conditions
* Need for clear guidelines and regulations around prenatal genome sequencing

Overall, the article suggests that we need a nuanced discussion about the ethics and implications of prenatal genome sequencing and its potential applications in society."
Deep Learning,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/deep-learning/,"Deep Learning
With massive amounts of computational power, machines can now recognize objects and translate speech in real time. Artificial intelligence is finally getting smart.
by Robert D. Hof archive page
April 23, 2013

JIMMY TURRELL
When Ray Kurzweil met with Google CEO Larry Page last July, he wasn’t looking for a job. A respected inventor who’s become a machine-intelligence futurist, Kurzweil wanted to discuss his upcoming book How to Create a Mind. He told Page, who had read an early draft, that he wanted to start a company to develop his ideas about how to build a truly intelligent computer: one that could understand language and then make inferences and decisions on its own.


It quickly became obvious that such an effort would require nothing less than Google-scale data and computing power. “I could try to give you some access to it,” Page told Kurzweil. “But it’s going to be very difficult to do that for an independent company.” So Page suggested that Kurzweil, who had never held a job anywhere but his own companies, join Google instead. It didn’t take Kurzweil long to make up his mind: in January he started working for Google as a director of engineering. “This is the culmination of literally 50 years of my focus on artificial intelligence,” he says.

Kurzweil was attracted not just by Google’s computing resources but also by the startling progress the company has made in a branch of AI called deep learning. Deep-learning software attempts to mimic the activity in layers of neurons in the neocortex, the wrinkly 80 percent of the brain where thinking occurs. The software learns, in a very real sense, to recognize patterns in digital representations of sounds, images, and other data.

The basic idea—that software can simulate the neocortex’s large array of neurons in an artificial “neural network”—is decades old, and it has led to as many disappointments as breakthroughs. But because of improvements in mathematical formulas and increasingly powerful computers, computer scientists can now model many more layers of virtual neurons than ever before.

With this greater depth, they are producing remarkable advances in speech and image recognition. Last June, a Google deep-learning system that had been shown 10 million images from YouTube videos proved almost twice as good as any previous image recognition effort at identifying objects such as cats. Google also used the technology to cut the error rate on speech recognition in its latest Android mobile software. In October, Microsoft chief research officer Rick Rashid wowed attendees at a lecture in China with a demonstration of speech software that transcribed his spoken words into English text with an error rate of 7 percent, translated them into Chinese-language text, and then simulated his own voice uttering them in Mandarin. That same month, a team of three graduate students and two professors won a contest held by Merck to identify molecules that could lead to new drugs. The group used deep learning to zero in on the molecules most likely to bind to their targets.

Google in particular has become a magnet for deep learning and related AI talent. In March the company bought a startup cofounded by Geoffrey Hinton, a University of Toronto computer science professor who was part of the team that won the Merck contest. Hinton, who will split his time between the university and Google, says he plans to “take ideas out of this field and apply them to real problems” such as image recognition, search, and natural-language understanding, he says.

All this has normally cautious AI researchers hopeful that intelligent machines may finally escape the pages of science fiction. Indeed, machine intelligence is starting to transform everything from communications and computing to medicine, manufacturing, and transportation. The possibilities are apparent in IBM’s Jeopardy!-winning Watson computer, which uses some deep-learning techniques and is now being trained to help doctors make better decisions. Microsoft has deployed deep learning in its Windows Phone and Bing voice search.

Extending deep learning into applications beyond speech and image recognition will require more conceptual and software breakthroughs, not to mention many more advances in processing power. And we probably won’t see machines we all agree can think for themselves for years, perhaps decades—if ever. But for now, says Peter Lee, head of Microsoft Research USA, “deep learning has reignited some of the grand challenges in artificial intelligence.”

Building a Brain

There have been many competing approaches to those challenges. One has been to feed computers with information and rules about the world, which required programmers to laboriously write software that is familiar with the attributes of, say, an edge or a sound. That took lots of time and still left the systems unable to deal with ambiguous data; they were limited to narrow, controlled applications such as phone menu systems that ask you to make queries by saying specific words.

Neural networks, developed in the 1950s not long after the dawn of AI research, looked promising because they attempted to simulate the way the brain worked, though in greatly simplified form. A program maps out a set of virtual neurons and then assigns random numerical values, or “weights,” to connections between them. These weights determine how each simulated neuron responds—with a mathematical output between 0 and 1—to a digitized feature such as an edge or a shade of blue in an image, or a particular energy level at one frequency in a phoneme, the individual unit of sound in spoken syllables.

Programmers would train a neural network to detect an object or phoneme by blitzing the network with digitized versions of images containing those objects or sound waves containing those phonemes. If the network didn’t accurately recognize a particular pattern, an algorithm would adjust the weights. The eventual goal of this training was to get the network to consistently recognize the patterns in speech or sets of images that we humans know as, say, the phoneme “d” or the image of a dog. This is much the same way a child learns what a dog is by noticing the details of head shape, behavior, and the like in furry, barking animals that other people call dogs.

Some of today’s artificial neural networks can train themselves to recognize complex patterns.
But early neural networks could simulate only a very limited number of neurons at once, so they could not recognize patterns of great complexity. They languished through the 1970s.


JIMMY TURRELL
In the mid-1980s, Hinton and others helped spark a revival of interest in neural networks with so-called “deep” models that made better use of many layers of software neurons. But the technique still required heavy human involvement: programmers had to label data before feeding it to the network. And complex speech or image recognition required more computer power than was then available.

Finally, however, in the last decade ­Hinton and other researchers made some fundamental conceptual breakthroughs. In 2006, Hinton developed a more efficient way to teach individual layers of neurons. The first layer learns primitive features, like an edge in an image or the tiniest unit of speech sound. It does this by finding combinations of digitized pixels or sound waves that occur more often than they should by chance. Once that layer accurately recognizes those features, they’re fed to the next layer, which trains itself to recognize more complex features, like a corner or a combination of speech sounds. The process is repeated in successive layers until the system can reliably recognize phonemes or objects.

Like cats. Last June, Google demonstrated one of the largest neural networks yet, with more than a billion connections. A team led by Stanford computer science professor Andrew Ng and Google Fellow Jeff Dean showed the system images from 10 million randomly selected YouTube videos. One simulated neuron in the software model fixated on images of cats. Others focused on human faces, yellow flowers, and other objects. And thanks to the power of deep learning, the system identified these discrete objects even though no humans had ever defined or labeled them.

What stunned some AI experts, though, was the magnitude of improvement in image recognition. The system correctly categorized objects and themes in the ­YouTube images 16 percent of the time. That might not sound impressive, but it was 70 percent better than previous methods. And, Dean notes, there were 22,000 categories to choose from; correctly slotting objects into some of them required, for example, distinguishing between two similar varieties of skate fish. That would have been challenging even for most humans. When the system was asked to sort the images into 1,000 more general categories, the accuracy rate jumped above 50 percent.

Big Data

Training the many layers of virtual neurons in the experiment took 16,000 computer processors—the kind of computing infrastructure that Google has developed for its search engine and other services. At least 80 percent of the recent advances in AI can be attributed to the availability of more computer power, reckons Dileep George, cofounder of the machine-learning startup Vicarious.

There’s more to it than the sheer size of Google’s data centers, though. Deep learning has also benefited from the company’s method of splitting computing tasks among many machines so they can be done much more quickly. That’s a technology Dean helped develop earlier in his 14-year career at Google. It vastly speeds up the training of deep-learning neural networks as well, enabling Google to run larger networks and feed a lot more data to them.

Already, deep learning has improved voice search on smartphones. Until last year, Google’s Android software used a method that misunderstood many words. But in preparation for a new release of Android last July, Dean and his team helped replace part of the speech system with one based on deep learning. Because the multiple layers of neurons allow for more precise training on the many variants of a sound, the system can recognize scraps of sound more reliably, especially in noisy environments such as subway platforms. Since it’s likelier to understand what was actually uttered, the result it returns is likelier to be accurate as well. Almost overnight, the number of errors fell by up to 25 percent—results so good that many reviewers now deem Android’s voice search smarter than Apple’s more famous Siri voice assistant.

For all the advances, not everyone thinks deep learning can move artificial intelligence toward something rivaling human intelligence. Some critics say deep learning and AI in general ignore too much of the brain’s biology in favor of brute-force computing.

One such critic is Jeff Hawkins, founder of Palm Computing, whose latest venture, Numenta, is developing a machine-learning system that is biologically inspired but does not use deep learning. Numenta’s system can help predict energy consumption patterns and the likelihood that a machine such as a windmill is about to fail. Hawkins, author of On Intelligence, a 2004 book on how the brain works and how it might provide a guide to building intelligent machines, says deep learning fails to account for the concept of time. Brains process streams of sensory data, he says, and human learning depends on our ability to recall sequences of patterns: when you watch a video of a cat doing something funny, it’s the motion that matters, not a series of still images like those Google used in its experiment. “Google’s attitude is: lots of data makes up for everything,” Hawkins says.

But if it doesn’t make up for everything, the computing resources a company like Google throws at these problems can’t be dismissed. They’re crucial, say deep-learning advocates, because the brain itself is still so much more complex than any of today’s neural networks. “You need lots of computational resources to make the ideas work at all,” says Hinton.

What’s Next

Although Google is less than forthcoming about future applications, the prospects are intriguing. Clearly, better image search would help YouTube, for instance. And Dean says deep-learning models can use phoneme data from English to more quickly train systems to recognize the spoken sounds in other languages. It’s also likely that more sophisticated image recognition could make Google’s self-driving cars much better. Then there’s search and the ads that underwrite it. Both could see vast improvements from any technology that’s better and faster at recognizing what people are really looking for—maybe even before they realize it.

This is what intrigues Kurzweil, 65, who has long had a vision of intelligent machines. In high school, he wrote software that enabled a computer to create original music in various classical styles, which he demonstrated in a 1965 appearance on the TV show I’ve Got a Secret. Since then, his inventions have included several firsts—a print-to-speech reading machine, software that could scan and digitize printed text in any font, music synthesizers that could re-create the sound of orchestral instruments, and a speech recognition system with a large vocabulary.

Sergey Brin has said he wants to build a benign version of HAL in “2001: A Space Odyssey.”
Today, he envisions a “cybernetic friend” that listens in on your phone conversations, reads your e-mail, and tracks your every move—if you let it, of course—so it can tell you things you want to know even before you ask. This isn’t his immediate goal at Google, but it matches that of Google cofounder Sergey Brin, who said in the company’s early days that he wanted to build the equivalent of the sentient computer HAL in 2001: A Space Odyssey—except one that wouldn’t kill people.

For now, Kurzweil aims to help computers understand and even speak in natural language. “My mandate is to give computers enough understanding of natural language to do useful things—do a better job of search, do a better job of answering questions,” he says. Essentially, he hopes to create a more flexible version of IBM’s Watson, which he admires for its ability to understand Jeopardy! queries as quirky as “a long, tiresome speech delivered by a frothy pie topping.” (Watson’s correct answer: “What is a meringue harangue?”)

Kurzweil isn’t focused solely on deep learning, though he says his approach to speech recognition is based on similar theories about how the brain works. He wants to model the actual meaning of words, phrases, and sentences, including ambiguities that usually trip up computers. “I have an idea in mind of a graphical way to represent the semantic meaning of language,” he says.

That in turn will require a more comprehensive way to graph the syntax of sentences. Google is already using this kind of analysis to improve grammar in translations. Natural-language understanding will also require computers to grasp what we humans think of as common-sense meaning. For that, Kurzweil will tap into the Knowledge Graph, Google’s catalogue of some 700 million topics, locations, people, and more, plus billions of relationships among them. It was introduced last year as a way to provide searchers with answers to their queries, not just links.

Finally, Kurzweil plans to apply deep-learning algorithms to help computers deal with the “soft boundaries and ambiguities in language.” If all that sounds daunting, it is. “Natural-language understanding is not a goal that is finished at some point, any more than search,” he says. “That’s not a project I think I’ll ever finish.”

Though Kurzweil’s vision is still years from reality, deep learning is likely to spur other applications beyond speech and image recognition in the nearer term. For one, there’s drug discovery. The surprise victory by Hinton’s group in the Merck contest clearly showed the utility of deep learning in a field where few had expected it to make an impact.

That’s not all. Microsoft’s Peter Lee says there’s promising early research on potential uses of deep learning in machine vision—technologies that use imaging for applications such as industrial inspection and robot guidance. He also envisions personal sensors that deep neural networks could use to predict medical problems. And sensors throughout a city might feed deep-learning systems that could, for instance, predict where traffic jams might occur.

In a field that attempts something as profound as modeling the human brain, it’s inevitable that one technique won’t solve all the challenges. But for now, this one is leading the way in artificial intelligence. “Deep learning,” says Dean, “is a really powerful metaphor for learning about the world.”

by Robert D. Hof",1-10%,"The article discusses the latest advancements in artificial intelligence (AI), particularly in deep learning, a technique that involves modeling the human brain's neural networks to enable computers to learn and make decisions on their own.

Google has been at the forefront of this research, with its acquisition of DeepMind, a UK-based AI startup, and the hiring of renowned AI expert Ray Kurzweil. Google's researchers have developed a new deep-learning algorithm that can recognize objects in images and videos with unprecedented accuracy.

The article highlights several potential applications of deep learning, including:

1. Image recognition: Google's algorithm can be used to improve image search on YouTube and other platforms.
2. Speech recognition: Deep learning can be applied to speech recognition systems to improve their accuracy and enable them to recognize spoken words in different languages.
3. Natural language processing: Kurzweil aims to develop a system that can understand and respond to natural language queries, enabling computers to communicate with humans more effectively.
4. Autonomous vehicles: Deep learning can be used to improve the image recognition capabilities of self-driving cars, making them safer and more efficient.

The article also mentions other potential applications of deep learning, such as:

1. Drug discovery: Researchers at Merck have used deep learning algorithms to identify new compounds that could lead to breakthroughs in disease treatment.
2. Machine vision: Deep learning can be applied to machine vision technologies to enable robots and machines to better understand their surroundings.
3. Personal sensors: Deep learning can be used to analyze data from personal sensors, such as wearable devices, to predict medical problems before they occur.

Overall, the article suggests that deep learning is a powerful technique that has the potential to revolutionize many areas of AI research and application.",Software,"The article discusses the current advancements and future applications of deep learning, a subset of machine learning that involves neural networks with multiple layers. The technology has shown significant improvements in speech recognition, image recognition, and natural language processing.

Google is at the forefront of this research, with its engineers exploring various applications of deep learning. One area of focus is on improving search results by better understanding what users are looking for. Another potential application is in self-driving cars, where deep learning can be used to recognize objects and scenes more accurately.

The article also mentions other researchers who are working on deep learning, including Geoff Hinton, a Canadian computer scientist who has made significant contributions to the field. Hinton's team recently won a competition sponsored by Merck, where they demonstrated the effectiveness of deep learning in drug discovery.

Other potential applications of deep learning mentioned in the article include:

* Machine vision: Deep learning can be used for industrial inspection and robot guidance.
* Personal sensors: Deep neural networks could use data from personal sensors to predict medical problems.
* City planning: Sensors throughout a city could feed into deep-learning systems that predict traffic patterns.

The article concludes by stating that while deep learning is not the only approach to artificial intelligence, it has shown significant promise in recent years and will likely continue to be an important area of research.

Some key quotes from the article:

* ""Deep learning is a really powerful metaphor for learning about the world."" - Jeff Dean
* ""Natural-language understanding is not a goal that is finished at some point, any more than search."" - Ray Kurzweil

Overall, the article highlights the current advancements and future potential of deep learning in various fields."
Additive Manufacturing,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/additive-manufacturing/,"Additive Manufacturing
GE, the world’s largest manufacturer, is on the verge of using 3-D printing to make jet parts.
by Martin LaMonica archive page
April 23, 2013
Prototypes of brackets for airplane engines show how additive manufacturing can produce complex, precisely designed shapes like the one at right.
Prototypes of brackets for airplane engines show how additive manufacturing can produce complex, precisely designed shapes like the one at right.
JENNIFER MAY
General Electric is making a radical departure from the way it has traditionally manufactured things. Its aviation division, the world’s largest supplier of jet engines, is preparing to produce a fuel nozzle for a new aircraft engine by printing the part with lasers rather than casting and welding the metal. The technique, known as additive manufacturing (because it builds an object by adding ultrathin layers of material one by one), could transform how GE designs and makes many of the complex parts that go into everything from gas turbines to ultrasound machines.

Additive manufacturing—the industrial version of 3-D printing—is already used to make some niche items, such as medical implants, and to produce plastic prototypes for engineers and designers. But the decision to mass-produce a critical metal-alloy part to be used in thousands of jet engines is a significant milestone for the technology. And while 3-D printing for consumers and small entrepreneurs has received a great deal of publicity, it is in manufacturing where the technology could have its most significant commercial impact (see “The Difference Between Makers and Manufacturers,” January/February 2013).

Last fall, GE purchased a pair of companies with know-how in automated precision manufacturing of metals and then folded the technology into the operations of GE Aviation. That group doesn’t have much time to demonstrate that its new technology can work at scale. CFM International, GE’s joint venture with France’s Snecma, will use the 3-D-printed nozzles in its LEAP jet engine, due to go into planes in late 2015 or early 2016 (CFM says it already has commitments of $22 billion). Each engine will use 10 to 20 nozzles; GE needs to make 25,000 of the nozzles annually within three years.

GE chose the additive process for manufacturing the nozzles because it uses less material than conventional techniques. That reduces GE’s production costs and, because it makes the parts lighter, yields significant fuel savings for airlines. Conventional techniques would require welding about 20 small pieces together, a labor-intensive process in which a high percentage of the material ends up being scrapped. Instead, the part will be built from a bed of cobalt-chromium powder. A computer-controlled laser shoots pinpoint beams onto the bed to melt the metal alloy in the desired areas, creating 20-micrometer-­thick layers one by one. The process is a faster way to make complex shapes because the machines can run around the clock. And additive manufacturing in general conserves material because the printer can handle shapes that eliminate unnecessary bulk and create them without the typical waste.

The rest of GE—together with its competitors—is watching closely. GE Power & Water, which makes large gas and wind turbines, has already identified parts it can make with the additive process, and GE Healthcare has developed a method to print transducers, the expensive ceramic probes used in ultrasound machines. “It’s really fundamentally changing the way we think about the company,” says Mark Little, GE’s chief technology officer.

Breaking with traditional manufacturing techniques, such as casting and machining material, gives GE product designers far greater flexibility. Additive manufacturing machines work directly from a computer model, so people can devise completely new shapes without regard for existing manufacturing ­limitations. “We can make configurations that we just couldn’t before,” Little says.

GE engineers are starting to explore how to use additive manufacturing with a wider range of metal alloys, including some materials specifically designed for 3-D printing. GE Aviation, for one, is looking to use titanium, aluminum, and nickel-chromium alloys. A single part could be made of multiple alloys, letting designers tailor its material characteristics in a way that’s not possible with casting. A blade for an engine or turbine, for example, could be made with different materials so that one end is optimized for strength and the other for heat resistance.

All that is still on paper—or rather, in the computerized designs of product engineers. For now, GE’s engine nozzle—a part small enough to fit in the palm of your hand—will be the first big test of whether additive manufacturing can revolutionize the way complex high-performance products are made.

by Martin LaMonica",0.5% - 5%,Positive,Hardware,methodology
Big Data from Cheap Phones,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/big-data-from-cheap-phones/,"Big Data from Cheap Phones
Collecting and analyzing information from simple cell phones can provide surprising insights into how people move about and behave—and even help us understand the spread of diseases.
by David Talbot archive page
April 23, 2013

DAN PAGE
At a computer in her office at the Harvard School of Public Health in Boston, epidemiologist Caroline Buckee points to a dot on a map of Kenya’s western highlands, representing one of the nation’s thousands of cell-phone towers. In the fight against malaria, Buckee explains, the data transmitted from this tower near the town of Kericho has been epidemiological gold.

When she and her colleagues studied the data, she found that people making calls or sending text messages originating at the Kericho tower were making 16 times more trips away from the area than the regional average. What’s more, they were three times more likely to visit a region northeast of Lake Victoria that records from the health ministry identified as a malaria hot spot. The tower’s signal radius thus covered a significant waypoint for transmission of malaria, which can jump from human to human via mosquitoes. Satellite images revealed the likely culprit: a busy tea plantation that was probably full of migrant workers. The implication was clear, Buckee says. “There will be a ton of infected [people] there.”

This work is now feeding into a new set of predictive models she is building. They show, for example, that even though malaria cases were seen at the tea plantation, taking steps to control malaria there would have less effect on the disease’s spread than concentrating those efforts at the source: Lake Victoria. That region has long been understood as a major center of malaria, but what hasn’t been available before is detailed information about the patterns of human travel there: how many people are coming and going, when they’re arriving and departing, which specific places they’re coming to, and which of those destinations attract the most people traveling on to new places.


Caroline Buckee, a Harvard epidemiologist, is using detailed data on population movements—gleaned from mobile phones— to build precise new tools for fighting the spread of malaria.
ADAM DETOUR
Existing efforts to gather that kind of travel data are spotty at best; sometimes public-health workers literally count people at transportation hubs, Buckee says, or nurses in far-flung clinics ask newly diagnosed malaria victims where they’ve been recently. “At many border crossings in Africa, they keep little slips of paper—but the slips get lost, and nobody keeps track,” she says. “We have abstractions and general models on travel patterns but haven’t been able to do this properly—ever.”

The data mining will help inform the design of new measures that are likely to include cheap, targeted campaigns of text messages—for example, warning visitors entering the Kericho tower’s signal zone to use bed netting. And it will help officials choose where to focus mosquito control efforts in the malarial areas. “You don’t want to be spraying every puddle for mosquito larvae all the time. But if you know there is a ton of importation from a certain spot, you want to increase your control program at that spot,” Buckee says. “And now I can pinpoint where the importation of a disease is especially important.”

Buckee’s most recent study, published last year in Science and based on records from 15 million Kenyan phones, is a result of a collaboration with her husband, Nathan Eagle, who has been working to make sense of cell-phone data for more than a decade. In the mid-2000s, after getting attention for his work mining data from the phones of volunteers at MIT, Eagle started to get calls from mobile carriers asking for insight into questions like why customers canceled their phone plans. Eagle began working with them. And when the couple spent 18 months in Africa starting in 2006—Buckee was doing work on the genetics of the malaria parasite—he studied call data for various purposes, trying to understand phenomena like ethnic divisions in Nairobi slums and the spread of cholera in Rwanda. Buckee’s results show what might be possible when the technology is turned on public-­health problems. “This demonstrated ‘Yeah, we can really provide not just insight, but actually something that is actionable,’” says Eagle, now CEO of Jana, which runs mobile-phone surveys in the developing world. “This really does work.”

That demonstration suggests how such data might be harnessed to build tools that health-care workers, governments, and others can use to detect and monitor epidemics, manage disasters, and optimize transportation systems. Already, similar efforts are being directed toward goals as varied as understanding commuting patterns around Paris and managing festival crowds in Belgium. But mining phone records could be particularly useful in poor regions, where there’s often little or no other data-­gathering infrastructure. “We are just at the start of using this data for these purposes,” says ­Vincent Blondel, a professor of applied mathematics at the University of Louvain in Belgium and a leading researcher on data gleaned from cell phones. “The exponential adoption of mobile phones in low-income settings—and the new willingness of some carriers to release data—will lead to new technological tools that could change everything.”

“This is the future of epidemiology. If we are to eradicate malaria, this is how we will do it.”
Blank Slate

The world’s six billion mobile phones generate huge amounts of data—including location tracking and information on commercial activity, search history, and links in social networks. Innumerable efforts to mine the data in different ways are under way in research and business organizations around the world. And of those six billion phones, five billion are in developing countries. Many of them are cheap phones that can do little besides make calls and send text messages. But all such activity can be tracked back to cell-phone towers, providing a rough way to trace a person’s movements. Throw in the spread of mobile payment technology for simple commerce and you have the raw material for insights not only into epidemiology but into employment trends, social tensions, poverty, transportation, and economic activity.

The prospect of mining data from phones is especially tantalizing in poor countries, where detailed, up-to-date information on these matters has been scarce. “In the developing world, there isn’t a functioning census, you don’t know where traffic is, you don’t always have the data-gathering infrastructure of government,” says Alex “Sandy” Pentland, director of the Human Dynamics Lab at MIT, who has long been interested in insights from data created by mobile-phone use. “But all of a sudden, the one thing you do have—cell phones everywhere, especially in the past few years—can give you the equivalent of all that infrastructure already built in the developed world.”


This map, a product of cell-phone data analytics, shows the most important sources of malaria infections (darker shades)—taking into account the potential for further transmission caused by human travel—as well as the major destinations of people exposed to the disease (lighter shades). It can be used to determine where best to focus warnings and mosquito control techniques.
SINK
When a call connects to a given base station, that station logs the ID number of the phone and the duration of the call; over time, this information can be used to get a sense of people’s regional movements and the shape of their social networks. Purchasing history on phones is also invaluable: records of agricultural purchases could be used to predict food supplies or shortages. And financial data collected by mobile payment systems can build credit histories and help millions of people without access to banking qualify for conventional loans. “The database analysis methods and the computers are very standard,” Pentland says. “It’s a matter of doing science and finding the right patterns.” Certain mobility patterns might relate to the spread of a disease; purchasing patterns could signify that a person has had a change in employment; behavioral changes or movement patterns might relate to the onset of an illness.

A powerful demonstration of how useful data from cheap phones can be came after the January 2010 earthquake in Haiti, which killed more than 200,000 people. Researchers at Sweden’s Karolinska Institute obtained data from Digicel, Haiti’s largest mobile carrier. They mined the daily movement data from two million phones—from 42 days before the earthquake to 158 days after—and concluded that 630,000 people who had been in Port-au-Prince on the day of the earthquake had left the city within three weeks. They also demonstrated that they could do such calculations in close to real time. They showed—within 12 hours of receiving the data—how many people had fled an area affected by a cholera outbreak, and where they went.

Most important, their work led to a model that could guide responses to future disasters. After analyzing data on pre-earthquake travel habits, the Swedish group found that Haitians generally fled the city for the same places where they’d spent Christmas and New Year’s Day. Such findings make it possible to predict where people will go when disaster hits.

Scaling Up

Until recently, these studies were done by researchers who made some special arrangement with carriers to get the data (Eagle obtained it through his academic connections). But last year Orange, the France-based global telecom giant, released to the world’s research community—subject to certain conditions and restrictions—data based on 2.5 billion anonymized records from five months’ worth of calls made by five million people in Ivory Coast. The first phase of this grand experiment involves seeing just what it’s possible to do with the data.

Nearly a hundred research groups worldwide leaped at the opportunity to analyze the records. The resulting papers were scheduled to be presented in May at a conference at MIT under the name Data for Development, part of a larger conference of data-mining projects in both the poor and rich worlds. “It’s the first time a large-scale mobile-phone data set has been released at that scale,” says Blondel, who is chairing the conference. The papers had not been formally released at the time of this writing. But one charts social and travel interactions across a traditional north-south ethnic divide, providing insights into how conflict might be averted; another proposes tools for mapping the spread of malaria and detecting disease outbreaks. One corporate lab built a transportation model using cell-phone data to track ridership on 539 buses, 5,000 mini-buses, and 11,000 shared taxis.


The world’s mobile carriers hold a data gold mine, says Nathan Eagle, CEO of Jana, which conducts surveys on mobile phones. But he says widespread use of the data will require new business models and privacy protections.
ADAM DETOUR
Even if the Ivory Coast experiment succeeds, replicating it in other countries may not be easy. Last year the World Economic Forum—the group of leading industry, academic, and political figures who converge annually at Davos, Switzerland—issued a call for governments, development organizations, and companies to develop data analysis tools to improve the lives of people in the poor world. “I shouldn’t have to go to operators and say ‘I’ll do free consulting for you—and in exchange I want to use your data to improve lives,’” Eagle says. “The operators should want to be affiliated with this. Right now many of them don’t see the upside, but if we can get world leaders knocking on their doors saying ‘Let’s do this!’ maybe we can get a lot more done.”

This will take some careful work to protect privacy and prevent the data from being used in the service of oppression. Orange says it took pains to anonymize its data, but the field needs clear and widely agreed-upon ways to bring the information to market. “There are risks and benefits of having a data-driven society,” Pentland says. “There is a question of who owns the data and who controls it. You can imagine what Muammar Qaddafi would have done with this sort of data. Orange is taking the steps to figure out how to create a data commons that induces greater transparency, accountability, and efficiency—to tell where there are unusual events, extreme events, to tell us where the infrastructure is breaking down. There are all sorts of things we can do with it—but it has to be available.”

“We can really provide not just insight, but actually something that is actionable. This really does work.”
As these larger questions play out, Buckee and Eagle are working on refining and augmenting the data-mining tools in Kenya. Eagle aims to use surveys to sharpen and confirm the picture created by mining cell-phone data on a large scale. Call records alone are often not enough, he says; surveying even a few people could allow researchers to weed out erroneous assumptions about what those records show. Once, while analyzing phone data in Rwanda, Eagle noted that people had not moved around very much after a flood. At first, he theorized that many of them were bedridden with cholera. But it turned out that the flood had washed out the roads.

Buckee hopes to mine phone data to target drug-resistant strains of the malaria parasite. These strains, emerging in Cambodia and elsewhere, could reverse progress against the disease if allowed to proliferate, she warns. So she wants to begin merging data on the parasites’ spread into mobility models to help produce targeted disease-fighting strategies. “This is the future of epidemiology,” she says. “If we are to eradicate malaria, this is how we will do it.”

by David Talbot",2-20%,"The article discusses the potential for using mobile phone data to improve lives in developing countries. Researchers have been able to use cell phone records to track population movements and identify areas of need during disasters and epidemics. For example, a team of researchers used mobile phone data to track the spread of cholera in Haiti after the 2010 earthquake.

The article highlights several key points:

1. **Mobile phones are ubiquitous**: Even in poor countries, many people have access to mobile phones, making it possible to collect large amounts of data.
2. **Data can be used for good**: By analyzing mobile phone records, researchers can identify areas of need and provide actionable insights to aid organizations and governments.
3. **Scalability is key**: With the right tools and approaches, it's possible to analyze large datasets in near real-time, allowing for quick responses to emerging crises.
4. **Collaboration is necessary**: Researchers, carriers, and governments must work together to make mobile phone data available for analysis while protecting user privacy.

The article also mentions several examples of how mobile phone data has been used to improve lives, including:

1. **Tracking population movements during disasters**: Researchers have used mobile phone data to track the movement of people during natural disasters, such as earthquakes and floods.
2. **Identifying areas of need**: By analyzing mobile phone records, researchers can identify areas where aid is most needed during a crisis.
3. **Predicting disease outbreaks**: Mobile phone data has been used to predict the spread of diseases, such as malaria and cholera.

However, the article also notes that there are challenges to overcome, including:

1. **Protecting user privacy**: Researchers must ensure that mobile phone data is anonymized and protected from misuse.
2. **Developing new business models**: Carriers and researchers need to develop new business models that allow for the use of mobile phone data while protecting user interests.

Overall, the article suggests that mobile phone data has the potential to revolutionize the way we respond to crises in developing countries, but it will require collaboration, innovation, and a commitment to protecting user privacy.",Software,"The article discusses the use of mobile phone data to analyze and respond to various challenges in developing countries. Researchers have used anonymized cell-phone records to track population movements during disasters, disease outbreaks, and other events, providing valuable insights for humanitarian organizations and governments.

Some examples mentioned in the article include:

1. After the 2010 Haiti earthquake, researchers analyzed cell-phone data to track population movements and identify areas where aid was most needed.
2. In Kenya, a team used mobile phone data to monitor malaria outbreaks and track the movement of infected individuals.
3. A study in Ivory Coast used anonymized call records from five million people over five months to analyze social and travel interactions, providing insights into how conflict might be averted.

The article highlights the potential benefits of using mobile phone data for development, including:

1. Improved disaster response: By analyzing population movements during disasters, responders can better target aid efforts.
2. Enhanced disease surveillance: Mobile phone data can help track the spread of diseases and identify areas where interventions are most needed.
3. Better transportation planning: Cell-phone data can provide insights into ridership patterns on public transportation systems.

However, the article also notes some challenges and concerns:

1. Privacy protections: The use of mobile phone data raises concerns about individual privacy and the potential for misuse.
2. Data access: Researchers often rely on special arrangements with carriers to obtain data, which can be difficult to replicate in other countries.
3. Business models: New business models are needed to incentivize carriers to share their data while ensuring that it is used responsibly.

To address these challenges, experts propose:

1. Developing clear guidelines for anonymizing and sharing mobile phone data.
2. Creating new business models that benefit both carriers and humanitarian organizations.
3. Establishing a ""data commons"" that promotes transparency, accountability, and efficiency in the use of mobile phone data.

Overall, the article suggests that mobile phone data has the potential to revolutionize development efforts by providing valuable insights into population movements, disease spread, and other critical issues. However, careful consideration must be given to ensuring that this data is used responsibly and with appropriate safeguards for individual privacy."
Temporary Social Media,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/temporary-social-media/,"Temporary Social Media
Messages that quickly self-destruct could enhance the privacy of online communication and make people feel freer to be spontaneous.
by Jeffrey Rosen and Christine Rosen archive page
April 23, 2013

BRIAN CRONIN
One essential aspect of privacy is the ability to control how much we disclose to others. Unfortunately, we’ve lost much of that control now that every photo, chat, or status update posted on a social-media site can be stored in the cloud: even though we intended to share that information with someone, we don’t necessarily want it to stay available, out of context, forever. The weight of our digital pasts is emerging as the central privacy challenge of our time.

But what if people could make their posts vanish automatically—making social media more of an analogue to everyday conversations that aren’t recorded for posterity? That’s the promise of services such as Snapchat, a mobile-phone app whose popularity has increased dramatically during the past year. Evan Speigel and Bobby Murphy, who met as undergrads at Stanford, came up with the idea two years ago, around the time New York congressman Anthony Weiner accidentally made racy photos of himself public on Twitter and was forced to resign. Snapchat lets users take photos or short videos and then decide how long they will be visible to the recipient. After 10 seconds or less, the images disappear forever. (Not for nothing is Snapchat’s mascot a picture of a grinning ghost.)

From the beginning, the service appealed to teenagers looking for a more private way of sending each other sexy pictures. But “sexting” alone can’t account for all 100 million photos and videos exchanged on Snapchat every day. And Mark ­Zuckerberg must worry that Snapchat addresses some misgivings people have about privacy on Facebook; in December, Facebook launched a Snapchat copycat app called Poke.

What makes temporary social media so appealing? Snapchat’s founders often remark that they wanted to give people a way to express themselves through something besides the idealized self-­portraits many feel required to maintain on social-media sites. Snapchats might be more exciting to send and receive than other social-media posts because they are ephemeral, but they are also arguably a more natural way to communicate. Whereas Facebook and Twitter record and store your every offhand observation and casual interaction, interactions in temporary social media can be something like brief, in-person conversations: you can speak your mind without worrying that what you say will be part of your digital dossier forever.

Although Snapchat’s posture as the anti-­Facebook is a large part of its allure, eventually its founders will have to confront some of the same privacy challenges that have vexed Facebook. Snapchat contains an obvious technological vulnerability: images that were meant to vanish can still be saved if the recipient uses a screen-capture feature to take a picture of the message during the seconds it appears. (If the recipient does this, Snapchat notifies the sender, but by then it’s too late to stop the image from being preserved and shared.) Moreover, while Snapchat promises to erase photos from its servers, the company’s privacy policy adds that it “cannot guarantee that  the message data will be deleted in every case.”  As soon as a racy Snapchat picture of a celebrity goes viral, trust in the company could be eroded.

But regardless of the fate of Snapchat in particular, the idea of temporary social media is important because the ability to be candid and spontaneous—and to be that way with only some people and not others—is the essence of friendship, individuality, and creativity. Facebook and Twitter do make it possible for their members to wall off posts from the wider world and share them only with trusted people in certain circles. But since those posts still last forever, this capacity for limited sharing is technologically insecure. To the degree that temporary social networks increase our sense of control over the conditions of our personal exposure, they represent a first step toward a more nuanced kind of digital connection—one acknowledging that our desire to share can coexist with a desire for reticence, privacy, and the possibility of a fresh start.

by Jeffrey Rosen and Christine Rosen",2-20%,Positive,Software,product
Supergrids,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/supergrids/,"Supergrids
A high-power circuit breaker could finally make DC power grids practical.
by Kevin Bullis archive page
April 23, 2013
At ABB’s lab in Sweden, equipment such as corona shields—polished disks linked to form spheres—are used to test a high-voltage DC circuit breaker.
At ABB’s lab in Sweden, equipment such as corona shields—polished disks linked to form spheres—are used to test a high-voltage DC circuit breaker.
ABB
High-voltage DC power lines can efficiently transport electricity over thousands of kilometers and for long distances underwater, outperforming the AC lines that dominate transmission grids now. But for a century, AC prevailed because high-voltage DC could be used only for point-to-point transmission, not to form the integrated grid networks needed for a stable electricity system.

The Swiss conglomerate ABB has solved the main technical hurdle to such grids. It has developed a practical high-voltage DC circuit breaker that disconnects parts of the grid that have a problem, allowing the rest to keep working.

DC grids would be more efficient at connecting far-flung sources of renewable energy, allowing utilities to average out local variations in wind and solar power while bringing power to areas without much sunshine or wind. Solar power from the Sahara could power cloudy Germany, and wind power from all over Europe could keep the lights on at night. The result: more reliable renewable energy that can better compete with fossil fuels.


A simulation center develops controls for DC grids.
ABB

ABB workers at a DC-to-AC conversion station.
ABB
by Kevin Bullis",0.01-0.1%,Positive,Hardware,platform
Baxter: The Blue-Collar Robot,2013.0,https://www.technologyreview.com/10-breakthrough-technologies/2013/,https://www.technologyreview.com/technology/baxter-the-blue-collar-robot/,"Baxter: The Blue-Collar Robot
Rethink Robotics’ new creation is easy to interact with, but the innovations behind the robot show just how hard it is to get along with people.
by Will Knight archive page
April 23, 2013

KEN RICHARDSON
Rethink Robotics’ new creation is easy to interact with, but the innovations behind the robot show just how hard it is to get along with people.


KEN RICHARDSON
by Will Knight",0.01% - 0.1%,Positive,Hardware,product
Agricultural Drones,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/agricultural-drones/,"Agricultural Drones
Relatively cheap drones with advanced sensors and imaging capabilities are giving farmers new ways to increase yields and reduce crop damage.
by Chris Anderson archive page
April 23, 2014

RC RIVERA
Ryan Kunde is a winemaker whose family’s picture-perfect vineyard nestles in the Sonoma Valley north of San Francisco. But Kunde is not your average farmer. He’s also a drone operator—and he’s not alone. He’s part of the vanguard of farmers who are using what was once military aviation technology to grow better grapes using pictures from the air, part of a broader trend of using sensors and robotics to bring big data to precision agriculture.

What “drones” means to Kunde and the growing number of farmers like him is simply a low-cost aerial camera platform: either miniature fixed-wing airplanes or, more commonly, quadcopters and other multibladed small helicopters. These aircraft are equipped with an autopilot using GPS and a standard point-and-shoot camera controlled by the autopilot; software on the ground can stitch aerial shots into a high-­resolution mosaic map. Whereas a traditional radio-­controlled aircraft needs to be flown by a pilot on the ground, in Kunde’s drone the autopilot (made by my company, 3D Robotics) does all the flying, from auto takeoff to landing. Its software plans the flight path, aiming for maximum coverage of the vineyards, and controls the camera to optimize the images for later analysis.


Top: A drone from PrecisionHawk is equipped with multiple sensors to image fields.

Bottom: This image depicts vegetation in near-­infrared light to show chlorophyll levels.
PRECISIONHAWK
Agricultural Drones

Breakthrough
Easy-to-use ­agricultural drones equipped with ­cameras, for less than $1,000.
Why it matters
Close monitoring of crops could improve water use and pest management.
Key players
3D Robotics; Yamaha; PrecisionHawk
This low-altitude view (from a few meters above the plants to around 120 meters, which is the regulatory ceiling in the United States for unmanned aircraft operating without special clearance from the Federal Aviation Administration) gives a perspective that farmers have rarely had before. Compared with satellite imagery, it’s much cheaper and offers higher resolution. Because it’s taken under the clouds, it’s unobstructed and available anytime. It’s also much cheaper than crop imaging with a manned aircraft, which can run $1,000 an hour. Farmers can buy the drones outright for less than $1,000 each.

The advent of drones this small, cheap, and easy to use is due largely to remarkable advances in technology: tiny MEMS sensors (accelerometers, gyros, magnetometers, and often pressure sensors), small GPS modules, incredibly powerful processors, and a range of digital radios. All those components are now getting better and cheaper at an unprecedented rate, thanks to their use in smartphones and the extraordinary economies of scale of that industry. At the heart of a drone, the autopilot runs specialized software—often open-source programs created by communities such as DIY Drones, which I founded, rather than costly code from the aerospace industry.

Drones can provide farmers with three types of detailed views. First, seeing a crop from the air can reveal patterns that expose everything from irrigation problems to soil variation and even pest and fungal infestations that aren’t apparent at eye level. Second, airborne cameras can take multispectral images, capturing data from the infrared as well as the visual spectrum, which can be combined to create a view of the crop that highlights differences between healthy and distressed plants in a way that can’t be seen with the naked eye. Finally, a drone can survey a crop every week, every day, or even every hour. Combined to create a time-series animation, that imagery can show changes in the crop, revealing trouble spots or opportunities for better crop management.

It’s part of a trend toward increasingly data-driven agriculture. Farms today are bursting with engineering marvels, the result of years of automation and other innovations designed to grow more food with less labor. Tractors autonomously plant seeds within a few centimeters of their target locations, and GPS-guided harvesters reap the crops with equal accuracy. Extensive wireless networks backhaul data on soil hydration and environmental factors to faraway servers for analysis. But what if we could add to these capabilities the ability to more comprehensively assess the water content of soil, become more rigorous in our ability to spot irrigation and pest problems, and get a general sense of the state of the farm, every day or even every hour? The implications cannot be stressed enough. We expect 9.6 billion people to call Earth home by 2050. All of them need to be fed. Farming is an input-­output problem. If we can reduce the inputs—water and pesticides—and maintain the same output, we will be overcoming a central challenge.

Agricultural drones are becoming a tool like any other consumer device, and we’re starting to talk about what we can do with them. Ryan Kunde wants to irrigate less, use less pesticide, and ultimately produce better wine. More and better data can reduce water use and lower the chemical load in our environment and our food. Seen this way, what started as a military technology may end up better known as a green-tech tool, and our kids will grow up used to flying robots buzzing over farms like tiny crop dusters.

by Chris Anderson",0.05% - 0.5%,Positive,Hardware,product
Ultraprivate Smartphones,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/ultraprivate-smartphones/,"Ultraprivate Smartphones
New models built with security and privacy in mind reflect the Zeitgeist of the Snowden era.
by David Talbot archive page
April 23, 2014

NOMA BA
On January 21 a text message flashed on phones held by the protesters thronging Kiev’s Independence Square. Ukraine’s president, Viktor Yanukovych, was then still clinging to power and brutalizing opponents. The message—from the number 111—read: “Dear subscriber, you are registered as a participant in a mass disturbance.” Widely presumed to have been sent from Yanukovych’s security apparatus to all phones in the protest zone, the message was a stark reminder of how mobile phones can be used for surveillance.

Soon after, a Ukrainian man walked into a nondescript office in National Harbor, Maryland, and sought help from a man named Phil Zimmermann.

Zimmermann is a cryptologist. His company, Silent Circle, encrypts voice calls, text messages, and any file attachments. If you use Silent Circle, your calls to other users are sent through the company’s servers and decrypted on the other phone. The service won’t stop the delivery of ominous messages in range of certain base stations. But it can block eavesdropping and prevent the snooper from knowing the number of the person you are calling or texting. Soon, access codes for Silent Circle were making their way to protest organizers in the heart of Kiev. “Those are the kinds of environments where you need widespread deployment of crypto technology,” Zimmermann says, with evident satisfaction.

In the past year, it’s become clearer that places like Kiev are not the only environments where people might want the privacy Zimmermann can provide. Documents brought to light by former U.S. National Security Agency contractor Edward Snowden suggest that the NSA gathers huge amounts of information from cloud computing platforms and wireless carriers, including the numbers ordinary people called and the times they called them. Not only could the government be watching you: so could websites, advertisers, and even retailers trying to track your movements within stores. Modern smartphones and the apps running on them are engineered to collect and disseminate enormous amounts of user data—such as location, Web browsing histories, search terms, and contact lists.

By summer Zimmermann will be delivering a new way to fight back: a highly secure smartphone, called Blackphone. Now being manufactured by a joint venture that includes Silent Circle, it uses Zimmermann’s encryption tools and adds other protections. It runs a special version of the Android operating system—PrivatOS—that blocks many of the ways phones leak data about your activities. While custom security phones have long been in the hands of military and government leaders, this effort may signal a shift toward mass-market phones that are far more private and secure.

Blackphone, which sells for $629 with subscriptions to privacy-protecting services, is one of many measures that technologists are taking in response to the Snowden revelations. One such effort involves wider encryption of ordinary Web traffic. Stephen Farrell, a computer scientist at Trinity College Dublin who is leading that project through the Internet Engineering Task Force, says a phone that encrypts communications and seals off data leaks is a crucial part of the strategy. “Personally, I really would like to have a phone with a much more hardened and privacy-friendly configuration,” he says.

Crypto Warrior

Growing up in Florida, Phil Zimmermann liked breaking into places and things: his youthful conquests included Disney World and the Miami Seaquarium. He studied computer science at Florida Atlantic University, and he became interested in cryptography in the 1970s, when papers on a technology called public-key cryptography emerged. Traditional crypto required the parties in an encrypted conversation to possess the same unique decoding tool (or “key”). The new approach was fundamentally different: it involved two mathematically linked keys, one private, the other public. Suddenly, applications such as digital signatures became possible. You could use a private key to “sign” a document; later, anyone else could use the public key to verify that you were indeed the author.

Zimmermann’s fascination with this new tool dovetailed with an activist streak. In the 1980s, while laboring as a software engineer by day, he was a peace activist by night, working on the nuclear weapons freeze movement and getting arrested at the Nevada nuclear test site. (He tells of seeing actor Martin Sheen and the celebrity scientist Carl Sagan in jail.) He viewed the Reagan White House as a threat to peace and human rights as it battled socialist movements and governments. He soon started putting his interests together. “I wanted to make crypto software to protect the grassroots community, for the people of El Salvador, for human-rights groups,” he says.


Privacy software from Zimmermann is key to Blackphone. “Like Steve Jobs said, if you want to do good software you want to build the computer, too,” he says.
JOSEPH VICTOR STEFANCHIK
Ultraprivate Smartphones

Breakthrough
Mobile phones for the consumer market that transmit minimal personal information.
Why it matters
Governments and advertisers gather intimate details from cell phones.
Key players
Blackphone; CryptoPhone; Open Whisper Systems
He eventually came up with something new for applications like e-mail. Now known as PGP, for “pretty good privacy,” it built on public-key cryptography with a few new tricks, using speedier algorithms and binding things like usernames and e-mail addresses to public keys. PGP quickly became the most popular way to encrypt e-mail. It also made ­Zimmermann a combatant in the so-called crypto wars of the 1990s. At the time, the U.S. government was worried about the prospect of strong encryption technologies slipping out of the country and making it harder to snoop on other countries. So after Zimmermann published his code on the Internet in 1991, the Justice Department opened a criminal investigation. It wasn’t dropped until 1996. By then, any fears that foreign governments would use cryptography to hide their activities from the U.S. were overshadowed by the great potential the technology had for American companies in the globalized business environment that emerged after the Cold War. Businesses were opening offices and factories in cheap labor markets, “which tend to be in countries with aggressive wiretapping environments and low on human rights,” Zimmermann says. These businesses were now facing threats once faced only by human-rights and political activists. To better serve that market, Zimmermann began selling cryptography tools through a startup, PGP Inc.

Zimmermann always wanted to take widespread encryption to the next level: secure telephony. Until the past few years, however, voice transmissions did not generally take the digital form required by cryptographic technologies. In the 1990s he’d built a prototype, but it required using modems tethered to PCs. “That product was never going to get any traction,” he says. Today, telephone companies and carriers do encrypt calls—but they hold the crypto keys in their servers, and “phone companies have historically been very coöperative with wiretapping,” he says. Zimmermann’s protocols instead kept the keys only at endpoints—preventing the carriers and even his own servers from decrypting the content of a call.

These days, almost all telephony is digital—not just obvious forms like Skype, but cellular and landlines, too. So when a former U.S. Navy SEAL, Mike Janke, approached Zimmermann in 2011 with an idea for providing a service to help U.S. military members make secure calls home, he was game. They joined with Jon Callas, creator of Apple’s whole-disk encryption, to found Silent Circle. (The company originally offered e-mail, too—a service called Silent Mail. But many users were opting to store keys with Silent Mail, leaving the company vulnerable to an NSA request for data. The team killed Silent Mail and is rebuilding it so it stores the keys differently.)

No Breadcrumbs

Silent Circle had a missing piece: the hardware. “Over the years, when people asked ‘How safe can I be using your crypto software?’ I had to say, ‘We think we have some good crypto here, but the computer you are running it on might be owned by a hacker, and it won’t matter,” ­Zimmermann says. “With Blackphone we are trying to do something about that.”

Blackphone is an amalgamation of technologies. Silent Circle provides the encrypted voice and text services; the device is being made by Geeksphone, a Spanish company that specializes in phones that run open-source operating systems. Together they created ­PrivatOS, which gives more control over what data apps can see, encrypts data stored on the phone, and allows you to get wireless security updates directly from Blackphone, rather than relying on carriers. The two companies also brought on other vendors of privacy and security services. For example, one blocks tracking companies from seeing the websites you visit and the searches you make.

By February, they had one carrier lined up to sell the phone (though any buyer could use it and put a SIM card in it): Netherlands-based KPN, which also serves Belgium and Germany. They were talking to other carriers, too. It would be “the unique device that nobody has dared to make yet,” said Geeksphone’s 22-year-old founder, Javier Agüera—at least, nobody who had the average user in mind.

Fast-forward to late February. ­Zimmermann and his team sat at a sidewalk restaurant in Barcelona, munching tapas. It was the eve of Blackphone’s launch at the largest mobile trade show, Mobile World Congress. Early versions of the phone were in their pockets. As I joined the group and learned more about the phone, I became aware of my digital nakedness. I glanced at my new iPhone 5S. Opening my Wi-Fi settings, I saw available networks called Barcelona Wi-Fi, Cbarc 1, Spyder, and several others. All were of unknown trustworthiness, but I didn’t think it mattered; after all, I wasn’t connecting with any of them. But it turns out that my phone’s automatic process of seeking such signals meant it was notifying those routers of my phone’s ID number. This is already being exploited by retailers, who use Wi-Fi probes to track customers’ habits. And because information from apps is merged with data from Web browsers, shopping sites, and other sources, dozens of companies can use that ID number to keep tabs on me.

Mike Kershaw, Blackphone’s chief architect, came over to my side of the table. He proudly explained how Blackphone would prevent any such thing. Software Kershaw developed programs the phone not to search for Wi-Fi signals unless it is in a predefined geographical area, such as one around your home or office. So as we ate tapas, I was the only person at the table leaving digital breadcrumbs. The others had tools to prevent browsing history and search terms from being tied to their identity; I didn’t. They had fine-grained control over app permissions; I didn’t.

The next morning, near a modest booth at Mobile World Congress, some 200 journalists and analysts crowded the hallways for Blackphone’s launch announcement. “We are not a phone company adding a privacy feature,” ­Zimmermann said. “We are a privacy company selling a phone.” But it was already clear that this was a kind of anti-phone, going against the grain of the mainstream smartphone industry. Later that day, ­Zimmermann walked by Samsung’s enormous installation. It was bristling with Galaxy 5 phones, loaded with Android configured largely the way Google likes it: to gather data. “They’ve got a pretty big booth,” Zimmermann deadpanned.

Not NSA-Proof

Top security experts are reserving judgment on Blackphone until they can test the phone. It won’t ship until June. But the underlying encryption Silent Circle uses—and the evident paranoia of its creators—is widely admired. “I very much like Silent Circle’s solutions,” says Bruce Schneier, a cryptologist who has been calling for more security in communication technologies and wider use of encryption.

While the phone is resistant to everyday threats like hacking and snooping by data brokers, even the company concedes that it’s not NSA-proof, and it could have an Achilles’ heel: the apps that its users will inevitably download. Xuxian Jiang, a computer scientist at North Carolina State University and an authority on Android security, says that’s how devices acquire many of their vulnerabilities. Blackphone also doesn’t protect e-mail on its own; whether your e-mail uses encryption technology such as PGP depends on your e-mail provider. Still, Jiang says of the phone: “These are certainly good privacy improvements.”

There are a few competing efforts. Open Whisper Systems has released an encryption system for Android calls. Nonetheless, Blackphone is already establishing itself: by March, Zimmermann says, hundreds of thousands of units had been ordered. The company expects to sell millions of phones in the first two years. In many ways, the NSA revelations, the growing awareness of how consumers are being tracked by commercial interests, and conflicts like the one in Ukraine have been the best possible advertising. “It used to be an uphill battle to make people believe there was a need for this kind of technology,” ­Zimmermann says. “Not anymore.”

by David Talbot",0.05-0.5%,"The article discusses the launch of Blackphone, a new smartphone designed with privacy and security in mind. The phone is developed by Silent Circle, a company co-founded by Phil Zimmermann, the creator of Pretty Good Privacy (PGP), an encryption technology used to secure email communications.

Blackphone's operating system, PrivatOS, gives users more control over their data and provides end-to-end encryption for voice and text communications. The phone also comes with built-in security features such as secure browsing, encrypted file storage, and secure messaging apps.

The article highlights the growing concern among consumers about online privacy and security, particularly in light of revelations about mass surveillance by government agencies and commercial interests. Zimmermann notes that it's no longer an uphill battle to convince people of the need for secure technology.

However, top security experts are reserving judgment on Blackphone until they can test the phone, which won't ship until June. They note that while the phone may be resistant to everyday threats like hacking and snooping, it may not be completely NSA-proof, particularly if users download vulnerable apps or use email services without encryption.

Despite these concerns, Blackphone is generating significant interest, with hundreds of thousands of units ordered by March. The company expects to sell millions of phones in the first two years.

The article also mentions other competing efforts to provide secure communication solutions, such as Open Whisper Systems' encryption system for Android calls.

Overall, the launch of Blackphone represents a new direction in smartphone design, one that prioritizes user privacy and security over commercial interests.",Hardware,"The article discusses the launch of Blackphone, a new smartphone designed with privacy and security in mind. The phone is the result of a collaboration between Silent Circle, a company founded by Phil Zimmermann, the creator of PGP encryption, and Geeksphone, a Spanish manufacturer of open-source smartphones.

Blackphone's operating system, PrivatOS, is designed to give users more control over their data and protect them from tracking and snooping. The phone comes with a range of security features, including encrypted voice and text services, secure browsing, and fine-grained control over app permissions.

The article highlights the growing concern about online privacy and security in the wake of revelations about government surveillance and commercial data collection. Blackphone's creators aim to provide a device that is resistant to everyday threats like hacking and snooping, but they concede that it may not be entirely NSA-proof.

Top security experts have expressed admiration for Silent Circle's encryption solutions, but some have reserved judgment on Blackphone until they can test the phone. The article notes that the phone's Achilles' heel may be the apps that users download, which could potentially introduce vulnerabilities.

Despite these limitations, Blackphone has generated significant interest, with hundreds of thousands of units ordered by March and expectations of selling millions in the first two years. The company's founders believe that the growing awareness of online privacy and security concerns has created a market for their product.

Overall, the article suggests that Blackphone represents a new approach to smartphone design, one that prioritizes user control and security over the convenience and data collection that are typical of mainstream smartphones."
Brain Mapping,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/brain-mapping/,"Brain Mapping
A new map, a decade in the works, shows structures of the brain in far greater detail than ever before, providing neuroscientists with a guide to its immense complexity.
by Courtney Humphries archive page
April 23, 2014

Neuroscientists have made remarkable progress in recent years toward understanding how the brain works. And in coming years, Europe’s Human Brain Project will attempt to create a computational simulation of the human brain, while the U.S. BRAIN Initiative will try to create a wide-ranging picture of brain activity. These ambitious projects will greatly benefit from a new resource: detailed and comprehensive maps of the brain’s structure and its different regions.

As part of the Human Brain Project, an international team of researchers led by German and Canadian scientists has produced a three-dimensional atlas of the brain that has 50 times the resolution of previous such maps. The atlas, which took a decade to complete, required slicing a brain into thousands of thin sections and digitally stitching them back together with the help of supercomputers. Able to show details as small as 20 micrometers, roughly the size of many human cells, it is a major step forward in understanding the brain’s three-dimensional anatomy.


A section of the human brain map created by a team of international researchers shows details as small as 20 micrometers.
BRAIN IMAGING CENTER/MONTREAL NEUROLOGICAL INSTITUTE
Brain Mapping

Breakthrough
A high-resolution map that shows structures of the human brain as small as 20 micrometers.
Why it matters
As neuroscientists try to understand how the brain works, they need a detailed map of its anatomy.
Key players
Katrin Amunts, Jülich Research Centre; Alan Evans, Montreal Neurological Institute; Karl Deisseroth, Stanford University
To guide the brain’s digital reconstruction, researchers led by Katrin Amunts at the Jülich Research Centre in Germany initially used an MRI machine to image the postmortem brain of a 65-year-old woman. The brain was then cut into ultrathin slices. The scientists stained the sections and then imaged them one by one on a flatbed scanner. Alan Evans and his coworkers at the Montreal Neurological Institute organized the 7,404 resulting images into a data set about a terabyte in size. Slicing had bent, ripped, and torn the tissue, so Evans had to correct these defects in the images. He also aligned each one to its original position in the brain. The result is mesmerizing: a brain model that you can swim through, zooming in or out to see the arrangement of cells and tissues.

At the start of the 20th century, a German neuroanatomist named Korbinian Brodmann parceled the human cortex into nearly 50 different areas by looking at the structure and organization of sections of brain under a microscope. “That has been pretty much the reference framework that we’ve used for 100 years,” Evans says. Now he and his coworkers are redoing ­Brodmann’s work as they map the borders between brain regions. The result may show something more like 100 to 200 distinct areas, providing scientists with a far more accurate road map for studying the brain’s different functions.

“We would like to have in the future a reference brain that shows true cellular resolution,” says Amunts—about one or two micrometers, as opposed to 20. That’s a daunting goal, for several reasons. One is computational: Evans says such a map of the brain might contain several petabytes of data, which computers today can’t easily navigate in real time, though he’s optimistic that they will be able to in the future. Another problem is physical: a brain can be sliced only so thin.

Advances could come from new techniques that allow scientists to see the arrangement of cells and nerve fibers inside intact brain tissue at very high resolution. Amunts is developing one such technique, which uses polarized light to reconstruct three-­dimensional structures of nerve fibers in brain tissue. And a technique called Clarity, developed in the lab of Karl Deisseroth, a neuroscientist and bioengineer at Stanford University, allows scientists to directly see the structures of neurons and circuitry in an intact brain. The brain, like any other tissue, is usually opaque because the fats in its cells block light. Clarity melts the lipids away, replacing them with a gel-like substance that leaves other structures intact and visible. Though Clarity can be used on a whole mouse brain, the human brain is too big to be studied fully intact with the existing version of the technology. But Deisseroth says the technique can already be used on blocks of human brain tissue thousands of times larger than a thin brain section, making 3-D reconstruction easier and less error prone. And Evans says that while Clarity and polarized-light imaging currently give fantastic resolution to pieces of brain, “in the future we hope that this can be expanded to include a whole human brain.”

by Courtney Humphries",0.05% - 0.5%,positive,Biotech,project
Neuromorphic Chips,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/neuromorphic-chips/,"Neuromorphic Chips
Microprocessors configured more like brains than traditional chips could soon make computers far more astute about what’s going on around them.
by Robert D. Hof archive page
April 23, 2014

NOMA BAR
A pug-size robot named pioneer slowly rolls up to the Captain America action figure on the carpet. They’re facing off inside a rough model of a child’s bedroom that the wireless-chip maker Qualcomm has set up in a trailer. The robot pauses, almost as if it is evaluating the situation, and then corrals the figure with a snowplow-like implement mounted in front, turns around, and pushes it toward three squat pillars representing toy bins. Qualcomm senior engineer Ilwoo Chang sweeps both arms toward the pillar where the toy should be deposited. Pioneer spots that gesture with its camera and dutifully complies. Then it rolls back and spies another action figure, Spider-Man. This time Pioneer beelines for the toy, ignoring a chessboard nearby, and delivers it to the same pillar with no human guidance.

This demonstration at Qualcomm’s headquarters in San Diego looks modest, but it’s a glimpse of the future of computing. The robot is performing tasks that have typically needed powerful, specially programmed computers that use far more electricity. Powered by only a smartphone chip with specialized software, Pioneer can recognize objects it hasn’t seen before, sort them by their similarity to related objects, and navigate the room to deliver them to the right location—not because of laborious programming but merely by being shown once where they should go. The robot can do all that because it is simulating, albeit in a very limited fashion, the way a brain works.

Later this year, Qualcomm will begin to reveal how the technology can be embedded into the silicon chips that power every manner of electronic device. These “neuromorphic” chips—so named because they are modeled on biological brains—will be designed to process sensory data such as images and sound and to respond to changes in that data in ways not specifically programmed. They promise to accelerate decades of fitful progress in artificial intelligence and lead to machines that are able to understand and interact with the world in humanlike ways. Medical sensors and devices could track individuals’ vital signs and response to treatments over time, learning to adjust dosages or even catch problems early. Your smartphone could learn to anticipate what you want next, such as background on someone you’re about to meet or an alert that it’s time to leave for your next meeting. Those self-driving cars Google is experimenting with might not need your help at all, and more adept Roombas wouldn’t get stuck under your couch. “We’re blurring the boundary between silicon and biological systems,” says Qualcomm’s chief technology officer, Matthew Grob.

Qualcomm’s chips won’t become available until next year at the earliest; the company will spend 2014 signing up researchers to try out the technology. But if it delivers, the project—known as the Zeroth program—would be the first large-scale commercial platform for neuromorphic computing. That’s on top of promising efforts at universities and at corporate labs such as IBM Research and HRL Laboratories, which have each developed neuromorphic chips under a $100 million project for the Defense Advanced Research Projects Agency. Likewise, the Human Brain Project in Europe is spending roughly 100 million euros on neuromorphic projects, including efforts at Heidelberg University and the University of Manchester. Another group in Germany recently reported using a neuromorphic chip and software modeled on insects’ odor-processing systems to recognize plant species by their flowers.

Today’s computers all use the so-called von Neumann architecture, which shuttles data back and forth between a central processor and memory chips in linear sequences of calculations. That method is great for crunching numbers and executing precisely written programs, but not for processing images or sound and making sense of it all. It’s telling that in 2012, when Google demonstrated artificial-­intelligence software that learned to recognize cats in videos without being told what a cat was, it needed 16,000 processors to pull it off.

Continuing to improve the performance of such processors requires their manufacturers to pack in ever more, ever faster transistors, silicon memory caches, and data pathways, but the sheer heat generated by all those components is limiting how fast chips can be operated, especially in power-stingy mobile devices. That could halt progress toward devices that effectively process images, sound, and other sensory information and then apply it to tasks such as face recognition and robot or vehicle navigation.

No one is more acutely interested in getting around those physical challenges than Qualcomm, maker of wireless chips used in many phones and tablets. Increasingly, users of mobile devices are demanding more from these machines. But today’s personal-assistant services, such as Apple’s Siri and Google Now, are limited because they must call out to the cloud for more powerful computers to answer or anticipate queries. “We’re running up against walls,” says Jeff Gehlhaar, the Qualcomm vice president of technology who heads the Zeroth engineering team.

Neuromorphic chips attempt to model in silicon the massively parallel way the brain processes information as billions of neurons and trillions of synapses respond to sensory inputs such as visual and auditory stimuli. Those neurons also change how they connect with each other in response to changing images, sounds, and the like. That is the process we call learning. The chips, which incorporate brain-inspired models called neural networks, do the same. That’s why Qualcomm’s robot—even though for now it’s merely running software that simulates a neuromorphic chip—can put Spider-Man in the same location as Captain America without having seen Spider-Man before.

Even if neuromorphic chips are nowhere near as capable as the brain, they should be much faster than current computers at processing sensory data and learning from it. Trying to emulate the brain just by using special software on conventional processors—the way Google did in its cat experiment—is way too inefficient to be the basis of machines with still greater intelligence, says Jeff Hawkins, a leading thinker on AI who created the Palm Pilot before cofounding Numenta, a maker of brain-inspired software. “There’s no way you can build it [only] in software,” he says of effective AI. “You have to build this in silicon.”

Qualcomm could add a “neural processing unit” to mobile-phone chips to handle sensory data and tasks such as image recognition.
Neural Channel

As smartphones have taken off, so has Qualcomm, whose market capitalization now tops Intel’s. That’s thanks in part to the hundreds of wireless-­communications patents that Qualcomm shows off on two levels of a seven-story atrium lobby at its San Diego headquarters. Now it’s looking to break new ground again. First in coöperation with Brain Corp., a neuroscience startup it invested in and that is housed at its headquarters, and more recently with its own growing staff, it has been quietly working for the past five years on algorithms to mimic brain functions as well as hardware to execute them. The Zeroth project has initially focused on robotics applications because the way robots can interact with the real world provides broader lessons about how the brain learns—lessons that can then be applied in smartphones and other products. Its name comes from Isaac Asimov’s “Zeroth Law” of robotics: “A robot may not harm humanity, or, by inaction, allow humanity to come to harm.”

The idea of neuromorphic chips dates back decades. Carver Mead, the Caltech professor emeritus who is a legend in integrated-circuit design, coined the term in a 1990 paper, describing how analog chips—those that vary in their output, like real-world phenomena, in contrast to the binary, on-or-off nature of digital chips—could mimic the electrical activity of neurons and synapses in the brain. But he struggled to find ways to reliably build his analog chip designs. Only one arguably neuromorphic processor, a noise suppression chip made by Audience, has sold in the hundreds of millions. The chip, which is based on the human cochlea, has been used in phones from Apple, Samsung, and others.


As a commercial company, Qualcomm has opted for pragmatism over sheer performance in its design. That means the neuromorphic chips it’s developing are still digital chips, which are more predictable and easier to manufacture than analog ones. And instead of modeling the chips as closely as possible on actual brain biology, Qualcomm’s project emulates aspects of the brain’s behavior. For instance, the chips encode and transmit data in a way that mimics the electrical spikes generated in the brain as it responds to sensory information. “Even with this digital representation, we can reproduce a huge range of behaviors we see in biology,” says M. Anthony Lewis, the project engineer for Zeroth.

The chips would fit neatly into the existing business of Qualcomm, which dominates the market for mobile-phone chips but has seen revenue growth slow. Its Snapdragon mobile-phone chips include components such as graphics processing units; Qualcomm could add a “neural processing unit” to the chips to handle sensory data and tasks such as image recognition and robot navigation. And given that Qualcomm has a highly profitable business of licensing technologies to other companies, it would be in a position to sell the rights to use algorithms that run on neuromorphic chips. That could lead to sensor chips for vision, motion control, and other applications.

Cognitive Companion

Matthew Grob was startled, then annoyed, when he heard the theme to Sanford and Son start playing in the middle of a recent meeting. It turns out that on a recent trip to Spain, he had set his smartphone to issue a reminder using the tune as an alarm, and the phone thought it was time to play it again. That’s just one small example of how far our personal devices are from being intelligent. Grob dreams of a future when instead of monkeying with the settings of his misbehaving phone, as he did that day, all he would have to do is bark, “Don’t do that!” Then the phone might learn that it should switch off the alarm when he’s in a new time zone.

Qualcomm is especially interested in the possibility that neuromorphic chips could transform smartphones and other mobile devices into cognitive companions that pay attention to your actions and surroundings and learn your habits over time. “If you and your device can perceive the environment in the same way, your device will be better able to understand your intentions and anticipate your needs,” says Samir Kumar, a business development director at Qualcomm’s research lab.

Pressed for examples, Kumar ticks off a litany: If you tag your dog in a photo, your phone’s camera would recognize the pet in every subsequent photo. At a soccer game, you could tell the phone to snap a photo only when your child is near the goal. At bedtime, it would know without your telling it to send calls to voice mail. In short, says Grob, your smartphone would have a digital sixth sense.

Qualcomm executives are reluctant to embark on too many flights of fancy before their chip is even available. But neuromorphic researchers elsewhere don’t mind speculating. According to ­Dharmendra Modha, a top IBM researcher in San Jose, such chips might lead to glasses for the blind that use visual and auditory sensors to recognize objects and provide audio cues; health-care systems that monitor vital signs, provide early warnings of potential problems, and suggest ways to individualize treatments; and computers that draw on wind patterns, tides, and other indicators to predict tsunamis more accurately. At HRL this summer, principal research scientist Narayan Srinivasa plans to test a neuromorphic chip in a bird-size device from AeroVironment that will be flown around a couple of rooms. It will take in data from cameras and other sensors so it can remember which room it’s in and learn to navigate that space more adeptly, which could lead to more capable drones.

It will take programmers time to figure out the best way to exploit the hardware. “It’s not too early for hardware companies to do research,” says Dileep George, cofounder of the artificial-­intelligence startup Vicarious. “The commercial products could take a while.” Qualcomm executives don’t disagree. But they’re betting that the technology they expect to launch this year will bring those products a lot closer to reality.

by Robert D. Hof",0.01% - 0.1%,"The article discusses the development of neuromorphic chips, which are designed to mimic the behavior of neurons and synapses in the brain. Qualcomm, a leading mobile chip manufacturer, is working on developing such chips, which could potentially revolutionize the way we interact with our devices.

Neuromorphic chips have been around for decades, but previous attempts to build them were plagued by reliability issues. However, recent advances in digital technology have made it possible to create more reliable and efficient neuromorphic chips.

Qualcomm's approach is to develop digital chips that emulate aspects of brain behavior, rather than trying to model the brain exactly. This allows for more predictable and easier-to-manufacture designs.

The potential applications of neuromorphic chips are vast, including:

* Creating smartphones that can learn a user's habits and anticipate their needs
* Developing devices that can recognize objects and provide audio cues for the blind
* Improving healthcare systems by monitoring vital signs and providing early warnings of potential problems
* Enhancing computer systems to predict natural disasters more accurately

Qualcomm expects to launch its neuromorphic chip this year, but it may take time for programmers to figure out how to exploit the hardware. However, the company is betting that the technology will bring commercial products closer to reality.

Some examples of how neuromorphic chips could be used in everyday life include:

* A smartphone camera that can recognize a user's dog and tag them in photos
* A phone that can snap a photo only when a child is near the goal during a soccer game
* A device that can learn a user's bedtime routine and automatically send calls to voicemail

Overall, neuromorphic chips have the potential to revolutionize the way we interact with technology and could lead to significant advances in fields such as healthcare, education, and transportation.",Hardware,"The article discusses the development of neuromorphic chips, which are designed to mimic the behavior of neurons and synapses in the brain. These chips have the potential to revolutionize the way we interact with devices, making them more intelligent and able to learn from their environment.

Qualcomm, a leading manufacturer of mobile phone chips, is at the forefront of this technology. The company's neuromorphic chip project, called Zeroth, aims to develop a digital chip that can emulate aspects of brain behavior. While not as complex as analog chips, which are more difficult to manufacture and predict, Qualcomm's digital approach has shown promising results.

The article highlights several potential applications for neuromorphic chips, including:

1. Cognitive companions: Devices that can learn from their environment and adapt to an individual's habits and needs.
2. Image recognition: Chips that can quickly and accurately recognize objects in images.
3. Robot navigation: Devices that can navigate through complex environments using sensory data.
4. Health-care systems: Systems that can monitor vital signs, provide early warnings of potential problems, and suggest personalized treatments.

Other researchers and companies are also exploring the possibilities of neuromorphic chips. For example, IBM is working on glasses for the blind that use visual and auditory sensors to recognize objects and provide audio cues.

While the technology is still in its early stages, Qualcomm executives believe that their neuromorphic chip will be available this year. They predict that it will take programmers time to figure out the best way to exploit the hardware, but they are confident that the technology has the potential to revolutionize the way we interact with devices.

Key players mentioned in the article include:

1. Carver Mead: A professor emeritus at Caltech who coined the term ""neuromorphic"" in a 1990 paper.
2. Matthew Grob: A Qualcomm executive who envisions a future where devices can learn from their environment and adapt to an individual's habits and needs.
3. Samir Kumar: A business development director at Qualcomm's research lab who highlights the potential of neuromorphic chips for cognitive companions.
4. Dharmendra Modha: An IBM researcher who is working on applications for neuromorphic chips, including glasses for the blind and health-care systems.

Overall, the article suggests that neuromorphic chips have the potential to revolutionize the way we interact with devices, making them more intelligent and able to learn from their environment. While the technology is still in its early stages, companies like Qualcomm are pushing forward with development, and we can expect to see significant advancements in the coming years."
Genome Editing,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/genome-editing/,"Genome Editing
The ability to create primates with intentional mutations could provide powerful new ways to study complex and genetically baffling brain disorders.
by Christina Larson archive pageAmanda Schaffer archive page
April 23, 2014
Above: The genomes of these twin infant macaques were modified with multiple mutations.
Above: The genomes of these twin infant macaques were modified with multiple mutations.
YUNNAN KEY LABORATORY OF PRIMATE BIOMEDICAL RESEARCH
The Experiment

By Christina Larson

Until recently, Kunming, capital of China’s southwestern Yunnan province, was known mostly for its palm trees, its blue skies, its laid-back vibe, and a steady stream of foreign backpackers bound for nearby mountains and scenic gorges. But Kunming’s reputation as a provincial backwater is rapidly changing. On a plot of land on the outskirts of the city—wilderness 10 years ago, and today home to a genomic research facility—scientists have performed a provocative experiment. They have created a pair of macaque monkeys with precise genetic mutations.

Last November, the female monkey twins, Mingming and Lingling, were born here on the sprawling research campus of Kunming Biomedical International and its affiliated Yunnan Key Laboratory of Primate Biomedical Research. The macaques had been conceived via in vitro fertilization. Then scientists used a new method of DNA engineering known as CRISPR to modify the fertilized eggs by editing three different genes, and they were implanted into a surrogate macaque mother. The twins’ healthy birth marked the first time that CRISPR has been used to make targeted genetic modifications in primates—potentially heralding a new era of biomedicine in which complex diseases can be modeled and studied in monkeys.

CRISPR, which was developed by researchers at the University of California, Berkeley, Harvard, MIT, and elsewhere over the last several years, is already transforming how scientists think about genetic engineering, because it allows them to make changes to the genome precisely and relatively easily (see “Genome Surgery,” March/April). The goal of the experiment at Kunming is to confirm that the technology can create primates with multiple mutations, explains Weizhi Ji, one of the architects of the experiment.

Ji began his career at the government-affiliated Kunming Institute of Zoology in 1982, focusing on primate reproduction. China was “a very poor country” back then, he recalls. “We did not have enough funding for research. We just did very simple work, such as studying how to improve primate nutrition.” China’s science ambitions have since changed dramatically. The campus in Kunming boasts extensive housing for monkeys: 75 covered homes, sheltering more than 4,000 primates—many of them energetically swinging on hanging ladders and scampering up and down wire mesh walls. Sixty trained animal keepers in blue scrubs tend to them full time.

The lab where the experiment was performed includes microinjection systems, which are microscopes pointed at a petri dish and two precision needles, controlled by levers and dials. These are used both for injecting sperm into eggs and for the gene editing, which uses “guide” RNAs that direct a DNA-cutting enzyme to genes. When I visited, a young lab technician was intently focused on twisting dials to line up sperm with an egg. Injecting each sperm takes only a few seconds. About nine hours later, when an embryo is still in the one-cell stage, a technician will use the same machine to inject it with the CRISPR molecular components; again, the procedure takes just a few seconds.

During my visit in late February, the twin macaques were still only a few months old and lived in incubators, monitored closely by lab staff. Indeed, Ji and his coworkers plan to continue to closely watch the monkeys to detect any consequences of the pioneering genetic modifications.


JOHN MACNEILL; SOURCE: CELL
Genome Editing

Breakthrough
The use of a genome-tool to create two monkeys with specific genetic mutations.
Why it matters
The ability to modify targeted genes in primates is a valuable tool in the study of human diseases.
Key players
Yunnan Key Laboratory; Jennifer Doudna, UC Berkeley; Feng Zhang, MIT; George Church, Harvard
The Impact

By Amanda Schaffer

The new genome-editing tool called CRISPR, which researchers in China used to genetically modify monkeys, is a precise and relatively easy way to alter DNA at specific locations on chromosomes. In early 2013, U.S. scientists showed it could be used to genetically engineer any type of animal cells, including human ones, in a petri dish. But the Chinese researchers were the first to demonstrate that this approach can be used in primates to create offspring with specific genetic alterations.

“The idea that we can modify primates easily with this technology is powerful,” says Jennifer Doudna, a professor of molecular and cell biology at the University of California, Berkeley, and a developer of CRISPR. The creation of primates with intentional gene alterations could lead to powerful new ways to study complex human diseases. It also poses new ethical dilemmas. From a technical perspective, the Chinese primate research suggests that scientists could probably alter fertilized human eggs with CRISPR; if monkeys are any guide, such eggs could grow to be genetically modified babies. But “whether that would be a good idea is a much harder question,” says Doudna.

The prospect of designer babies remains remote and far from the minds of most researchers developing CRISPR. Far more imminent are the potential opportunities to create animals with mutations linked to human disorders. Experimenting with primates is expensive and can raise concerns about animal welfare, says Doudna. But the demonstration that CRISPR works in monkeys has gotten “a lot of people thinking about cases where primate models may be important.”

At the top of that list is the study of brain disorders. Robert Desimone, director of MIT’s McGovern Institute for Brain Research, says that there is “quite a bit of interest” in using CRISPR to generate monkey models of diseases like autism, schizophrenia, Alzheimer’s disease, and bipolar disorder. These disorders are difficult to study in mice and other rodents; not only do the affected behaviors differ substantially between these animals and humans, but the neural circuits involved in the disorders can be different. Many experimental psychiatric drugs that appeared to work well in mice have not proved successful in human trials. As a result of such failures, many pharmaceutical companies have scaled back or abandoned their efforts to develop treatments.

Primate models could be especially helpful to researchers trying to make sense of the growing number of mutations that genetic studies have linked to brain disorders. The significance of a specific genetic variant is often unclear; it could be a cause of a disorder, or it could just be indirectly associated with the disease. CRISPR could help researchers tease out the mutations that actually cause the disorders: they would be able to systematically introduce the suspected genetic variants into monkeys and observe the results. CRISPR is also useful because it allows scientists to create animals with different combinations of mutations, in order to assess which ones—or which combinations of them—matter most in causing disease. This complex level of manipulation is nearly impossible with other methods.

Guoping Feng, a professor of neuroscience at MIT, and Feng Zhang, a colleague at the Broad Institute and McGovern Brain Institute who showed that CRISPR could be used to modify the genomes of human cells, are working with Chinese researchers to create macaques with a version of autism. They plan to mutate a gene called SHANK3 in fertilized eggs, producing monkeys that can be used to study the basic science of the disorder and test possible drug treatments. (Only a small percentage of people with autism have the SHANK3 mutation, but it is one of the few genetic variants that lead to a high probability of the disorder.)

The Chinese researchers responsible for the birth of the genetically engineered monkeys are still focusing on developing the technology, says Weizhi Ji, who helped lead the effort at the Yunnan Key Laboratory of Primate Biomedical Research in Kunming. However, his group hopes to create monkeys with Parkinson’s, among other brain disorders. The aim would be to look for early signs of the disease and study the mechanisms that allow it to progress.

The most dramatic possibility raised by the primate work, of course, would be using CRISPR to change the genetic makeup of human embryos during in vitro fertilization. But while such manipulation should be technically possible, most scientists do not seem eager to pursue it.

Indeed, the safety concerns would be daunting. When you think about “messing with a single cell that is potentially going to become a living baby,” even small errors or side effects could turn out to have enormous consequences, says Hank Greely, director of the Center for Law and the Biosciences at Stanford. And why even bother? For most diseases with simple genetic causes, it wouldn’t be worthwhile to use CRISPR; it would make more sense for couples to “choose a different embryo that doesn’t have the disease,” he says. This is already possible as part of in vitro fertilization, using a procedure called preimplantation genetic diagnosis.

It’s possible to speculate that parents might wish to alter multiple genes in order to reduce children’s risk, say, of heart disease or diabetes, which have complex genetic components. But for at least the next five to 10 years, that, says Greely, “just strikes me as borderline crazy, borderline implausible.” Many, if not most, of the traits that future parents might hope to alter in their kids may also be too complex or poorly understood to make reasonable targets for intervention. Scientists don’t understand the genetic basis, for instance, of intelligence or other higher-order brain functions—and that is unlikely to change for a long time.

Ji says creating humans with CRISPR-edited genomes is “very possible,” but he concurs that “considering the safety issue, there would still be a long way to go.” In the meantime, his team hopes to use genetically modified monkeys to “establish very efficient animal models for human diseases, to improve human health in the future.”

by Christina Larson & Amanda Schaffer",0.01% - 0.1%,"The article discusses the potential of using CRISPR gene editing technology to create genetically modified primates, such as monkeys and apes. Researchers have already successfully used CRISPR to edit the genes of monkeys, creating animals with specific genetic mutations that can be used to study human diseases.

One of the main goals of this research is to create animal models of brain disorders, such as autism, schizophrenia, Alzheimer's disease, and bipolar disorder. These disorders are difficult to study in mice and other rodents because they do not exhibit the same behaviors or neural circuits as humans. By creating primate models with specific genetic mutations, researchers hope to gain a better understanding of the underlying biology of these disorders and develop more effective treatments.

The article highlights several potential benefits of using CRISPR-edited primates for research:

1. **Improved animal models**: Primates are closer to humans in terms of evolution and physiology, making them more relevant models for studying human diseases.
2. **Reduced reliance on rodents**: Rodents have limitations as disease models, and primates can provide a more accurate representation of human biology.
3. **Enhanced understanding of brain disorders**: By creating primate models with specific genetic mutations, researchers can gain insights into the underlying mechanisms of brain disorders.

However, the article also notes that there are significant challenges and concerns associated with using CRISPR-edited primates for research:

1. **Safety concerns**: The use of CRISPR in humans raises concerns about unintended consequences, such as off-target effects or mosaicism.
2. **Ethical considerations**: The creation of genetically modified primates raises ethical questions about animal welfare and the potential misuse of this technology.

The article concludes that while the use of CRISPR-edited primates for research is a promising area of study, it is essential to carefully consider the benefits and risks associated with this technology.",Biotech,"This article discusses the recent breakthroughs and potential applications of CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) technology in genetically modifying primates. Here are some key points:

**Breakthrough in genetic modification of monkeys**: Chinese researchers have successfully used CRISPR to edit the genes of monkeys, creating a new generation of animals that can be used for medical research.

**Potential applications**: The most significant potential application is the creation of animal models for brain disorders such as autism, schizophrenia, Alzheimer's disease, and bipolar disorder. Monkeys are more closely related to humans than mice or other rodents, making them better suited for studying these diseases.

**Benefits over traditional methods**: CRISPR allows researchers to systematically introduce specific genetic variants into monkeys and observe the results, which can help identify the causes of brain disorders. This level of manipulation is difficult or impossible with other methods.

**Current research**: Researchers at MIT's McGovern Institute for Brain Research are using CRISPR to create macaques with a version of autism by mutating the SHANK3 gene. Another group in China plans to create monkeys with Parkinson's disease to study its progression.

**Ethical concerns and limitations**: The article highlights the potential risks and uncertainties associated with using CRISPR in human embryos, including safety concerns and the complexity of genetic traits such as intelligence. While technically possible, most scientists are not eager to pursue this application due to these concerns.

**Current focus on animal models**: Researchers agree that the immediate focus should be on creating genetically modified monkeys for medical research, which can help improve human health in the future.

Overall, the article suggests that CRISPR has significant potential in advancing our understanding of brain disorders and developing new treatments. However, it also emphasizes the need for caution and careful consideration when exploring its applications in humans."
Microscale 3-D Printing,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/microscale-3-d-printing/,"Microscale 3-D Printing
Inks made from different types of materials, precisely applied, are greatly expanding the kinds of things that can be printed.
by David Rotman archive page
April 23, 2014

Despite the excitement that 3-D printing has generated, its capabilities remain rather limited. It can be used to make complex shapes, but most commonly only out of plastics. Even manufacturers using an advanced version of the technology known as additive manufacturing typically have expanded the material palette only to a few types of metal alloys. But what if 3-D printers could use a wide assortment of different materials, from living cells to semiconductors, mixing and matching the “inks” with precision?


To show off its ability to do multimaterial 3-D printing, Lewis’s lab has printed a complex lattice using different inks.
Microscale 3-D Printing

Breakthrough
3-D printing that uses multiple materials to create objects such as biological tissue with blood vessels.
Why it matters
Making biological materials with desired functions could lead to artificial organs and novel cyborg parts.
Key players
Jennifer Lewis, Harvard University; Michael McAlpine, Princeton University; Keith Martin, University of Cambridge
Jennifer Lewis, a materials scientist at Harvard University, is developing the chemistry and machines to make that possible. She prints intricately shaped objects from “the ground up,” precisely adding materials that are useful for their mechanical properties, electrical conductivity, or optical traits. This means 3-D printing technology could make objects that sense and respond to their environment. “Integrating form and function,” she says, “is the next big thing that needs to happen in 3-D printing.”


Left: For the demonstration, the group formulated four polymer inks, each dyed a different color.

Right: The different inks are placed in standard print heads.

Bottom: By sequentially and precisely depositing the inks in a process guided by the group’s software, the printer quickly produces the colorful lattice.

A group at Princeton University has printed a bionic ear, combining biological tissue and electronics (see “Cyborg Parts”), while a team of researchers at the University of Cambridge has printed retinal cells to form complex eye tissue. But even among these impressive efforts to extend the possibilities of 3-D printing, Lewis’s lab stands out for the range of materials and types of objects it can print.

Last year, Lewis and her students showed they could print the microscopic electrodes and other components needed for tiny lithium-ion batteries (see “Printing Batteries”). Other projects include printed sensors fabricated on plastic patches that athletes could one day wear to detect concussions and measure violent impacts. Most recently, her group printed biological tissue interwoven with a complex network of blood vessels. To do this, the researchers had to make inks out of various types of cells and the materials that form the matrix supporting them. The work addresses one of the lingering challenges in creating artificial organs for drug testing or, someday, for use as replacement parts: how to create a vascular system to keep the cells alive.


Top: Inks made of silver nanoparticles are used to print electrodes as small as a few micrometers.  

Bottom: As in the other 3-D printing processes, the operation is controlled and monitored by computers.

Left: Jennifer Lewis’s goal is to print complex architectures that integrate form and function.

Right: A glove with strain sensors is made by printing electronics into a stretchable elastomer.
In a basement lab a few hundred yards from Lewis’s office, her group has jury-rigged a 3-D printer, equipped with a microscope, that can precisely print structures with features as small as one micrometer (a human red blood cell is around 10 micrometers in diameter). Another, larger 3-D printer, using printing nozzles with multiple outlets to print multiple inks simultaneously, can fabricate a meter-sized sample with a desired microstructure in minutes.

The secret to Lewis’s creations lies in inks with properties that allow them to be printed during the same fabrication process. Each ink is a different material, but they all can be printed at room temperature. The various types of materials present different challenges; cells, for example, are delicate and easily destroyed as they are forced through the printing nozzle. In all cases, though, the inks must be formulated to flow out of the nozzle under pressure but retain their form once in place—think of toothpaste, Lewis says.


Left: The ­largest printer in Lewis’s lab makes objects up to a meter by a meter.

Top: For such jobs, the printer uses a 64- or 128-­nozzle array to speed up the process.

Bottom: A test sample with a layered microstructure was printed in minutes using wax ink.
Before coming to Harvard from the University of Illinois at Urbana-­Champaign last year, Lewis had spent more than a decade developing 3-D printing techniques using ceramics, metal nanoparticles, polymers, and other nonbiological materials. When she set up her new lab at Harvard and began working with biological cells and tissues for the first time, she hoped to treat them the same way as materials composed of synthetic particles. That idea might have been a bit naïve, she now acknowledges. Printing blood vessels was an encouraging step toward artificial tissues capable of the complex biological functions found in organs. But working with the cells turns out to be “really complex,” she says. “And there’s a lot more that we need to do before we can print a fully functional liver or kidney. But we’ve taken the first step.”

by David Rotman",0.01% - 0.1%,Positive,Hardware,methodology
Mobile Collaboration,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/mobile-collaboration/,"Mobile Collaboration
The smartphone era is finally getting the productivity software it needs.
by Ted Greenwald archive page
April 23, 2014

NOMA BAR
Mobile Collaboration

Breakthrough
Services that make it fruitful to create and edit documents on mobile devices.
Why it matters
Much of today’s office work is done outside an office.
Key players
Quip; Quickoffice; Box; Dropbox; Microsoft; Google; CloudOn
One afternoon last fall, David Levine took the subway from his office in lower Manhattan to a meeting at Rockefeller Center in midtown. The 35-year-old CIO of the startup investment firm Artivest was working on a blog post with colleagues and with freelancers in Boston and Crete. Levine used a new app called Quip to type the post on his iPhone, his wireless connection waxing and waning as the F train clattered through the tunnels. Quip let the team make changes, add comments, and chat via text, all presented in a Facebook-style news feed. Whenever Levine’s connection returned, the app synchronized his contributions with everyone else’s, so they all were working on the same version.

Had they been working with a traditional word-processing program, the process would probably have been a drawn-out round-robin of e-mail messages, proliferating attachments, and manual collation of disparate contributions. Instead, “by the time I got out of the subway, the post was done,” Levine recalls, “and by the time I got out of the meeting, it was on the website.”

It has taken a while for the software that helps people get work done to catch up with the fact that many people are increasingly working on tablets and phones. Now new apps are making it easier to create and edit documents on the go. Meanwhile, cloud-based file storage services, including Box, Dropbox, Google Drive, and Microsoft’s ­OneDrive—which have plunged in cost and soared in usage—help keep the results in sync even as multiple users work on the same file simultaneously. Some cloud services do this by separating what look to users like unified files into separate entries—paragraphs, words, even individual characters—in easily manipulated databases. That lets them smoothly track and merge changes made by different people at different times.

But the most interesting new mobile collaboration services don’t just replicate the software we’re accustomed to using on desktop computers. They also highlight an aspect of group work that received scant attention in the days when coworkers gathered together in offices: the communication that is part and parcel of collaboration. That back-and-forth can have as much value as the content itself. It can keep the team on track, inform participants who join the process late, and spark new ideas.

In traditional word-processing software, much of that conversation gets lost in “notes,” comments, or e-mail. But new document-­editing apps capture the stream of collaborative communication and put it on equal footing with the nominal output of the process. Box’s document-­collaboration service Box Notes displays avatar icons along the left-hand margin to show who contributed what; CloudOn, a mobile editor for Microsoft Office documents, gives prime placement to both conversations (comments, messages) and tasks (editing, approvals, permissions); and Quip displays a running text-message thread.

“It’s like you walked over to someone’s desk and said, ‘Read this and let me know if you have any questions,’” says Bret ­Taylor, Quip’s founder and CEO, who was formerly CTO at Facebook. “It’s a very personal, intimate experience that has been lost since the days of e-mail.”

By incorporating streams of messages about the work being created, these apps reflect the fact that many communications are now brief, informal, and rapid. “Most younger people rely on short-form mobile messaging and use e-mail only for more formal communications,” ­Taylor points out.

For Levine, who has been known to fire off a blog post before getting out of bed in the morning (much to his wife’s dismay), this mobile way of working is far more consonant with the way he lives—striving to squeeze every last iota of productivity out of each moment. “It allows me to accomplish what I need to do without interrupting my flow,” he says. Even when he’s in a subway tunnel.

by Ted Greenwald",2-20%,Positive,Software,product
Oculus Rift,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/oculus-rift/,"Oculus Rift
Thirty years after virtual-reality goggles and immersive virtual worlds made their debut, the technology finally seems poised for widespread use.
by Simon Parkin archive page
April 23, 2014
Above: A version of the Oculus Rift headset was made available to developers last year.
Above: A version of the Oculus Rift headset was made available to developers last year.
BRUCE PETERSON, MODEL: JJ BANICKI
Palmer Luckey had not been born when The Lawnmower Man was released in 1992, but the movie, with its vision of computer-generated sensory immersion, helped seed his interest in virtual reality as soon as he saw it. He dreamed of playing video games in simulated 3-D worlds—a dream that led him to amass one of the world’s largest collections of head-mounted displays and, eventually, inspired him to attempt to make his own. With no formal engineering training, Luckey designed his first working prototype in his garage at the age of 16.

Today, the 21-year-old is the founder of Oculus VR, a company that is on the verge of releasing the Rift, an affordable virtual-reality headset for playing ultra-immersive video games. Facebook bought the company for $2 billion this spring.

Oculus VR had already attracted more than $91 million in venture funding, a near-fanatical following, and team members like the game programmer John Carmack, who led the development of influential video games such as Doom, Quake, and Rage. But the Facebook deal is a sign of faith that virtual reality is now sharp enough and cheap enough to have huge potential for more than video games. The idea of merging immersive virtual reality with social communications is intriguing. It could also be a compelling tool for teleconferencing, online shopping, or more passive forms of entertainment. Some filmmakers are, in fact, already experimenting with movies designed just for the Rift.


JOHN MACNEILL
Oculus Rift

Breakthrough
High-quality virtual-reality hardware that is cheap enough for the consumer market.
Why it matters
Visually immersive interfaces will lead to new forms of entertainment and communications.
Key players
Oculus Vry; Sony; Vuzix; Nvidia
Virtual-reality headsets could be found in some arcades when The Lawnmower Man was in the theaters. But the technology wasn’t good enough to catch on widely. This time around, Luckey realized that cheap smartphone components could be combined to stunning effect, rendering bright, crisp worlds much more compelling than the blocky graphics often seen through earlier virtual-­reality headsets.

When you use the Rift, you feel as though you’re actually inside these worlds. The technology follows the movement of your head in real time; lean in to take a better look at a virtual flower or look to the skies to gaze at a virtual cloud, and your mind is drawn into the simulation. You can almost believe you are fully there.

The vast audience for home video games appears hungry for the device. In August 2012, Oculus VR set out to raise $250,000 on Kickstarter and met the goal in a matter of hours. It surpassed $1 million within two days.

Luckey started shipping a version of the Rift for software developers in March 2013 for just $300, and in the past year, the hardware has improved significantly. The retail version, which is expected to launch later this year or early next, will offer resolution higher than 1,920 by 1,080 pixels per eye. Such stunningly sharp definition has only recently become possible at such a low price.

While video games are where this improved virtual-reality technology is likely to take off first, it could also have applications in telepresence, architecture, computer-aided design, emergency response training, and phobia therapy.

Indeed, in some niches, older VR technology has been in use for years. Some surgeons routinely practice operations using VR simulations, while some industrial designers use the technology to view their designs as if they had already been constructed. But 30 years ago, when Jaron Lanier founded VPL Research, the first company to sell virtual-reality goggles, such products were too expensive for the consumer mainstream (a single head-mounted display cost as much as $100,000).

There were other reasons, too, that earlier versions of virtual reality failed commercially. Players of Nintendo’s Virtual Boy, a low-end VR game system launched in the mid-1990s, complained of nausea after extended play. For other players, the keen sense of wonder and presence they felt inside a virtual world soon dissipated. “Your first time playing a game in a virtual world is incredible,” Lanier says, “but the 20th time is wearying.”

Things may be different now. Though some testers have experienced nausea using the Oculus Rift, the company says the latest version has almost eliminated this problem. And today’s virtual environments offer so much more fidelity that they could remain captivating for much longer. Artists have been able to create a more stimulating range of worlds, from the rigorously realistic to the more abstract and painterly.

Already Oculus has inspired imitators. Acknowledging the Rift as an inspiration, Sony has demonstrated a VR headset that players will be able to use with the PlayStation 4. Sony is also working with NASA to create a virtual-reality simulation of Mars using images pulled from the Mars Rover. A more mundane but potentially useful application that Sony is exploring would let travelers visit virtual hotel rooms before booking the real thing. Assuming they ever want to take the headsets off.

by Simon Parkin",0.01% - 0.1%,Positive,Hardware,product
Agile Robots,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/agile-robots/,"Agile Robots
Computer scientists have created machines that have the balance and agility to walk and run across rough and uneven terrain, making them far more useful in navigating human environments.
by Will Knight archive page
April 23, 2014
Researchers at MIT, led by Seth Teller and Russ Tedrake, replaced the dynamic balance software that comes with Atlas with their own version. This lets the robot walk relatively quickly over uneven and unfamiliar ground.
Researchers at MIT, led by Seth Teller and Russ Tedrake, replaced the dynamic balance software that comes with Atlas with their own version. This lets the robot walk relatively quickly over uneven and unfamiliar ground.
WEBB CHAPPELL
Agile Robots

Breakthrough
Legged machines that stride over uneven or unsteady terrain.
Why it matters
Much of the world is inaccessible to wheeled machines but not legged ones.
Key players
Boston Dynamics; Schaft; Honda
Walking is an extraordinary feat of biomechanical engineering. Every step requires balance and the ability to adapt to instability in a split second. It requires quickly adjusting where your foot will land and calculating how much force to apply to change direction suddenly. No wonder, then, that until now robots have not been very good at it.

Meet Atlas, a humanoid robot created by Boston Dynamics, a company that Google acquired in December 2013. It can walk across rough terrain and even run on flat ground. Although previous robots such as Honda’s ASIMO and Sony’s diminutive QRIO are able to walk, they cannot quickly adjust their balance; as a result, they are often awkward, and limited in practical value. Atlas, which has an exceptional sense of balance and can stabilize itself with ease, demonstrates the abilities that robots will need to move around human environments safely and easily.

Robots that walk properly could eventually find far greater use in emergency rescue operations. They could also play a role in routine jobs such as helping elderly or physically disabled people with chores and daily tasks in the home.

Marc Raibert, cofounder of Boston Dynamics, pioneered machines with “dynamic balance”—the use of continual motion to stay upright—in the early 1980s. As a professor at Carnegie Mellon University, he built a one-legged robot that leaped around his lab like a pogo stick possessed, calculating with each jump how to reposition its leg and its body, and how aggressively to push itself off the ground with its next bound. Atlas demonstrates dynamic balance as well, using high-powered hydraulics to move its body in a way that keeps it steady. The robot can walk across an unsteady pile of debris, walk briskly on a treadmill, and stay balanced on one leg when whacked with a 20-pound wrecking ball. Just as you instinctively catch yourself when pushed, shifting your weight and repositioning your legs to keep from falling over, Atlas can sense its own instability and respond quickly enough to right itself. The possibilities opened up by its humanlike mobility surely impressed Google. Though it’s not clear why the company is acquiring robotics businesses, it bought seven others last year, including ones specializing in vision and manipulation.

Atlas isn’t ready to take on home or office chores: its powerful diesel engine is external and noisy, and its titanium limbs thrash around dangerously. But the robot could perform repair work in environments too dangerous for emergency workers to enter, such as the control room of a nuclear power plant on the brink of a meltdown. “If your goals are to make something that’s the equivalent of a person, we have a ways to go,” Raibert says. But as it gets up and running, Atlas won’t be a bad example to chase after.

by Will Knight",0.1% to 1.3%,Positive,Hardware,project
Smart Wind and Solar Power,2014.0,https://www.technologyreview.com/10-breakthrough-technologies/2014/,https://www.technologyreview.com/technology/smart-wind-and-solar-power/,"Smart Wind and Solar Power
Big data and artificial intelligence are producing ultra-accurate forecasts that will make it feasible to integrate much more renewable energy into the grid.
by Kevin Bullis archive page
April 23, 2014
Turbines at this wind farm north of Limon, Colorado, collect and transmit massive amounts of data.
Turbines at this wind farm north of Limon, Colorado, collect and transmit massive amounts of data.
KEVIN BULLIS
Wind power is booming on the open plains of eastern Colorado. Travel seven miles north of the town of Limon on Highway 71 and then head east on County Road 3p, a swath of dusty gravel running alongside new power lines: within minutes you’ll be surrounded by towering wind turbines in rows stretching for miles. Three large wind farms have been built in the area since 2011. A new one is going up this year.

Every few seconds, almost every one of the hundreds of turbines records the wind speed and its own power output. Every five minutes they dispatch data to high-performance computers 100 miles away at the National Center for Atmospheric Research (NCAR) in Boulder. There artificial-intelligence-based software crunches the numbers, along with data from weather satellites, weather stations, and other wind farms in the state. The result: wind power forecasts of unprecedented accuracy that are making it possible for Colorado to use far more renewable energy, at lower cost, than utilities ever thought possible.

The amount of wind power has more than doubled since 2009.

The forecasts are helping power companies deal with one of the biggest challenges of wind power: its intermittency. Using small amounts of wind power is no problem for utilities. They are accustomed to dealing with variability—after all, demand for electricity changes from season to season, even from minute to minute. However, a utility that wants to use a lot of wind power needs backup power to protect against a sudden loss of wind. These backup plants, which typically burn fossil fuels, are expensive and dirty. But with more accurate forecasts, utilities can cut the amount of power that needs to be held in reserve, minimizing their role.


DATA SOURCES: U.S. ENERGY INFORMATION ADMINISTRATION AND XCEL ENERGY
Smart Wind and Solar Power

Breakthrough
Ultra-accurate ­forecasting of wind and solar power.
Why it matters
Dealing with the intermittency of renewable energy will be crucial for its expansion.
Key players
Xcel Energy; GE Power; National Center for Atmospheric Research
Before the forecasts were developed, Xcel Energy, which supplies much of Colorado’s power, ran ads opposing a proposal that it use renewable sources for a modest 10 percent of its power. It mailed flyers to its customers claiming that such a mandate would increase electricity costs by as much as $1.5 billion over 20 years.

But thanks in large part to the improved forecasts, Xcel, one of the country’s largest utilities, has made an about-face.

It has installed more wind power than any other U.S. utility and supports a mandate for utilities to get 30 percent of their energy from renewable sources, saying it can easily handle much more than that.

Solar power generation lags wind power production by about a decade.

An early version of NCAR’s forecasting system was released in 2009, but last year was a breakthrough year—accuracy improved significantly, and the forecasts saved Xcel nearly as much money as they had in the three previous years combined. This year NCAR is testing a similar forecasting system for solar power.


DATA SOURCES: U.S. ENERGY INFORMATION ADMINISTRATION AND XCEL ENERGY
Mining these detailed forecasts to develop a more flexible and efficient electricity system could make it much cheaper to hit ambitious international goals for reducing carbon emissions, says Bryan Hannegan, director of a $135 million facility at the National Renewable Energy Laboratory (NREL) in Golden, Colorado, that uses supercomputer simulations to develop ways to scale up renewable power. “We’ve got a line of sight to where we want to go in the long term with our energy and environment goals,” he says. “That’s not something we’ve been able to say before.”

Chasing the Wind

No one is more aware of the challenges of integrating wind power into the grid than Dayton Jones, a power plant dispatcher for Xcel Energy. From his perch on the 10th floor of the Xcel building in downtown Denver, he’s responsible for keeping the lights on in Colorado. Doing so requires matching power production to electricity demand by turning power plants on and off and controlling their output. Generating too much or too little power can damage electrical appliances or even plunge the grid into a blackout. Wind power, with its sharp fluctuations, makes his job harder.

A few years ago, dispatchers like Jones couldn’t trust forecasts of how much wind power would be available to the grid at a given time. Those forecasts were typically off by 20 percent, and sometimes wind power completely failed to materialize when predicted. The solution was to have fossil-fuel plants idling, ready to replace all of that wind power in a few minutes. This approach is expensive, and the more the system is intended to rely on wind power, the more expensive it gets. What’s more, running the backup fossil-fuel plants means you’re “throwing carbon up into the sky,” says William Mahoney, deputy director of the Research Applications Laboratory at NCAR. “It costs money, and it’s bad for the environment.”

Running backup fossil-fuel plants means “throwing carbon up into the sky”: “It costs money, and it’s bad for the environment.”
Actual power output (green line) is overlaid on a three-day wind power forecast (red line). The larger the yellow shaded area, the more uncertain the forecast.

NCAR’s forecasts give Jones enough confidence in wind power to shut down many of the idling backup plants. The number varies depending on the certainty of the forecast. If the weather is cold and wet and there’s a chance ice could form on wind turbines and slow them down or stop them from spinning, he might need enough fossil-fuel backup to completely replace his wind power.


DATA SOURCES: U.S. ENERGY INFORMATION ADMINISTRATION AND XCEL ENERGY
But on nice days with steady, abundant wind, he might shut down all his fast-response backup plants, even those normally reserved for responding to changes in demand. Under such circumstances, Jones can use the wind farms themselves to ensure that power supply matches demand: the output of a wind turbine can be changed almost instantly by angling the blades so they capture more or less wind. Computers at Xcel’s building in Denver tell wind farms how much power to produce, and automated controls coördinate hundreds of turbines, changing output minute by minute if needed.

Xcel’s original forecasts used data from just one or two weather stations per wind farm. Now NCAR collects information from nearly every wind turbine. The data feeds into a high-resolution weather model and is combined with the output from five additional wind forecasts. Using historical data, NCAR’s software learns which forecasts are best for each wind farm and assigns different weights to each accordingly. The resulting über-forecast is more accurate than any of the original ones. Then, using data about how much power each turbine in the field will generate in response to different wind speeds, NCAR tells Xcel how much power to expect, in 15-minute increments, for up to seven days.

Forecasting solar power is next for NCAR and Xcel, but that can be even trickier than wind. For one thing, Xcel doesn’t get information about how much power private rooftop solar panels are generating, so it doesn’t know how much of that power it could lose when clouds roll in. NCAR’s new solar forecasts will use data from satellites, sky imagers, pollution monitors, and publicly owned solar panels to infer how much solar power is being generated and then predict how that amount will change.

Virtual Energy

How might extremely accurate wind and solar forecasts help us use enough renewable energy to reach climate goals of significantly reducing carbon dioxide emissions? Researchers at NREL’s new Energy Systems Integration Facility start by looking at how well wind and solar power can offset each other. To what extent, for example, can wind blowing at night make up for the lack of sunshine? But they are also looking at how to couple forecasts with smart dishwashers, water heaters, solar-panel inverters, water treatment plants, and electric-car chargers, not only to accommodate shifts in the wind but to ride out inevitable windless periods and weeks of cloudy weather without resorting to fossil fuels


DATA SOURCES: U.S. ENERGY INFORMATION ADMINISTRATION AND XCEL ENERGY
The red line—the result of subtracting wind power supply (blue) from demand (black)—shows the amount of power Xcel needs to generate with its fossil-fuel plants. The lighter lines are forecasts.

Take the example of electric cars. A car stores enough electricity to power a house for anywhere from half a day to several days, depending on the size of the battery pack. And it has sophisticated power electronics that can control the timing and vary the rate of charging, which could offer a way to match fluctuating wind power to electricity demand. With small modifications, the cars’ batteries can deliver stored power to a home and to the power grid. There aren’t many electric cars now, but that could easily change in the decades it will take before renewable energy makes up more than 30 or 40 percent of the electricity supply (wind supplies 4 percent now, and solar less than 1 percent).

At NREL, researchers can plug 30 electric cars into docks that let them interface with power-grid simulations on a supercomputer, to project what would happen if thousands of cars were connected to the grid. The idea is that electric cars might store power from solar panels and use it to power neighborhoods when electricity demand peaks in the evening, and then recharge their batteries using wind power in the early morning hours.

Forecasts like the ones being developed at NCAR will be “absolutely critical,” says Bri-Mathias Hodge, a senior research engineer at NREL. They will help determine when the cars’ batteries should charge to maximize the electricity they make available to the grid without leaving drivers short of the power they need.

Even before that becomes a reality, though, forecasts from NCAR are already having a big effect. Last year, on a windy weekend when power demand was low, Xcel set a record: during one hour, 60 percent of its electricity for Colorado was coming from the wind. “That kind of wind penetration would have given dispatchers a heart attack a few years ago,” says Drake Bartlett, who heads renewable-energy integration for Xcel. Back then, he notes, they wouldn’t have known whether they might suddenly lose all that power. “Now we’re taking it in stride,” he says. “And that record is going to fall.”

by Kevin Bullis",0.5% - 1.5%,"The article discusses how accurate wind and solar forecasts are crucial for integrating renewable energy into the power grid. Xcel Energy, a utility company in Colorado, has partnered with the National Center for Atmospheric Research (NCAR) to develop highly accurate weather forecasts that can predict wind and solar power output up to seven days in advance.

With these forecasts, Xcel's dispatchers can better manage their power supply and demand, knowing exactly how much renewable energy they will have available at any given time. This allows them to adjust their fossil-fuel backup plants accordingly, reducing the amount of greenhouse gas emissions released into the atmosphere.

The article highlights several key benefits of accurate wind and solar forecasts:

1. **Increased wind penetration**: Xcel was able to set a record for wind power generation on a single hour, with 60% of its electricity coming from wind.
2. **Improved grid management**: Dispatchers can adjust their fossil-fuel backup plants in real-time, reducing emissions and improving overall grid efficiency.
3. **Enhanced reliability**: With accurate forecasts, Xcel can better anticipate periods of low wind or solar output, allowing them to make adjustments to maintain a reliable power supply.

The article also touches on the potential for coupling wind and solar forecasts with smart devices, such as electric cars, water heaters, and solar-panel inverters. This could enable greater flexibility in the grid, allowing for more efficient use of renewable energy and reducing reliance on fossil fuels.

Overall, the article emphasizes the importance of accurate weather forecasting in integrating renewable energy into the power grid and highlights the potential benefits of this approach in reducing greenhouse gas emissions and improving overall grid efficiency.",Climate Tech,"The article discusses how advanced weather forecasting and data analysis are being used to improve the integration of renewable energy sources, particularly wind and solar power, into the electrical grid.

The main focus is on a partnership between Xcel Energy, an electric utility company, and the National Center for Atmospheric Research (NCAR), which uses high-resolution weather models and machine learning algorithms to predict wind and solar output. This allows Xcel to better manage its energy supply and demand in real-time, reducing the need for fossil fuel backup power plants.

Some key points include:

* The use of advanced forecasting techniques has allowed Xcel to increase the amount of renewable energy it can integrate into the grid without relying on fossil fuels.
* On a windy weekend last year, Xcel was able to generate 60% of its electricity from wind power for one hour, setting a new record.
* The forecasting system uses data from nearly every wind turbine, as well as weather stations and other sources, to provide highly accurate predictions of wind output.
* The article also discusses the potential for using forecasts to manage solar energy output, which is more variable than wind power.
* Researchers at the National Renewable Energy Laboratory (NREL) are exploring ways to use advanced forecasting and data analysis to optimize the integration of renewable energy into the grid.

Some potential applications of this technology include:

* Using electric cars as a form of distributed energy storage, with the ability to charge and discharge in response to changes in wind or solar output.
* Developing smart appliances that can adjust their energy usage in real-time based on forecasts of available renewable energy.
* Improving the efficiency of the grid by using advanced forecasting to predict and manage energy demand.

Overall, the article suggests that advances in weather forecasting and data analysis are playing a critical role in enabling the widespread adoption of renewable energy sources."
Magic Leap,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/magic-leap/,"Magic Leap
A startup is betting more than half a billion dollars that it will dazzle you with its approach to creating 3-D imagery.
by Rachel Metz archive page
February 18, 2015

JOSH COCHRAN
Logically, I know there isn’t a hulking four-armed, twisty-horned blue monster clomping in circles in front of me, but it sure as hell looks like it.

I’m sitting behind a workbench in a white-walled room in Dania Beach, Florida, in the office of a secretive startup called Magic Leap. I’m staring wide-eyed through a pair of lenses attached to what looks like metal scaffolding that towers over my head and contains a bunch of electronics and lenses. It’s an early prototype of the company’s so-called cinematic-­reality technology, which makes it possible for me to believe that the muscular beast with the gruff expression and two sets of swinging arms is actually in the room with me, hovering about seven feet in front of my face.

He’s not just visible at a set distance. I’m holding a video-game controller that’s connected to the demo station, and at the press of a button I can make the monster smaller or larger, move him right or left, bring him closer, or push him farther away.

Of course, I bring him as near as possible; I want to see how real he looks up close. Now he’s about 30 inches from my eyeballs and, though I’ve made him pocket-sized, looks about as authentic as a monster could—he seems to have rough skin, muscular limbs, and deep-set beady eyes. I extend my hand to give him a base to walk on, and I swear I feel a tingling in my palm in expectation of his little feet pressing into it. When, a split second later, my brain remembers that this is just an impressively convincing 3-D image displayed in the real space in front of me, all I can do is grin.

Virtual- and augmented-reality technologies used in movies, smartphone apps, and gadgets tend to underdeliver on overhyped promises with images that look crappy. Typically that’s because stereoscopic 3-D, the most commonly used method, is essentially tricking your eyes instead of working with the way you normally see things. It produces a sense of depth by showing each eye a separate image of the same object at a different angle. But since that forces you to look simultaneously at a flat screen in the distance and images that appear to be moving in front of you, it can make you dizzy and lead to headaches and nausea.


A video by the musician St. Vincent floats on a virtual screen in a break area in Magic Leap’s headquarters.
Magic Leap

Breakthrough
A device that can make virtual objects appear in real life.
Why it matters
The technology ­ could open new opportunities for the film, gaming, travel, and telecommunications industries.
Key players
Magic Leap; Microsoft
To be sure, stereoscopic 3-D has recently started getting better. The best system you can currently buy comes from Oculus VR, which Facebook purchased last spring for $2 billion; the $199 Gear VR, which was built in collaboration with Samsung and is aimed at software developers, lets you slide a Samsung smartphone into a headset to play games and watch videos.

But while Oculus wants to transport you to a virtual world for fun and games, Magic Leap wants to bring the fun and games to the world you’re already in. And in order for its fantasy monsters to appear on your desk alongside real pencils, Magic Leap had to come up with an alternative to stereoscopic 3-D—something that doesn’t disrupt the way you normally see things. Essentially, it has developed an itty-bitty projector that shines light into your eyes—light that blends in extremely well with the light you’re receiving from the real world.

Abovitz says he and his employees are trying to “blow away” their inner 11-year-olds.
As I see crisply rendered images of monsters, robots, and cadaver heads in Magic Leap’s offices, I can envision someday having a video chat with faraway family members who look as if they’re actually sitting in my living room while, on their end, I appear to be sitting in theirs. Or walking around New York City with a virtual tour guide, the sides of buildings overlaid with images that reveal how the structures looked in the past. Or watching movies where the characters appear to be right in front of me, letting me follow them around as the plot unfolds. But no one really knows what Magic Leap might be best for. If the company can make its technology not only cool but comfortable and easy to use, people will surely dream up amazing applications.

That’s no doubt why Google took the lead in an astonishingly large $542 million investment round in Magic Leap last October. Whatever it is cooking up has a good chance of being one of the next big things in computing, and Google would be crazy to risk missing out. The investment looked especially prescient in January, when Microsoft revealed plans to release a sleek-looking headset this year. HoloLens, which lets you interact with holograms, sounds as if it’s very similar to what Magic Leap is working on.


Top: In a demonstration of a medical or educational application, a cadaver head can be dissected one slice at a time.

Bottom: A fake robot appears to stand on a real hand.
MAGIC LEAP
Behind the magic

Magic Leap won’t say when it will release a product or how much the thing will cost, beyond that the price will be within the range of today’s consumer mobile devices. When I press founder and CEO Rony Abovitz about such details, he’ll only smile and say, “It’s not far away.”

He’s sitting behind the desk in his office, which is just down the road from the Fort Lauderdale–Hollywood airport. The shelves are lined with toys and View-Masters—the plastic gadgets that let you look at pictures in 3-D. Abovitz, 44, is a bear of a guy with a kind smile, and when I meet him he’s dressed in black Nikes, a long-sleeved shirt, and slacks, his graying curly hair topped with a yarmulke. He’s thoughtful and composed, which I find somewhat surprising given that the only time I had seen him before was in a video of his talk at a TEDx event in 2012 in Sarasota, Florida. It featured two people dressed as furry creatures called “Shaggles,” Abovitz walking on stage dressed as an astronaut, and unintelligible rock music. Though the talk, called “The Synthesis of Imagination,” came off as performance art (perhaps even a mockery of a TED talk), he swears there is a coherent message embedded in it; figure it out, he says, and he’ll give you a yo-yo.

By day, Abovitz is a technology entrepreneur with a background in biomedical engineering. He previously founded Mako Surgical, a company in Fort Lauderdale that makes a robotic arm equipped with haptic technology, which imparts a sense of touch so that orthopedic surgeons have the sensation of actually working on bones as they trigger the robot’s actions. Mako was sold to a medical technology company, Stryker, for nearly $1.7 billion in 2013. By night, Abovitz likes to rock out. He sings and plays guitar and bass in a pop-rock band called Sparkydog & Friends. And as he tells it, Magic Leap has its origins in both the robotic-surgery company and his life as a musician.

Combining virtual reality with the physical world appealed to Abovitz even at Mako. Although the robotic-arm technology could give surgeons the sensation of touching their instruments to bones, Abovitz also wanted to let them see virtual bones as they went about this work. Over and over, he says, he tried out head-mounted displays made by different companies, but he was disappointed with them all. “They were all just complete crap,” he says. “You’d put it on and it would give you a headache and it was awful, and I was wondering, ‘Why is this so bad?’”

At the same time, Abovitz also wanted to take Sparkydog & Friends on a virtual tour. In U2’s 1987 video for “Where the Streets Have No Name,” the group, in a nod to an earlier move by the Beatles, plays an impromptu show on the roof of a Los Angeles liquor store. Abovitz yearned for his band to be able to do that, but virtually, and on a thousand rooftops at once.

About four years ago, he started mulling the problem over with John Graham Macnamara, a high school friend who had dropped out of Caltech’s theoretical physics program. They became captivated by the idea of displaying moving holograms like the one in Star Wars. Holograms—3-D images that can be viewed from many angles—are made by accurately re-creating light fields, the patterns made when light rays bounce off an object. But Abovitz figured it would cost a lot and take lots of time to project even low-resolution holographic images. At one point, he remembers muttering, “There is no display that can actually work.”


MAGIC LEAP
The next morning, though, he awoke with an idea: why bother with the painstaking steps needed to send a hologram out into a room for multiple people to see at once? Why not, instead, essentially make a hologram that only you see, doing it in a way that is natural for the eyes and brain to perceive, unlike stereoscopic 3-D? “We’re spending half a billion dollars–plus to effectively make nothing happen to you, physiologically,” Abovitz says.

The solution he and Macnamara and the rest of Magic Leap’s team have come up with is still largely under wraps, and on the record they avoid discussing how the technology works except in vague terms, citing concerns about competition. But it’s safe to say Magic Leap has a tiny projector that shines light onto a transparent lens, which deflects the light onto the retina. That pattern of light blends in so well with the light you’re receiving from the real world that to your visual cortex, artificial objects are nearly indistinguishable from actual objects.

If the company can get this to work in a head-mounted display, showing images near the eyes and consistently refocusing them to keep everything looking sharp, it will make 3-D images much more comfortable to view, says Gordon ­Wetzstein, an assistant professor of electrical engineering at Stanford who researches computational imaging and displays. “If they do what people suspect they do,” Wetzstein says, “it will be amazing.”

From virtual to reality

Magic Leap is working feverishly to get to that point. Since building its first prototype in 2011, the company has continued to shrink its technology down.

Already it works on something smaller than the unwieldy scaffolding I used. In another demonstration, using hardware on a cart, I can poke at a tiny flying steampunk robot, a character from a first-person-shooter game called Dr. Grordbort’s Invaders that Magic Leap is making with Weta Workshop, which created many of the special effects in the Hobbit movies. The robot can follow my finger around with surprising accuracy, right between the cubicles in Magic Leap’s office.

To judge from a look I get at a design prototype—a realistic-looking piece of hardware that’s completely nonfunctional—the company appears to be aiming to fit its technology into a chunky pair of sports sunglasses wired to a square pack that fits into your pocket. A somewhat similar image in a patent application Magic Leap filed in January suggests as much, too. The company won’t say for sure, though; Abovitz confirms that the headset will be a glasses-like wearable device, but I have to twist his arm to get him to agree to use even that hazy phrasing on the record.

It’s clear that getting the technology into that small form will be very hard. The smallest demo hardware I’ve seen at Magic Leap can’t yet match the experience of the bigger demo units. It includes a projector, built into a black wire, that’s smaller than a grain of rice and channels light toward a single see-through lens. Peering through the lens, I spy a crude green version of the same four-armed monster that earlier seemed to stomp around on my palm. In addition to improving the resolution of smaller units, Magic Leap will have to cram in sensors and software that will track your eyes and fingers, so you can control and interact with its virtual creatures—which themselves will have to incorporate real-life objects into whatever they appear to be doing.


Abovitz was enigmatic in his brief appearance on a TEDx stage in 2012. “A few awkward steps for me; a magic leap for mankind,” he said from inside his spacesuit.
TEDX
That’s where last year’s half-billion dollars of investment come in. Magic Leap is hiring like crazy. It’s looking for software engineers for everything from eye tracking and iris recognition to the branch of artificial intelligence known as deep learning. It needs optical engineers, game designers, and other people who will dream up virtual objects to display. To give you a sense of where their minds might go, I saw ray guns and magic wands lying around the office. As its chief futurist, Magic Leap has hired the science fiction author Neal Stephenson, whose 1992 novel Snow Crash imagined a virtual world called the Metaverse.

The excitement of such quick growth is palpable at Magic Leap’s brightly decorated headquarters, where staid office trappings are punctuated by red high-backed love seats and yellow chairs. Employees energetically describe the games, sensors, and ray guns they’re working on.

With the massive investment last year, interest in the company has intensified. Abovitz says, “We went from ‘Does anyone care about this?’ to ‘Okay, people do care.’” Now he and the team are feeling the weight of these expectations. He says, “The inner 11-year-old—we want to blow that away.”

by Rachel Metz",0.001% - 0.01%,"Magic Leap is a technology company that has been working on a top-secret project to create a new type of augmented reality (AR) experience. The company's founder, Rony Abovitz, had an idea for a device that would allow users to see and interact with virtual objects in the real world, but he soon realized that creating such a device would be extremely challenging.

Instead of pursuing this goal, Abovitz had a change of heart and decided to focus on creating a more personal and intimate AR experience. He wanted to create a device that would allow users to see virtual objects in a way that was natural for the eyes and brain to perceive, unlike traditional stereoscopic 3D.

The solution that Magic Leap came up with is a tiny projector that shines light onto a transparent lens, which deflects the light onto the retina. This pattern of light blends in so well with the light from the real world that it creates a seamless AR experience.

Magic Leap is working to miniaturize this technology and fit it into a wearable device, such as a pair of sports sunglasses. The company has filed a patent application for a design that resembles a chunky pair of glasses with a square pack that fits into the user's pocket.

The company is hiring aggressively to bring on engineers, designers, and other experts who can help them develop this technology. They are working on creating software that will track the user's eyes and fingers, allowing them to control and interact with virtual objects in 3D space.

Magic Leap has also hired science fiction author Neal Stephenson as their chief futurist, whose work has inspired many of the company's ideas about AR and virtual reality. The company is feeling intense pressure to deliver on its promises, but Abovitz says that they are committed to creating something truly innovative and groundbreaking.

Some of the key points from the article include:

* Magic Leap is working on a top-secret project to create a new type of augmented reality experience
* The company's founder, Rony Abovitz, had an idea for a device that would allow users to see and interact with virtual objects in the real world
* Instead of pursuing this goal, Abovitz decided to focus on creating a more personal and intimate AR experience
* Magic Leap has developed a technology that uses a tiny projector to shine light onto a transparent lens, which deflects the light onto the retina
* The company is working to miniaturize this technology and fit it into a wearable device, such as a pair of sports sunglasses
* Magic Leap is hiring aggressively to bring on engineers, designers, and other experts who can help them develop this technology.",Hardware,"Magic Leap is a company working on a revolutionary new technology that blends the virtual and real worlds in a way that's comfortable for the human brain to process. The technology uses a tiny projector to shine light onto a transparent lens, which deflects the light onto the retina, creating images that are nearly indistinguishable from real objects.

The company was founded by Rony Abovitz, who initially had trouble getting investors interested in his idea. However, after a successful demonstration of the technology, Magic Leap raised over $500 million in funding last year and has been growing rapidly since then.

Abovitz's goal is to create a device that allows people to interact with virtual objects in a way that feels natural and intuitive. He wants to make it possible for people to see and interact with virtual objects as if they were real, without the need for cumbersome headsets or other equipment.

The technology has the potential to revolutionize a wide range of fields, from gaming and entertainment to education and healthcare. Magic Leap is working on developing games and other experiences that take advantage of the technology, including a first-person shooter game called Dr. Grordbort's Invaders.

One of the key challenges facing Magic Leap is miniaturizing the technology so that it can be fit into a wearable device. The company has made significant progress in this area, with prototype hardware that is smaller than earlier versions. However, there are still many technical hurdles to overcome before the technology can be released to the public.

Magic Leap has attracted a team of talented engineers and designers from around the world, including science fiction author Neal Stephenson as its chief futurist. The company's headquarters in Florida is a hub of activity, with employees working on everything from eye-tracking software to game design.

Abovitz and his team are feeling the pressure of living up to the hype surrounding Magic Leap's technology. However, they remain confident that they can deliver on their promise and create something truly revolutionary. As Abovitz says, ""The inner 11-year-old – we want to blow that away."""
Nano-Architecture,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/nano-architecture/,"Nano-Architecture
A Caltech scientist creates tiny lattices with enormous potential.
by Katherine Bourzac archive page
February 18, 2015
Fine-tuning materials’ architecture at the nanoscale yields distinctive patterns—and unusual properties.
Fine-tuning materials’ architecture at the nanoscale yields distinctive patterns—and unusual properties.
JULIA R. GREER
Nano-Architecture

Breakthrough
Materials whose structures can be precisely tailored so they are strong yet flexible and extremely light.
Why it matters
Lighter structural materials would be more energy-efficient and versatile.
Key players
Julia Greer, Caltech; William Carter, HRL Laboratories; Nicholas Fang, MIT; Christopher Spadaccini, Lawrence Livermore National Laboratory
To visit the lab of Caltech materials ­scientist Julia Greer is to enter a realm where the ordinary rules of physical stuff don’t seem to apply. Greer designs and builds nanomaterials that behave in ways surprising to those of us who spend our days in a world where strong materials like ceramic and steel tend to be heavy, while lightweight ones are weak. When Greer controls architecture at the nanoscale, the rules change.

Conventional ceramics are strong, heavy, and (as anyone who has dropped a plate knows) brittle, prone to shattering. But last year Greer created a ceramic that is one of the strongest and lightest substances ever made. It’s also not brittle. In a video Greer made (you can see it below), a cube of the material shudders a bit as a lab apparatus presses down hard on it, then collapses. When the pressure is removed, it rises back up “like a wounded soldier,” she says. “It’s unreal, isn’t it?” Greer often rushes to meetings around campus on Rollerblades and talks so fast that she demands focused listening. Peering into this beautiful, otherworldly nanolattice on her computer screen, she slows down for a while.

If materials like Greer’s could be produced in large quantities, they could replace composites and other materials used in a wide range of applications, because they’d be just as strong at a fraction of the weight. Another possibility is to greatly increase the energy density of batteries—the amount of power they can hold at a given size. To do that, researchers have been trying to develop electrodes that are lighter than the ones used in today’s batteries but can store more energy. However, promising electrode materials such as silicon are prone to cracking under strain. An electrode made by coating a metal nanolattice with silicon could have crack-resistant toughness in its very structure. The key to creating such wondrous materials is an arsenal of specialized machines—some of which Greer has rebuilt to suit her purposes—that make it possible to precisely control structure at the nanoscale over relatively large areas.

Greer jogs down two floors of stairs to the basement lab where she keeps these precision instruments to isolate them from vibrations. One machine, found behind two heavy black curtains, is a sort of 3-D printer that uses flashes of laser light to very slowly build intricate polymer scaffolds. A student of Greer’s coats the polymer with metals, ceramics, or other materials and then shaves off the sides, making it possible to etch away the polymer inside. The result is a little block of material made up of nanoscale trusses crisscrossed like the struts in the Eiffel Tower—but each strut’s walls are only about 10 nanometers thick.

Without Greer’s method, building something like this is impossible. She shows me a sample that came about from an earlier collaboration with researchers at HRL Laboratories in Malibu, California, who are producing materials with larger, microscale trusses. It’s made out of nickel and looks somewhat like a metal scouring sponge. When she lets it drift onto my palm, I can barely feel it touch down, and the subversion of expectations is confusing. This metal is, literally, lighter than a feather. It could make for ultralight thermal insulation—an application her HRL colleagues are pursuing.

The featherweight nickel shows the promise of architectural control in making new materials with weird properties. But it’s also a reminder of how far Greer has to go in scaling up her methods: so far, she can’t make enough of the nanostructured materials to cover your palm.

Greer is determined to use her nanofabrication methods for a variety of materials, and a long list of collaborators are interested in their unusual properties. She can space the nanoscale walls in light-emitting materials or thermal insulation to precisely control the flow of light or heat. She’s working with two battery makers to use her nanostructures to study electrochemistry. And she is teaming with biologists to see whether the nanostructured ceramic could serve as a scaffold for growing bones—such as the tiny ones in the ear whose degeneration is one cause of deafness.

In hopes of making such applications feasible, she is working to speed up the high-resolution laser-printing process. Greer has a six-millimeter-square fleck of the nanostructured ceramic she made last year. It is about as thick as a sheet of paper but took about a week to make. “For us to do scientific experiments, we don’t need a large amount,” she says. “The question now is: how do you scale this?”

by Katherine Bourzac",0.01% - 0.1%,Positive,Nanotech,concept
Car-to-Car Communication,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/car-to-car-communication/,"Car-to-Car Communication
A simple wireless technology promises to make driving much safer.
by Will Knight archive page
February 18, 2015
Fine-tuning materials’ architecture at the nanoscale yields distinctive patterns—and unusual properties.
Fine-tuning materials’ architecture at the nanoscale yields distinctive patterns—and unusual properties.
LAURENT CILLUFO
Car-to-Car Communication

Breakthrough
Cars that can talk to each other to avoid crashes.
Why it matters
More than a ­million people are killed on roads worldwide every year.
Key players
General Motors; University of Michigan National; Highway Traffic Safety Administration
Hariharan Krishnan hardly looks like a street racer. With thin-rimmed glasses and a neat mustache, he reminds me of a math teacher. And yet on a sunny day last September, he was speeding, seemingly recklessly, around the parking lot at General Motors’ research center in Warren, Michigan, in a Cadillac DTS.

I was in the passenger seat as Krishnan wheeled around a corner and hit the gas. A moment later a light flashed on the dashboard, there was a beeping sound, and our seats started buzzing furiously. Krishnan slammed on the brakes, and we lurched to a stop just as another car whizzed past from the left, its approach having been obscured by a large hedge. “You can see I was completely blinded,” he said calmly.

The technology that warned of the impending collision will start appearing in cars in just a couple of years. Called car-to-car or vehicle-to-vehicle communication, it lets cars broadcast their position, speed, steering-wheel position, brake status, and other data to other vehicles within a few hundred meters. The other cars can use such information to build a detailed picture of what’s unfolding around them, revealing trouble that even the most careful and alert driver, or the best sensor system, would miss or fail to anticipate.

Already many cars have instruments that use radar or ultrasound to detect obstacles or vehicles. But the range of these sensors is limited to a few car lengths, and they cannot see past the nearest obstruction.

Car-to-car communication should also have a bigger impact than the advanced vehicle automation technologies that have been more widely heralded. Though self-driving cars could eventually improve safety, they remain imperfect and unproven, with sensors and software too easily bamboozled by poor weather, unexpected obstacles or circumstances, or complex city driving. Simply networking cars together wirelessly is likely to have a far bigger and more immediate effect on road safety.

Creating a car-to-car network is still a complex challenge. The computers aboard each car process the various readings being broadcast by other vehicles 10 times every second, each time calculating the chance of an impending collision. Transmitters use a dedicated portion of wireless spectrum as well as a new wireless standard, 802.11p, to authenticate each message.

Krishnan took me through several other car-to-car safety scenarios in the company’s parking lot. When he started slowly pulling into a parking spot occupied by another car, a simple alert sounded. When he attempted a risky overtaking maneuver, a warning light flashed and a voice announced: “Oncoming vehicle!”

More than five million crashes occur on U.S. roads alone every year, and more than 30,000 of those are fatal. The prospect of preventing many such accidents will provide significant impetus for networking technology.

Just an hour’s drive west of Warren, the town of Ann Arbor, Michigan, has done much to show how valuable car-to-car communication could be. There, between 2012 and 2014, the National Highway Traffic Safety Administration and the University of Michigan equipped nearly 3,000 cars with experimental transmitters. After studying communication records for those vehicles, NHTSA researchers concluded that the technology could prevent more than half a million accidents and more than a thousand fatalities in the United States every year. The technology stands to revolutionize the way we drive, says John Maddox, a program director at the University of Michigan’s Transportation Research Institute.

Shortly after the Ann Arbor trial ended, the U.S. Department of Transportation announced that it would start drafting rules that could eventually mandate the use of car-to-car communication in new cars. The technology is also being tested in Europe and Japan.

There will, of course, also be a few obstacles to navigate. GM has committed to using car-to-car communication in a 2017-model Cadillac. Those first Cadillacs will have few cars to talk to, and that will limit the value of the technology. It could still be more than a decade before vehicles that talk to each other are commonplace.

by Will Knight",0.01% - 0.1%,Positive,Hardware,methodology
Project Loon,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/project-loon/,"Project Loon
Billions of people could get online for the first time thanks to helium balloons that Google will soon send over many places cell towers don’t reach.
by Tom Simonite archive page
February 18, 2015

You climb 170 steps up a series of dusty wooden ladders to reach the top of Hangar Two at Moffett Federal Airfield near Mountain View, California. The vast, dimly lit shed was built in 1942 to house airships during a war that saw the U.S. grow into a technological superpower. A perch high in the rafters is the best way to appreciate the strangeness of something in the works at Google—a part of the latest incarnation of American technical dominance.

On the floor far below are Google employees who look tiny as they tend to a pair of balloons, 15 meters across, that resemble giant white pumpkins. Google has launched hundreds of these balloons into the sky, lofted by helium. At this moment, a couple of dozen float over the Southern Hemisphere at an altitude of around 20 kilometers, in the rarely visited stratosphere—nearly twice the height of commercial airplanes. Each balloon supports a boxy gondola stuffed with solar-powered electronics. They make a radio link to a telecommunications network on the ground and beam down high-speed cellular Internet coverage to smartphones and other devices. It’s known as Project Loon, a name chosen for its association with both flight and insanity.

Google says these balloons can deliver widespread economic and social benefits by bringing Internet access to the 60 percent of the world’s people who don’t have it. Many of those 4.3 billion people live in rural places where telecommunications companies haven’t found it worthwhile to build cell towers or other infrastructure. After working for three years and flying balloons for more than three million kilometers, Google says Loon balloons are almost ready to step in.

It is odd for a large public company to build out infrastructure aimed at helping the world’s poorest people. But in addition to Google’s professed desires to help the world, the economics of ad-­supported Web businesses give the company other reasons to think big. It’s hard to find new customers in Internet markets such as the United States. Getting billions more people online would provide a valuable new supply of eyeballs and personal data for ad targeting. That’s one reason Project Loon will have competition: in 2014 Facebook bought a company that makes solar-powered drones so it can start its own airborne Internet project.


The helium balloons are inflated to the size they reach in the stratosphere. The “ballonets” inside are filled with air or emptied to make the balloon fall or rise.
RC RIVERA
Project Loon

Breakthrough
A reliable and cost-effective way to beam Internet service from the sky to places lacking it.
Why it matters
Internet access could expand educational and economic opportunities for the 4.3 billion people who are offline.
Key players
Google; Facebook
Google’s planet-scale social-engineering project is much further along. In tests with major cellular carriers, the balloons have provided high-speed connections to people in isolated parts of Brazil, Australia, and New Zealand. Mike Cassidy, Project Loon’s leader, says the technology is now sufficiently cheap and reliable for Google to start planning how to roll it out. By the end of 2015, he wants to have enough balloons in the air to test nearly continuous service in several parts of the Southern Hemisphere. Commercial deployment would follow: Google expects cellular providers to rent access to the balloons to expand their networks. Then the number of people in the world who still lack Internet access should start to shrink, fast.

Balloon revolution

“HARMLESS SCIENCE EXPERIMENT.” That’s what was written on the boxes carried by the balloons that the secretive Google X lab began to launch over California’s Central Valley in 2012, along with a phone number and the promise of a reward for safe return. Inside the boxes was a modified office Wi-Fi router. The balloons were made by two seamsters hired from the fashion industry, from supplies bought at hardware stores.

Project Loon is now much less like a science project. In 2013, Google began working with a balloon manufacturer, Raven Aerostar, which expanded a factory and opened another to make the inflatable “envelope” for the balloons. That June, Google revealed the existence of the project and described its first small-scale field trials, in which Loon balloons provided Internet service to people in a rural area of New Zealand. In 2014, Project Loon focused on turning a functional but unwieldy prototype into technology that’s ready to expand the world’s communication networks.


The 15-kilogram box carried by a Loon balloon has computers that act on commands from flight engineers, as well as equipment to transmit Internet connectivity to the ground below.
RC RIVERA
Loon’s leaders planned to buy their own space on the radio spectrum so their balloons could operate independently of existing wireless networks. But Google CEO Larry Page nixed that idea and said the balloons should instead be leased to wireless carriers, who could use the chunks of the airwaves they already own and put up ground antennas to link the balloons into their networks. That saved Google from spending billions on spectrum licenses and turned potential competitors into allies. “Nearly every telco we talk to wants to do it,” says Cassidy.

Google has also made major improvements to its stratospheric craft. One of the most significant was developing a way to accurately pilot balloons across thousands of miles without any form of propulsion. The stratosphere, which typically is used only by weather balloons and spy planes, is safely above clouds, storms, and commercial flights. But it has strong winds, sometimes exceeding 300 kilometers per hour. Providing reliable wireless service means being able to guarantee that there will always be a balloon within 40 kilometers.

Project Loon aims to change the economics of Internet access.
Google solved that aviation problem by turning it into a computer problem. Winds blow in different directions and at different speeds in different layers of the stratosphere. Loon balloons exploit that by changing altitude. As a smaller balloon inside the main one inflates or deflates, they can rise or fall to seek out the winds that will send them where Google wants them to go. It’s all directed by software in a Google data center that incorporates wind forecasts from the U.S. National Oceanic and Atmospheric Administration into a simulation of stratospheric airflow. “The idea is to find a way through the maze of the winds,” says Johan Mathe, a software engineer working on Loon’s navigation system. A fleet of balloons can be coördinated that way to ensure there is always one over any particular area.

The first version of this system sent new commands to Loon balloons once a day. It could find a way for a balloon launched over New Zealand, for example, to dawdle over land until prevailing winds pushed it east and over the Pacific Ocean. Then it would have the balloon ride the fastest winds possible for the 9,000-kilometer trip east to Chile. But that system could only get balloons within hundreds of kilometers of their intended target. For tests of Internet service in New Zealand and elsewhere, the company had to cheat, launching Loon balloons nearby to make sure they would be overhead. In late 2014, Google upgraded its balloon navigation system to give balloons fresh orders as frequently as every 15 minutes. They can now be steered with impressive accuracy over intercontinental distances. In early 2015, a balloon traveled 10,000 kilometers and got within 500 meters of its desired cell tower.


A balloon that was intentionally burst in a test is checked for flaws.
Google has also had to figure out how to make the balloons sturdier, so they can spend more time in the stratosphere. The longer they stay up, the lower the cost of operating the network. However, weight considerations mean a balloon’s envelope must be delicate. Made from polyethylene plastic with the feel of a heavy-weight trash bag, the material is easily pierced with a fingertip, and a stray grain of grit in the factory can make a pinprick-size hole that will bring a balloon back to earth after less than two weeks.

Preventing those leaks is the work of a squad inside Project Loon that has doggedly chased down every possible cause and come up with preventive measures. These researchers have studied balloons retrieved from the stratosphere, pored over video footage of others inflated to bursting on the ground, and developed a “leak sniffer” to find tiny holes by detecting helium. The leak squad’s findings have led to changes in the design of the balloon envelope, fluffier socks for factory workers who must step on the envelopes during production, and new machines to automate some manufacturing steps. Altogether, Google has introduced the first major changes the balloon industry has seen in decades, says Mahesh ­Krishnaswamy, who oversees manufacturing for Project Loon and previously worked on Apple’s manufacturing operations. Those changes have paid off. In the summer of 2013, Loon balloons lasted only eight days before having to be brought down, says ­Krishnaswamy. Today balloons last on average over 100 days, with most exceeding that time in flight; a handful last as long as 130 days.

Google has also made many improvements to the design of the Loon balloons’ payloads and electronics. But it still has problems left to solve. For example, Google needs to perfect a way of making radio or laser connections between balloons, so that they can pass data along in an aerial chain to connect areas far from any ground station.


Loon balloons can be landed by carefully releasing helium, but they pack a ­parachute for ­emergencies.
RC RIVERA
But Cassidy says Project Loon’s technology is already at a point where stratospheric Internet service can be tested at a global scale. In 2015 he aims to evaluate “quasi-continuous” service along a thin ribbon around the Southern Hemisphere. That ribbon is mostly ocean, but it will require a fleet of more than 100 Loon balloons circling the globe, says Cassidy. “Maybe 90 percent of the time,” he says, “people in that ring will have at least one balloon overhead and be able to use it.”

Good signals

“It was just for some minutes, but it was wonderful,” says ­Silvana Pereira, a school principal in a rural area of northeastern Brazil. She’s thinking back to an unusual geography class last summer in which pupils at Linoca Gayoso Castelo Branco School could use the Internet thanks to a Loon balloon drifting, invisibly, high overhead. Internet service is nonexistent in the area, but that day’s lesson on Portugal was enhanced by Wikipedia and online maps. “They were so involved that the 45 minutes of a regular class wouldn’t be enough to satisfy their demand for knowledge,” says Pereira.

Her school is only around 100 kilometers from a metro area of more than one million people, but its location is too poor and sparsely populated for Brazil’s wireless carriers to invest in Internet infrastructure. Google’s goal is for Project Loon to change those economics. It should be possible to operate one Loon balloon for just hundreds of dollars per day, ­Cassidy says, and each one should be able to serve a few thousand connections at any time. The company won’t reveal how much it is spending to set all this up, or even how many people work on the project.

Cassidy is also confident that his balloons will be able to hold their own against Internet delivered by drones (both Google and Facebook are working on that) or satellites (an idea being pursued by SpaceX CEO Elon Musk). Those projects are less far along than Loon, and it’s expensive to build and power drones or launch satellites. “For quite some time, balloons will have a big cost advantage,” Cassidy says. Nevertheless, Google might be hedging its bets with more than just drones: in January it invested $900 million in SpaceX.


Among the upgrades Google is testing for its balloons (seen here from the rafters): using hydrogen, which is cheaper than helium, and having a motor move their solar panels to track the sun.
RC RIVERA
Technology is not the only thing keeping 4.3 billion people offline, though. For example, policies in India mandate that telecom companies provide coverage to poor as well as rich areas, but the government hasn’t enforced the rules, says Sunil Abraham, executive director of the Centre for Internet and Society, a think tank in Bangalore. He is also wary of Project Loon because of the way Google and other Western Internet companies have operated in developing countries in recent years. They have cut deals with telecoms in India and other countries to make it free to access their websites, disadvantaging local competitors. “Anyone coming with deep pockets and new technology I would welcome,” he says, but he adds that governments should fix up their patchy regulatory regimes first to ensure that everyone—not just Google and its partners—really does benefit.

Those working on Project Loon are confident the public good will be served. They seem as motivated by a desire to make people’s lives better as by Loon’s outlandish technology. Cassidy’s voice wavers with emotion when he thinks back to seeing the delight of Pereira’s pupils during their ­Internet-enabled geography lesson. “This is a way of changing the world,” he says.

by Tom Simonite",0.01-0.1%,"The article discusses Project Loon, an initiative by Google to provide internet access to remote and underserved areas using high-altitude balloons. The project aims to bring online connectivity to 4.3 billion people who lack it, with a focus on developing countries.

Google's engineers have been working on improving the design of the balloons and their payloads, which carry radio transceivers that communicate with antennas on the ground. They have also developed a new system for tracking and controlling the balloons using GPS and wind patterns.

One of the major challenges facing Project Loon is the fragility of the balloons themselves. To address this, Google has introduced changes to the design of the balloon envelope, including using thicker materials and changing the way they are manufactured. They have also developed a ""leak squad"" that uses helium detectors to identify tiny holes in the balloons.

The project has already shown promising results, with some balloons lasting over 130 days in flight. However, there are still many technical challenges to overcome before Project Loon can be scaled up globally.

Google plans to test its technology at a global scale in 2015, with a fleet of over 100 balloons circling the globe along a thin ribbon around the Southern Hemisphere. The company aims to provide ""quasi-continuous"" internet access to people in this area, which will require a significant investment in infrastructure and resources.

The article also highlights some of the potential social benefits of Project Loon, including improved education and healthcare outcomes for underserved communities. However, there are also concerns about the project's impact on local economies and regulatory frameworks.

Some experts have raised concerns that Project Loon could exacerbate existing inequalities by providing internet access only to those who can afford it. Others have pointed out that governments in developing countries may not be prepared to regulate the technology effectively, which could lead to unintended consequences.

Overall, Project Loon represents a significant investment in innovation and infrastructure by Google, with the potential to bring online connectivity to millions of people around the world. However, its success will depend on careful planning, execution, and regulation to ensure that it serves the public good.",Hardware,"The article discusses Google's Project Loon, an initiative aimed at providing internet access to remote and underserved areas using high-altitude balloons. The project has made significant progress since its inception, with improvements in balloon design, manufacturing, and electronics. However, challenges still remain, such as perfecting a way of making radio or laser connections between balloons.

The article highlights the potential impact of Project Loon on people's lives, particularly in rural areas where internet access is scarce. A school principal in Brazil shares her experience of using the internet via a Loon balloon during a geography lesson, which allowed students to access online resources and enhance their learning experience.

Google's goal is to make internet access more affordable and accessible to the 4.3 billion people currently offline. The company aims to operate each Loon balloon for just hundreds of dollars per day, making it a cost-effective solution compared to other technologies such as drones or satellites.

However, experts also raise concerns about the regulatory environment and the potential impact on local internet service providers. They argue that governments need to create a more level playing field to ensure that everyone benefits from initiatives like Project Loon.

Overall, the article suggests that Project Loon has the potential to make a significant difference in people's lives, particularly in areas where internet access is limited or non-existent."
Liquid Biopsy,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/liquid-biopsy/,"Liquid Biopsy
Fast DNA-sequencing machines are leading to simple blood tests for cancer.
by Michael Standaert archive page
February 18, 2015

JAVIER JAEN
Liquid Biopsy

Breakthrough
A blood test to catch cancer early.
Why it matters
Cancer kills some eight million people a year around the world.
Key players
Dennis Lo, Chinese University of Hong Kong; Illumina; Bert Vogelstein, Johns Hopkins
Everything about China is big, including its cancer problem. In some wealthier cities, like Beijing, cancer is now believed to be the most frequent killer. Air pollution, high rates of smoking, and notorious “cancer villages” scarred by industrial pollution are increasing death rates around the country. Liver cancer in particular is four times as prevalent as it is in the West, in part because one in 14 people in China carry hepatitis B, which puts them at risk. Of all the people worldwide who die of cancer each year, some 27 percent are Chinese.

In December, I traveled by metro from Shenzhen to Hong Kong. There I had arranged to meet Dennis Lo, a doctor who has worked for nearly 20 years on a technique called the “liquid biopsy,” which is meant to detect liver and other cancers very early—even before symptoms arise—by sequencing the DNA in a few drops of a person’s blood.

Lo appeared fastidiously dressed as usual in a sharp blazer, a habit that called to mind formal dinners at the University of Oxford, where he studied in the 1980s. He is well known for having been the first to show that a fetus sheds bits of its DNA into the bloodstream of its mother. That finding, first made in 1997, has in recent years led to a much safer, simpler screening test for Down syndrome. By now more than one million pregnant women have been tested.

Today Lo is competing with labs around the world to repeat that scientific and commercial success by developing cancer screening tests based on a simple blood draw. That’s possible because dying cancer cells also shed DNA into a person’s blood. Early on, the amount is vanishingly small—and obscured by the healthy DNA that also circulates. That makes it difficult to measure. But Lo says the objective is simple: an annual blood test that finds cancer while it’s curable.

Cancers detected at an advanced stage, when they are spreading, remain largely untreatable. In the United States, early detection is behind medicine’s most notable successes in applying technology to cut deaths from common cancers. Half of the steep decline in deaths from colorectal cancer is due to screening exams like colonoscopies.

Lo’s hospital is involved in two of the largest studies anywhere to prove that DNA analysis can also act as a screening test. The researchers are following a thousand people with hepatitis B to see if the DNA test can spot liver tumors before an ultrasound can. An even larger study is on nasopharyngeal carcinoma, a cancer that starts in the upper part of the throat. It’s rare elsewhere in the world, but in south China men have a one in 60 chance of contracting it in their ­lifetimes.

This cancer appears to be linked to eating salted fish, as well as to a genetic susceptibility among Chinese and to infection by the Epstein-Barr virus, the germ that causes mononucleosis. The role of the virus, says Lo, creates a special situation. The test he developed searches for easy-to-spot viral DNA that dying cancer cells release into a person’s plasma.

The study involves 20,000 healthy middle-aged men recruited in Hong Kong, and it’s halfway done. Among the first 10,000 men screened, the researchers picked up 17 cases of cancer—13 of those at stage I, the earliest kind. Nearly all these men have now beaten the cancer with radiation treatment. The typical survival rate is less than 70 percent if patients seek out a doctor only when they have the most advanced symptoms, like a mass in the neck. “They would normally be just walking on the street not knowing that there was a time bomb waiting to go off, and now we have alarmed them,” says Lo. As he sees it, every man in south China could be screened. One private hospital in Hong Kong has started offering the test already. “We believe it will save lives,” he says.

Lo’s lab is now locked in a technology race with scientists at other institutions, including Johns Hopkins University, to see if these ideas can turn into a general-purpose test for nearly any cancer, not only those involving a virus. The approach relies on gene-sequencing machines, which rapidly decode millions of short fragments of DNA that are loose in the bloodstream. The results are compared with the reference map of the human genome. Researchers can then spot the specific patterns of rearranged DNA that are telltale signs of a tumor.

Lo showed me several older sequencing machines during a tour of his laboratory, located at the Chinese University of Hong Kong. He says that the next generation of DNA sequencers, some no larger than a cell phone, could allow routine screening for cancer to become less expensive and far more widely used. For the time being, the cost of the DNA test being tried out on people at risk for liver cancer is still too high for routine use. Lo notes that the fetal tests were similarly expensive at first but that prices have since declined to as little as $800. That’s led to much wider use. “The same thing should happen [with cancer],” he says.

Building on the foundations put in place by doctors like Lo, commercial interest in liquid biopsies has recently started to explode. Eric Topol, a professor of genomics at the Scripps Research Institute, predicted this January that the technology, applied to cancer and other diseases, will become the “stethoscope for the next 200 years.” Jay Flatley, CEO of Illumina, the San Diego company that builds fast gene-­sequencing machines, told investors this year that the market for such tests could be worth at least $40 billion. Calling the technology “perhaps the most exciting breakthrough” in cancer diagnostics, he said his company would begin offering researchers a liquid-biopsy test kit to facilitate the search for signs of cancer.

In addition to screening for cancer, liquid biopsies could be a way to help people already fighting the disease. Doctors can pick a drug according to the specific DNA mutation driving a cancer forward. Tests to identify the mutation are sometimes done on tissue taken from a tumor, but a noninvasive blood test would be appropriate in more cases. Lo told me that 40 percent of Chinese lung cancer patients have a mutation in one gene, EGFR, that would make them eligible for new targeted drugs.

Cancer comes in many types, and Lo says that for each, researchers must methodically make their case that liquid biopsies can really save lives. He believes he’s close with nasopharyngeal cancer. “If you can screen and prognosticate in very common cancer types, that is the time when it will go mainstream,” he says.

by Michael Standaert",0.01% - 0.1%,Positive,Biotech,product
Megascale Desalination,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/megascale-desalination/,"Megascale Desalination
The world’s largest and cheapest reverse-osmosis desalination plant is up and running in Israel.
by David Talbot archive page
February 18, 2015

LUKE SHUMAN
On a Mediterranean beach 10 miles south of Tel Aviv, Israel, a vast new industrial facility hums around the clock. It is the world’s largest modern seawater desalination plant, providing 20 percent of the water consumed by the country’s households. Built for the Israeli government by Israel Desalination Enterprises, or IDE Technologies, at a cost of around $500 million, it uses a conventional desalination technology called reverse osmosis (RO). Thanks to a series of engineering and materials advances, however, it produces clean water from the sea cheaply and at a scale never before achieved.

Worldwide, some 700 million people don’t have access to enough clean water. In 10 years the number is expected to explode to 1.8 billion. In many places, squeezing fresh water from the ocean might be the only viable way to increase the supply.

The new plant in Israel, called Sorek, was finished in late 2013 but is just now ramping up to its full capacity; it will produce 627,000 cubic meters of water daily, providing evidence that such large desalination facilities are practical. Indeed, desalinated seawater is now a mainstay of the Israeli water supply. Whereas in 2004 the country relied entirely on groundwater and rain, it now has four seawater desalination plants running; Sorek is the largest. Those plants account for 40 percent of Israel’s water supply. By 2016, when additional plants will be running, some 50 percent of the country’s water is expected to come from desalination.

The traditional criticism of reverse-osmosis technology is that it costs too much. The process uses a great deal of energy to force salt water against polymer membranes that have pores small enough to let fresh water through while holding salt ions back. However, Sorek will profitably sell water to the Israeli water authority for 58 U.S. cents per cubic meter (1,000 liters, or about what one person in Israel uses per week), which is a lower price than today’s conventional desalination plants can manage. What’s more, its energy consumption is among the lowest in the world for large-scale desalination plants.


DATA ON CAPACITY FROM GWI DESALDATA/IDA; MAP DATA FROM ALCAMO ET AL. (2003), IPCC; DATA ON ISRAEL FROM ISRAEL WATER AUTHORITY; DATA ON ENERGY USE FROM UNITED NATIONS
Megascale Desalination

Breakthrough
Demonstrating that seawater desalination can cost-effectively provide a substantial portion of a nation’s water supply.
Why it matters
The world’s supplies of fresh water are inadequate to meet the needs of a growing population.
Key players
IDE Technologies; Poseidon Water; Desalitech; Evoqua
The Sorek plant incorporates a number of engineering improvements that make it more efficient than previous RO facilities. It is the first large desalination plant to use pressure tubes that are 16 inches in diameter rather than eight inches. The payoff is that it needs only a fourth as much piping and other hardware, slashing costs. The plant also has highly efficient pumps and energy recovery devices. “This is indeed the cheapest water from seawater desalination produced in the world,” says Raphael Semiat, a chemical engineer and desalination expert at the Israel Institute of Technology, or Technion, in Haifa. “We don’t have to fight over water, like we did in the past.” Australia, Singapore, and several countries in the Persian Gulf are already heavy users of seawater desalination, and California is also starting to embrace the technology (see “Desalination Out of Desperation”). Smaller-scale RO technologies that are energy-efficient and relatively cheap could also be deployed widely in regions with particularly acute water problems—even far from the sea, where brackish underground water could be tapped.

Earlier in development are advanced membranes made of atom-thick sheets of carbon, which hold the promise of further cutting the energy needs of desalination plants.

by David Talbot",0.01% - 0.1%,Positive,Climate tech,product
Apple Pay,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/apple-pay/,"Apple Pay
A clever combination of technologies makes it faster and more secure to buy things with a wave of your phone.
by Robert D. Hof archive page
February 18, 2015

SIMON LANDREIN
Apple Pay

Breakthrough
A service that makes it practical to use your smartphone as a wallet in everyday situations.
Why it matters
Credit card fraud damages the economy by raising the costs of goods and services.
Key players
Apple; Visa; MasterCard; Google
When Apple Pay was announced in September, Osama Bedier was unimpressed. A longtime PayPal executive who now runs a payment startup called Poynt, Bedier had spent more than two years leading Google’s mobile wallet service, which lets people use their phones to pay for things at checkout counters. It used some of the same technologies as Apple Pay and failed to catch on widely. So despite Apple Pay’s appealing promise—safe payment with just the press of a thumb on your iPhone—there was good reason to be skeptical of its chances, too.

Yet when Apple Pay launched just a few weeks later, Bedier was a convert. Poynt makes a new kind of payment terminal—one that retailers can use to accept Apple Pay—and the advent of the service helped send the company’s orders soaring. “Now merchants have people walking in saying, ‘Why can’t I use Apple Pay?’” he says at Poynt’s Palo Alto headquarters, whose lobby displays a 100-year-old National cash register, testament to the long history of payment technologies. Originally Bedier expected Poynt to sell 20,000 payment terminals in 2015, but after the launch of Apple Pay, he scrambled to find a new manufacturer in Taiwan that could handle far greater demand. “Apple Pay will touch off a rush to mobile payment,” he says.

Momentum for mobile payment technologies was building even before Apple Pay debuted last fall. Some 17 percent of all smartphone users reported making a point-of-sale payment with their phone in 2013, up from 6 percent in 2012, according to a U.S. Federal Reserve survey. In-person mobile payments in the United States more than doubled in 2014, to $3.7 billion, according to Forrester Research. Meanwhile, as services such as Uber and stores like Starbucks allow people to pay via mobile app, transactions that once brought out the wallet are disappearing into the phone, where they are faster and should be more secure. You can use your existing credit card accounts, but you never have to pull out the physical cards. “We know after people tap their phone to pay two or three times, they don’t go back to their old behavior,” says Ed McLaughlin, MasterCard’s executive in charge of new payment technologies.

But even if Apple didn’t invent mobile payments, it has significantly enhanced them. Just as Apple made it far easier to use a computer, listen to music, and communicate on the go, Apple Pay is all about doing the same for buying goods and services, online and off. Each financial innovation from the invention of money to the credit card reduced friction in commercial exchange and accelerated the pace of commerce. Apple Pay does that too: it marks the end of scrawling a signature, producing a driver’s license, and other hassles that came with earlier forms of payment. It’s also smoother than mobile services that came before it. Apple Pay works automatically when your phone is held up to the checkout terminal, with no need to open an app as you must to use Google Wallet or PayPal. Pressing your thumb to the phone eliminates the need to use a PIN, speeding the transaction. This is true no matter whether you’re booking a room on Airbnb or buying sandwiches at Subway. It fuses the virtual and physical worlds of commerce in a way that no other payment system has done.

None of the individual technologies is novel, but Apple turned them into a service that is demonstrably easier than any other.
That doesn’t mean most of us will be ditching our wallets and waving phones in every store in 2015—far from it. The $3.7 billion worth of mobile payments made in U.S. stores last year was just a drop in the $4 trillion bucket of consumer retail spending. Beyond that, an additional $12 trillion was spent on services. Apple Pay itself faces a raft of challenges, too, and not just from rival wallets offered by Google, PayPal, retailers, and wireless carriers. Currently only people with the new iPhone 6 can use Apple Pay in stores. It’s officially available only in the United States for now, but 98 percent of U.S. stores lack the right checkout terminals to accept it. Finally, Apple Pay is far from replacing some of the things in a physical wallet—in particular, popular store rewards cards. Starbucks’s app, which is a combination store locator, rewards card, and payment engine all in one, still accounts for the majority of all mobile payments in retail stores.

Still, Apple has done a lot of things right, suggesting that Apple Pay will turn out to be a milestone. Even if it is only a moderate success for Apple, it seems certain to be a driver of mobile payments in general. None of the individual technologies in it is novel, but the extent of Apple’s control over both the software and the hardware in the iPhone—which exceeds what Google can do for Google Wallet even on Android phones—allowed it to combine those technologies into a service demonstrably easier to use than any other.

As a result, Apple is now cementing standards for the payment industry. Merchants have been debating whether bar codes or the radio technology near-field communication (NFC), for instance, should be the method that a phone uses to relay payment information when you wave it at a checkout terminal. Apple’s choice to build NFC into iPhones means many stores will feel compelled to get terminals with NFC support if they want to maximize their appeal to millions of iPhone owners.

Likewise, Apple Pay is setting the pace in payment security, outdoing credit cards with multiple layers of protection (see “Tighter Security,” at left). The phone doesn’t store real card numbers, and even the merchant doesn’t see them, let alone keep them in the databases that hackers routinely plunder. Each transaction generates a unique code that can be used only once. The capper: the payment is triggered with Touch ID, which responds only to the owner’s fingerprint. This level of fraud protection is one reason banks representing 90 percent of U.S. consumer payments support Apple Pay, says Avin Arumugam, head of next-generation payment products at JP Morgan Chase.


Most of all, Apple’s timing is impeccable. Card networks have set an October 2015 deadline for merchants to upgrade to terminals that can take credit cards with embedded chips for security—after that date, the merchants who don’t upgrade will have to eat fraudulent charges. Most of those terminals they’ll need to install will have NFC built in. Although that upgrade cycle will take years to reach most stores, Apple Pay could speed it up, says Keith Rabois, a former executive at PayPal and Square and an investor in several payment startups. “Apple Pay removes most of the barriers to adoption of mobile payment,” he says.

Already, Apple Pay has taken off more quickly than Google Wallet or any other mobile payment system to date. “The time was ripe for Apple,” says Jason Buechel, chief information officer at Whole Foods Market, where almost 2 percent of store sales were coming in through Apple Pay by mid-January. McDonald’s said Apple Pay was accounting for half its mobile-phone transactions, and Walgreens’s mobile payments doubled after Apple Pay debuted. Some 60 percent of customers used it on multiple days in November, using it three times as frequently as new PayPal customers used that system in the same time period, according to a study by the brokerage firm Investment Technology Group.

Apple stands to gain big if Apple Pay’s momentum continues. Not from the 0.15 percent of each transaction that it charges card-issuing banks: those fees would bring in only $2.5 billion by 2017 even if the new system got an unexpectedly large 30 percent share of U.S. credit and debit card expenditures, according to one estimate by investor Carl Icahn. That’s a tiny fraction of Apple’s fiscal 2014 revenue of $183 billion. The bigger impact will be ensuring the iPhone’s appeal. Once you’re using Apple Pay every day, in addition to other Apple services like iCloud and iTunes, you may think thrice before switching to an Android.

For all the focus on Apple Pay in retail stores, its biggest opportunity in the next few years will probably be greasing payments for countless apps and services. When you take a ride with Uber, the payment happens almost invisibly, without friction. Rabois suggests that Apple Pay could bring that level of ease to thousands of on-demand services in transportation, food delivery, and more. Once people get used to making app payments with a touch, they’ll start expecting to do the same everywhere else they can.

by Robert D. Hof",0.01% - 0.1%,Positive,Software,"The article discusses the impact of Apple Pay, a mobile payment system introduced by Apple, on the retail industry. Despite its relatively small market share (around 1% of all consumer spending in stores), Apple Pay is considered a milestone that will drive the adoption of mobile payments.

Apple has done several things right with Apple Pay:

* Combined existing technologies to create a service that is demonstrably easier to use than any other.
* Built NFC into iPhones, which means many stores will feel compelled to get terminals with NFC support to maximize their appeal to iPhone owners.
* Set a high standard for payment security, outdoing credit cards with multiple layers of protection.

The article highlights several challenges faced by Apple Pay, including:

* Limited availability (only on the new iPhone 6 and only in the US).
* Lack of widespread adoption among merchants (98% of stores lack the right checkout terminals to accept it).

Despite these challenges, Apple Pay has taken off more quickly than other mobile payment systems. The article cites several examples of retailers that have seen significant growth in mobile payments since the introduction of Apple Pay.

The article suggests that Apple's timing is impeccable, as card networks have set an October 2015 deadline for merchants to upgrade to terminals that can take credit cards with embedded chips for security. This will lead to a wider adoption of NFC-enabled terminals, which will support Apple Pay.

The potential impact on Apple's business model is significant:

* While the transaction fees are small (0.15% of each transaction), they could add up to $2.5 billion by 2017 if Apple Pay achieves an unexpectedly large share of US credit and debit card expenditures.
* More importantly, Apple Pay will ensure the iPhone's appeal, making it more likely that customers will stick with the device.

The article concludes that Apple Pay's biggest opportunity in the next few years will be greasing payments for countless apps and services. The author suggests that Apple Pay could bring the same level of ease to thousands of on-demand services in transportation, food delivery, and more."
Brain Organoids,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/brain-organoids/,"Brain Organoids
A new method for growing human brain cells could unlock the mysteries of dementia, mental illness, and other neurological disorders.
by Russ Juskalian archive page
February 18, 2015
Organoids in a dish.
Organoids in a dish.
REGINA HUEGL
Brain Organoids

Breakthrough
Three-dimensional clusters of living neurons that can be grown in a lab from human stem cells.
Why it matters
Researchers need new ways of understanding brain disorders and testing possible treatments.
Key players
Madeline Lancaster and Jürgen Knoblich, Institute of Molecular Biotechnology; Rudolph Tanzi and Doo Yeon Kim, Massachusetts General Hospital
As Madeline Lancaster lifts a clear plastic dish into the light, roughly a dozen clumps of tissue the size of small baroque pearls bob in a peach-­colored liquid. These are cerebral organoids, which possess certain features of a human brain in the first trimester of development—including lobes of cortex. The bundles of human tissue are not exactly “brains growing in a dish,” as they’re sometimes called. But they do open a new window into how neurons grow and function, and they could change our understanding of everything from basic brain activities to the causes of schizophrenia and autism.

Before it grows in one of Lancaster’s dishes, a brain organoid begins as a single skin cell taken from an adult. With the right biochemical prodding, that cell can be turned into an induced pluripotent stem cell (the kind that can mature into one of several types of cells) and then into a neuron. This makes it possible to do things that were impossible before. Now scientists can directly see how networks of living human brain cells develop and function, and how they’re affected by various drug compounds or genetic modifications. And because these mini-brains can be grown from a specific person’s cells, organoids could serve as unprecedentedly accurate models for a wide range of diseases. What goes wrong, for example, in neurons derived directly from someone with Alzheimer’s disease?

The prospect of finding answers to such questions is leading pharmaceutical companies and academic researchers to seek collaborations with Lancaster and Jürgen Knoblich, whose lab at the Institute of Molecular Biotechnology (IMBA) in Vienna, Austria, is where Lancaster developed the organoids as a postdoc. The first of these collaborations was an investigation of microcephaly, a disorder characterized by small brain size, with Andrew Jackson of the University of Edinburgh. Using cells derived from a patient with microcephaly, the team cultured organoids that shared characteristics with the patient’s brain. Then the researchers replaced a defective protein associated with the disorder and were able to culture organoids that appeared partially cured.


Top: Madeline Lancaster figured out a way to keep neurons growing in a dish until they develop characteristics of living human brains.

Middle: Magdalena Renner, a graduate student in the lab, examines organoids under a microscope.

Bottom: A variety of organoids are kept alive on a shaker plate in an incubator.
REGINA HUEGL
This is just the beginning, says Lancaster. Researchers such as Rudolph Jaenisch at MIT and Guo-li Ming at Johns Hopkins are beginning to use brain organoids to investigate autism, schizophrenia, and epilepsy. What makes cerebral organoids particularly useful is that their growth mirrors aspects of human brain development. The cells divide, take on the characteristics of, say, the cerebellum, cluster together in layers, and start to look like the discrete three-dimensional structures of a brain. If something goes wrong along the way—which is observable as the organoids grow—scientists can look for potential causes, mechanisms, and even drug treatments.

The breakthrough in creating these organoids happened as part of a side project. Other researchers had grown neurons in a dish before, and like them, Lancaster started by using a flat plate to “play” with neural stem cells—the kind that form into neurons and other cells in the nervous system. Sometimes, she says, “I’d get neural stem cells that wouldn’t really stay in 2-D, and they would kind of fall off the plate and they’d make 3-D clumps—and rather than ignoring them or throwing them away, I thought, ‘Those are cool—let’s see what happens if I let them keep growing.’” But there was a major challenge: how to keep the tissue at the center of the organoids fed without the benefit of veins. ­Lancaster’s solution was to encapsulate each organoid in a matrix known to nurture cells, put a dozen of these blobs in a nutritious bath, and shake or spin it all to keep the organoids awash in cellular food.

Since publishing her method, Lancaster has pushed the brain tissue to further levels of complexity with neurons at later stages of development. The number of possible applications grows with each advance. Most tantalizing to Lancaster herself is the prospect that cerebral organoids might solve the deepest of mysteries: what happens in our brains to set us apart from other animals? “I’m mainly interested,” she says, “in figuring out what it is that makes us human.”

by Russ Juskalian",0.001-0.01%,Positive,Biotech,methodology
Supercharged Photosynthesis,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/supercharged-photosynthesis/,"Supercharged Photosynthesis
Advanced genetic tools could help boost crop yields and feed billions more people.
by Kevin Bullis archive page
February 18, 2015

LUKE SHUMAN
In December, geneticists announced that they’d made a major advance in engineering rice plants to carry out photosynthesis in a more efficient way—much as corn and many fast-growing weeds do. The advance, by a consortium of 12 laboratories in eight countries, removes a big obstacle from scientists’ efforts to dramatically increase the production of rice and, potentially, wheat. It comes at a time when yields of those two crops, which together feed nearly 40 percent of the world, are dangerously leveling off, making it increasingly difficult to meet rapidly growing food demand.

The supercharged process, called C4 photosynthesis, boosts plants’ growth by capturing carbon dioxide and concentrating it in specialized cells in the leaves. That allows the photosynthetic process to operate much more efficiently. It’s the reason corn and sugarcane grow so productively; if C4 rice ever comes about, it will tower over conventional rice within a few weeks of planting. Researchers calculate that engineering C4 photosynthesis into rice and wheat could increase yields per hectare by roughly 50 percent; alternatively, it would be possible to use far less water and fertilizer to produce the same amount of food.

The December results, achieved by the C4 consortium and led by Paul Quick at the International Rice Research Institute (IRRl) in the Philippines, introduced key C4 photosynthesis genes into a rice plant and showed that it carried out a rudimentary version of the supercharged photosynthesis process. “It’s the first time we’ve seen evidence of the C4 cycle in rice, so it’s very exciting,” says Thomas Brutnell, a researcher at the Danforth Plant Science Center in St. Louis. Brutnell is part of the C4 Rice Consortium headed by IRRI, which has funding from the Bill & Melinda Gates Foundation, but was not directly involved in the most recent breakthrough.


DATA SOURCES: FOOD AND AGRICULTURE ORGANIZATION OF THE UNITED NATIONS; CORNELL UNIVERSITY; INTERNATIONAL RICE RESEARCH INSTITUTE; PATRICIO GRASSINI, ET AL., NATURE COMMUNICATIONS; DEEPAK RAY ET AL., PLOSONE
Supercharged Photosynthesis

Breakthrough
Engineering rice plants to extract energy from sunlight far more efficiently than they do now.
Why it matters
Crop yields aren’t increasing fast enough to keep up with demand from a growing population.
Key players
Paul Quick, International Rice Research Institute; Daniel Voytas, University of Minnesota; Julian Hibberd, University of Cambridge; Susanne von Caemmerer, Australian National University
Despite the genetic changes, the altered rice plants still rely primarily on their usual form of photosynthesis. To get them to switch over completely, researchers need to engineer the plants to produce specialized cells in a precise arrangement: one set of cells to capture the carbon dioxide, surrounding another set of cells that concentrate it. That’s the distinctive wreath anatomy found in the leaves of C4 plants. However, scientists still don’t know all the genes involved in producing these cells and suspect that they could number in the dozens.

New genome editing methods that allow scientists to precisely modify parts of plant genomes could help solve the problem. Using conventional breeding to manipulate more than one or two genes is a “nightmare,” Brutnell says, let alone trying to engineer a plant with dozens of gene changes. Genome editing could make it possible to change a large number of genes easily. Says Brutnell: “Now we have the toolbox to go after this.”

It can be a decade or more before even simple crop modifications reach farmers, let alone changes as complex as reëngineering how plants carry out photosynthesis. But once scientists solve the C4 puzzle in a plant such as rice, they hope, the method can be extended to dramatically increase production of many other crops, including wheat, potatoes, tomatoes, apples, and soybeans.

by Kevin Bullis",0.01% - 0.1%,positive,Biotech,project
Internet of DNA,2015.0,https://www.technologyreview.com/10-breakthrough-technologies/2015/,https://www.technologyreview.com/technology/internet-of-dna/,"Internet of DNA
A global network of millions of genomes could be medicine’s next great advance.
by Antonio Regalado archive page
February 18, 2015

DADU SHIN
Internet of DNA

Breakthrough
Technical standards that let DNA databases communicate.
Why it matters
Your medical treatment could benefit from the experiences of millions of others.
Key players
Global Alliance for Genomics and Health; Google; Personal Genome Project
Noah is a six-year-old suffering from a disorder without a name. This year, his physicians will begin sending his genetic information across the Internet to see if there’s anyone, anywhere, in the world like him.

A match could make a difference. Noah is developmentally delayed, uses a walker, speaks only a few words. And he’s getting sicker. MRIs show that his cerebellum is shrinking. His DNA was analyzed by medical geneticists at the Children’s Hospital of Eastern Ontario. Somewhere in the millions of As, Gs, Cs, and Ts is a misspelling, and maybe the clue to a treatment. But unless they find a second child with the same symptoms, and a similar DNA error, his doctors can’t zero in on which mistake in Noah’s genes is the crucial one.

In January, programmers in Toronto began testing a system for trading genetic information with other hospitals. These facilities, in locations including Miami, Baltimore, and Cambridge, U.K., also treat children with so-called ­Mendelian disorders, which are caused by a rare mutation in a single gene. The system, called MatchMaker Exchange, represents something new: a way to automate the comparison of DNA from sick people around the world.

One of the people behind this project is David Haussler, a bioinformatics expert based at the University of California, Santa Cruz. The problem Haussler is grappling with now is that genome sequencing is largely detached from our greatest tool for sharing information: the Internet. That’s unfortunate because more than 200,000 people have already had their genomes sequenced, a number certain to rise into the millions in years ahead. The next era of medicine depends on large-scale comparisons of these genomes, a task for which he thinks scientists are poorly prepared. “I can use my credit card anywhere in the world, but biomedical data just isn’t on the Internet,” he says. “It’s all incomplete and locked down.” Genomes often get moved around in hard drives and delivered by FedEx trucks.

Haussler is a founder and one of the technical leaders of the Global Alliance for Genomics and Health, a nonprofit organization formed in 2013 that compares itself to the W3C, the standards organization devoted to making sure the Web functions correctly. Also known by its unwieldy acronym, GA4GH, it’s gained a large membership, including major technology companies like Google. Its products so far include protocols, application programming interfaces (APIs), and improved file formats for moving DNA around the Web. But the real problems it is solving are mostly not technical. Instead, they are sociological: scientists are reluctant to share genetic data, and because of privacy rules, it’s considered legally risky to put people’s genomes on the Internet.

But pressure is building to use technology to study many, many genomes at once and begin to compare that genetic information with medical records. That is because scientists think they’ll need to sort through a million genomes or more to solve cases—like Noah’s—that could involve a single rogue DNA letter, or to make discoveries about the genetics of common diseases that involve a complex combination of genes. No single academic center currently has access to information that extensive, or the financial means to assemble it.

The unfolding calamity in genomics is that a great deal of life-saving information, though already collected, is inaccessible.
Haussler and others at the alliance are betting that part of the solution is a peer-to-peer computer network that can unite widely dispersed data. Their standards, for instance, would permit a researcher to send queries to other hospitals, which could choose what level of information they were willing to share and with whom. This control could ease privacy concerns. Adding a new level of complexity, the APIs could also call on databases to perform calculations—say, to reanalyze the genomes they store—and return answers.

The day I met Haussler, he was wearing a faded ­Hawaiian shirt and taking meetings on a plastic lawn chair by a hotel pool in San Diego. Both of us were there to attend one of the world’s largest annual gatherings of geneticists. He told me he was worried that genomics was drifting away from the open approach that had made the genome project so powerful. If people’s DNA data is made more widely accessible, Haussler hopes, medicine may benefit from the same kind of “network effect” that’s propelled so many commercial aspects of the Web. The alternative is that this vital information will end up marooned in something like the disastrous hodgepodge of hospital record systems in the United States, few of which can share information.

One argument for quick action is that the amount of genome data is exploding. The largest labs can now sequence human genomes to a high polish at the pace of two per hour. (The first genome took about 13 years.) Back-of-the-envelope calculations suggest that fast machines for DNA sequencing will be capable of producing 85 petabytes of data this year worldwide, twice that much in 2019, and so on. For comparison, all the master copies of movies held by Netflix take up 2.6 petabytes of storage.

“This is a technical question,” says Adam Berrey, CEO of Curoverse, a Boston startup that is using the alliance’s standards in developing open-source software for hospitals. “You have what will be exabytes of data around the world that nobody wants to move. So how do you query it all together, at once? The answer is instead of moving the data around, you move the questions around. No industry does that. It’s an insanely hard problem, but it has the potential to be transformative to human life.”

Today scientists are broadly engaged in what is, in effect, a project to document every variation in every human gene and determine what the consequences of those differences are. Individual human beings differ at about three million DNA positions, or one in every 1,000 genetic letters. Most of these differences don’t matter, but the rest explain many things that do: heartbreaking disorders like Noah’s, for example, or a higher than average chance of developing glaucoma.

So imagine that in the near future, you had the bad luck to develop cancer. A doctor might order DNA tests on your tumor, knowing that every cancer is propelled by specific mutations. If it were feasible to look up the experience of everyone else who shared your tumor’s particular mutations, as well as what drugs those people took and how long they lived, that doctor might have a good idea of how to treat you. The unfolding calamity in genomics is that a great deal of this life-saving information, though already collected, is inaccessible. “The limiting factor is not the technology,” says David Shaywitz, chief medical officer of DNAnexus, a bioinformatics company that hosts several large collections of gene data. “It’s whether people are willing.”

Last summer Haussler’s alliance launched a basic search engine for DNA, which it calls Beacon. Currently, Beacon searches through about 20 databases of human genomes that were previously made public and have implemented the alliance’s protocols. Beacon offers only yes-or-no answers to a single type of question. You can ask, for instance, “Do any of your genomes have a T at position 1,520,301 on chromosome 1?” “It’s really just the most basic question there is: have you ever seen this variant?” says Haussler. “Because if you did see something new, you might want to know, is this the first patient in the world that has this?” Beacon is already able to access the DNA of thousands of people, including hundreds of genomes put online by Google.

One of the cofounders of the Global Alliance is David ­Altshuler, who is now head of science at Vertex Pharmaceuticals but until recently was deputy chief of the MIT-Harvard Broad Institute, one of the largest academic DNA-sequencing centers in the United States. The day I visited Altshuler in his Broad office, his whiteboard was covered with diagrams showing genetic inheritance in families, as well the word “Napster” written in large blue letters—a reference to the famously disruptive music-sharing service of the 1990s. Altshuler has his own reasons for wanting to connect massive amounts of genetic data. As an academic researcher, he hunted for the genetic causes of common diseases like diabetes. That work was carried out by comparing the DNA of afflicted and unafflicted people, trying to spot the differences that come up most often. After burning through countless research grants this way, geneticists realized there would be no easy answers, no common “diabetes genes” or “depression genes.” It turns out that common diseases aren’t caused by single, smoking-gun defects. Instead, a person’s risk, scientists have learned, is determined by a combination of hundreds, if not tens of thousands, of rare variations in the DNA code.

That’s created a huge statistical headache. Last July, in a report listing 300 authors, Broad looked at the genes of 36,989 people with schizophrenia. Even though schizophrenia is highly heritable, the 108 gene regions identified by the scientists explained only a small percentage of a person’s risk for the disease. Altshuler believes that big gene studies are still a good way to “crack” these illnesses, but he thinks it will probably take millions of genomes to do it.

The way the math works out, sharing data no longer looks optional, whether researchers are trying to unravel the causes of common diseases or ultra-rare ones. “There’s going to be an enormous change in how science is done, and it’s only because the signal-to-noise ratio necessitates it,” says Arthur Toga, a researcher who leads a consortium studying the science of Alzheimer’s at the University of Southern California. “You can’t get your result with just 10,000 patients—you are going to need more. Scientists will share now because they have to.”

Privacy, of course, is an obstacle to sharing. People’s DNA data is protected because it can identify them, like a fingerprint—and their medical records are private too. Some countries don’t permit personal information to be exported for research. But Haussler thinks a peer-to-peer network can sidestep some of these worries, since the data won’t move and access to it can be gated. More than half of Europeans and Americans say they’re comfortable with the idea of sharing their genomes, and some researchers believe patient consent forms should be dynamic, a bit like Facebook’s privacy controls, letting individuals decide what they’ll share and with whom—and then change their minds. “Our members want to be the ones to decide, but they aren’t that worried about privacy. They’re sick,” says Sharon Terry, head of the Genetic Alliance, a large patient advocacy organization.

The risk of not getting data sharing right is that the genome revolution could sputter. Some researchers say they are seeing signs that it’s happening already. Kym Boycott, head of the research team that sequenced Noah’s genome, says that when the group adopted sequencing as a research tool in 2010, it met with immediate success. Over two years, between 2011 and 2013, a network of Canadian geneticists uncovered the precise molecular causes of 146 conditions, solving 55 percent of their undiagnosed cases.

But the success rate appears to be tailing off, says ­Boycott. Now it’s the tougher cases like Noah’s that are left, and they are getting solved only half as often as the others. “We don’t have two patients with the same thing anymore. That’s why we need the exchange,” she says. “We need more patients and systematic sharing to get the [success rate] back up.” In late January, when I asked if MatchMaker Exchange had yielded any matches yet, she demurred, saying that it could be a matter of weeks before the software was fully operational. As for Noah, she said, “We are still waiting to sort him out. It’s important for this little guy.”

by Antonio Regalado",<0.01% - <1%,"The article discusses the importance of sharing genetic data in order to advance our understanding of human diseases and develop new treatments. The Global Alliance for Genomics and Health is a coalition of organizations working together to create a framework for responsible sharing of genomic data. One of the key challenges is balancing the need for data sharing with concerns about patient privacy.

The article highlights several initiatives aimed at facilitating data sharing, including:

1. The Beacon Project: A peer-to-peer network that allows researchers to query genomes without having to share or move the actual data.
2. MatchMaker Exchange: A platform that enables researchers to match rare genetic variants with similar cases from around the world.

The article also quotes several experts in the field, including David Altshuler, who emphasizes the need for large-scale data sharing in order to crack the genetic code of complex diseases like schizophrenia and Alzheimer's.

Some key points made by the experts include:

* The current approach to studying genetics is not working, and a new paradigm is needed.
* Sharing data is no longer optional; it's necessary to advance our understanding of human disease.
* Patient consent forms should be dynamic, allowing individuals to decide what they want to share and with whom.
* The risk of not getting data sharing right is that the genome revolution could sputter.

The article also mentions a specific case study of a young boy named Noah, who has a rare genetic disorder. Researchers are using MatchMaker Exchange to try to find other cases like Noah's in order to better understand his condition and develop new treatments.

Overall, the article highlights the importance of data sharing in advancing our understanding of human disease and developing new treatments. While there are challenges to be addressed, including concerns about patient privacy, experts agree that responsible data sharing is essential for progress in this field.",Biotech,"The article discusses the importance of sharing genetic data in order to advance our understanding of human diseases and develop new treatments. The Global Alliance for Genomics and Health is a coalition of researchers, clinicians, and patients that aims to create a platform for sharing genomic and clinical data.

One of the key challenges in genomics research is identifying the genetic causes of rare diseases. Currently, it can take years or even decades to find a second patient with the same condition, making it difficult to confirm the diagnosis and develop effective treatments.

The article highlights several initiatives that aim to address this challenge by creating platforms for sharing genomic data. For example, the MatchMaker Exchange is a web-based platform that allows researchers to share genetic data from patients with rare diseases in order to find matches and identify potential causes.

Another initiative mentioned in the article is the Beacon project, which is a peer-to-peer network that allows researchers to query large datasets of genomic information without having to physically access the data. This approach can help to reduce concerns about privacy and data protection.

The article also notes that some countries have laws that restrict the export of personal genetic data for research purposes. However, researchers believe that with proper safeguards in place, sharing genomic data can be done in a way that respects patient privacy while also advancing medical knowledge.

Overall, the article argues that sharing genetic data is essential for advancing our understanding of human diseases and developing new treatments. While there are challenges to overcome, initiatives like the Global Alliance for Genomics and Health and the MatchMaker Exchange are helping to make progress in this area.

Some key points from the article include:

* The Global Alliance for Genomics and Health aims to create a platform for sharing genomic and clinical data.
* Sharing genetic data is essential for identifying the causes of rare diseases and developing new treatments.
* Initiatives like the MatchMaker Exchange and Beacon are helping to make it easier for researchers to share and access genomic data.
* Concerns about privacy and data protection must be addressed in order to facilitate the sharing of genomic data.
* Some countries have laws that restrict the export of personal genetic data for research purposes.

Some potential questions and topics for discussion based on this article include:

* What are some of the benefits and challenges of sharing genetic data?
* How can researchers balance the need to share genomic data with concerns about patient privacy and data protection?
* What role do initiatives like the Global Alliance for Genomics and Health play in facilitating the sharing of genomic data?
* How might advances in genomics research impact our understanding of human diseases and the development of new treatments?"
Immune Engineering,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/immune-engineering/,"Immune Engineering
Genetically engineered immune cells are saving the lives of cancer patients. That may be just the start.
by Antonio Regalado archive page
February 23, 2016

JON HAN
The doctors looking at Layla Richards saw a little girl with leukemia bubbling in her veins. She’d had bags and bags of chemotherapy and a bone marrow transplant. But the cancer still thrived. By last June, the 12-month-old was desperately ill. Her parents begged—wasn’t there anything?

There was. In a freezer at her hospital—Great Ormond Street, in London—sat a vial of white blood cells. The cells had been genetically altered to hunt and destroy leukemia, but the hospital hadn’t yet sought permission to test them. They were the most extensively engineered cells ever proposed as a therapy, with a total of four genetic changes, two of them introduced by the new technique of genome editing.

Where the technology stands, it's a pretty radical treatment.
Soon a doctor from Great Ormond was on the phone to Cellectis, a biotechnology company with French roots that is now located on the East Side of Manhattan. The company owned the cancer treatment, which it had devised using a gene-editing method called TALENs, a way of making cuts and fixes to DNA in living cells. “We got a call. The doctors said, ‘We’ve got a girl who is out of T cells and out of options,’” André Choulika, the CEO of Cellectis, remembers. “They wanted one of the vials made during quality-control testing.”

The doctors hoped to make Layla a “special,” a patient who got the drug outside a clinical trial. It was a gamble, since the treatment had been tried only in mice. If it failed, the company’s stock and reputation could tank, and even if it succeeded, the company might get in trouble with regulators. “It was saving a life versus the chance of bad news,” Choulika says.

Immune Engineering

Breakthrough
Killer T cells programmed to wipe out cancer.
Why It Matters
Cancer, multiple sclerosis, and HIV could all be treated by engineering the immune system.
Key Players in Immune Therapies
- Cellectis 
- Juno Therapeutics 
- Novartis
Cellectis began developing the treatment in 2011 after doctors in New York and Philadelphia reported that they’d found a way to gain control over T cells, the so-called killer cells of the immune system. They had shown that they could take T cells from a person’s bloodstream and, using a virus, add new DNA instructions to aim them at the type of blood cell that goes awry in leukemia. The technique has now been tested in more than 300 patients, with spectacular results, often resulting in complete remission. A dozen drug firms and biotechnology companies are now working to bring such a treatment to market.

The T cells created by Cellectis could have even broader applications. The previous treatments use a person’s own cells. But some patients, especially small children like Layla, don’t have enough T cells.


Foreseeing this problem, Cellectis had set out to use gene editing to create a more highly engineered but ultimately simpler “universal” supply of T cells made from the blood of donors. The company would still add the new DNA, but it would also use gene editing to delete the receptor that T cells normally use to sniff out foreign-looking molecules.

“The T cell has a huge potential for killing. But the thing you can’t do is inject T cells from Mr. X into Mr. Y,” Choulika says. “They’d recognize Mr. Y as ‘non-self’ and start firing off at everything, and the patient will melt down.” But if the T cells are stripped down with gene editing, like the ones that were sitting in Great Ormond’s freezer, that risk is mostly eliminated. Or so everyone hoped.

In November, Great Ormond announced that Layla was cured. The British press jumped on the heartwarming story of a brave kid and daring doctors. Accounts splashed on front pages sent Cellectis’s stock price shooting upward. Two weeks later, the drug companies Pfizer and Servier announced they would ante up $40 million to purchase rights to the treatment. 

Although many of the details of Layla’s case have yet to be disclosed, and some cancer experts say the role of the engineered T cells in her cure remains murky, her recovery pointed a spotlight on “immune engineering,” and on the way that advances in controlling and manipulating the immune system are leading to unexpected breakthroughs in cancer treatment. They also could lead to new treatments for HIV and autoimmune diseases like arthritis and multiple sclerosis.

Known killer

The human immune system has been called nature’s “weapon of mass destruction.” It has a dozen major cell types, including several kinds of T cells. It defends against viruses it’s never seen before, suppresses cancer (though not always), and for the most part manages to avoid harming the body’s own tissue. It even has a memory, which is the basis of all vaccines.

More than 100 years ago, the American surgeon William Coley observed that an unexpected infection could sometimes make a tumor evaporate. Subsequently, Coley injected streptococcal cultures into cancer patients and saw the tumors shrink in some cases. The finding, published in 1893, showed the immune system could confront and fight cancer—but how did it work? Until recently, the answers weren’t known, and cancer immunotherapy was seen as a failed idea. 

But scientists have gradually mapped the network of molecules that govern how the immune system interacts with a tumor. And over the last few years, these insights have allowed drug companies and labs to start tinkering with the immune system’s behavior. “From 40 years and more of science, we know the general nature of the conversation between the tumor cells and the immune system,” says Philip Sharp, a biologist at MIT’s Koch Institute for Integrative Cancer Research and a recipient of the 1993 Nobel Prize in medicine. “That’s the conversation we’re trying to join in order to have a therapeutic effect. We are still at the level of a five-year-old kid. We know there are nouns, and that there are verbs. But the diversity of the vocabulary is still being mapped out.”

T cells can crawl, sense things, and even kill other cells. They’re little robots.
The most extreme of these proposals is to change the genetic instructions inside the T cell itself, something that’s become much easier using gene-editing methods like TALENs and the even newer CRISPR. Last year, the gene-editing startups Editas Medicine and Intellia Therapeutics each struck deals with companies developing T-cell-based therapeutics. “It’s the perfect setup,” says Jeffrey Bluestone, a researcher at the University of California, San Francisco. “Immune cells are machines that work pretty well, but we can make them work even better.”

Researchers are building on decades of research (and several Nobel Prizes involving immunology) that worked out many important details, including how T cells recognize invaders and go in for the kill. Seen through a microscope, these cells display almost animal--like behavior: they crawl, probe, then grab another cell and shoot it full of toxic granules. “What’s exciting is they have the ability to move all around; they’re autonomous,” says Wendell Lim, a synthetic biologist who is also at UCSF. “Immune cells talk to other cells, they deliver poisons, they can change what happens in a microenvironment, they have a memory, and they make more of themselves. I think of them as little robots.”


Lim is now breaking new ground in what he calls “synthetic immunology.” This year and last, he produced some futuristic T cells. Tested only in mice so far, the cells deploy their targeted search-and-kill behavior only if a specific drug is added—a feature that could be used to turn the cells on at specific places and times, which Lim calls “remote control.” Another T cell he designed is a two-stage affair, which kills only if it locates not one but two different markers on a cancer cell; it is like a dual authentication method for the enemy cell. Lim thinks of it as a sensing circuit or “advanced Google search.”

Such work is critical because targeting T cells to tumors of the liver, lung, or brain is dangerous, and some patients have been killed in trials. The problem has been friendly fire. So far, easy ways to target only cancer cells are lacking. Lim has founded his own startup, Cell Design Labs, to commercialize his engineering ideas. He declined to say how much money he has raised, but he says everyone working with T cells is stunned by the kind of money being thrown at the idea. “It’s a ‘wow’ type of situation,” he says.

Googling cures

The search to expand immune therapy now involves not only the world’s largest drug companies but also tech firms. Sharp says that last year Google held two summits at MIT of top immune oncologists and bioengineers to determine what parts of the problem could be “Googlified.” Attendees say the search giant paid special attention to new research techniques that fingerprint cells from a tumor biopsy in rapid-fire fashion. These methods might generate big data about what immune system cells are actually doing inside a tumor, and new clues about how to influence them. So far, Google’s life science unit, named Verily, hasn’t revealed its plans in cancer immunotherapy. But in New York’s Union Square, I met Jeffrey Hammerbacher, a former Facebook employee who now runs a lab that is part of Mount Sinai, the hospital and medical school. With 12 programmers in a light-soaked loft—the nearest thing to blood and guts is a photo of an exhausted surgeon on the wall—he’s also spending time on T cells. He’s developing software to interpret the DNA sequence in a patient’s cancer and predict from it how to goose the response of killer T cells.

A clinical trial by Mount Sinai should start this year. The patients receive a dose of abnormal protein fragments that Hammerbacher’s software predicts will train T cells to attack the cancer. “What was fun was that what we submitted to the [U.S. Food and Drug Administration] was not a molecule but an algorithm,” he says. “It might be one of the first times the output of a program is the therapy.”

In January, Juno Therapeutics (see “Biotech’s Coming Cancer Cure”) paid $125 million to acquire AbVitro, a Boston-area company that specializes in sequencing the DNA inside single T cells. Now Juno is trying to locate T cells that are active inside cancers and study their receptors. Juno’s chief scientist, Hyam Levitsky, says an experiment that used to take seven months now takes seven days. And data is piling up: an average experiment generates 100 gigabytes of information. “A lot of what is happening is technology-driven,” he says. “The questions have been there for a while, but there was no way to get at the answers. Now we’re visualizing them with new technology in ways we never could before.”  

Beyond cancer


In March Pfizer appointed John Lin to head its San Francisco biotech unit, which develops cancer drugs and recently started making engineered T cells. He says the company had been negotiating with Cellectis well before the news of Layla’s treatment and that no one there was even aware the girl had been treated before it hit the news. “The publicity was a big surprise,” he says.

Lin says years of scientific work have finally resulted in a level of mastery that makes therapeutic products seem practical. He thinks the treatments will go beyond leukemia, and beyond cancer. “We think that this fundamental principle, engineering human cells, could have broad implications,” he says, “and the immune system will be the most convenient vehicle for it, because they can move and migrate and play such important roles.”

Researchers are already working on autoimmune disorders, like diabetes, multiple sclerosis, and lupus. Infectious disease is also in the sights of T-cell engineers. Edward Berger, a virologist at the National Institutes of Health who helped discover how HIV enters human cells, thinks it may be possible to permanently keep the virus in check, a so-called “functional cure.” In February, he says, he will start giving monkeys T cells genetically programmed to find and destroy any cell in which the simian version of HIV is replicating.

The actual process isn’t as simple as the theory. Berger is sure that years of missteps and do-overs lie ahead. Also, most protocols involving engineered T cells require patients, or monkeys, to take drugs that temporarily kill off their own T cells, which isn’t without risks. “Where the technology stands, it’s a pretty radical treatment,” Berger says. “You aren’t going to use it on a cold sore.” But despite all the progress that has been made treating HIV, a better approach is still needed. Because the virus hides in the body even after treatment, patients have to take antiretroviral drugs for life. With immune engineering, maybe not. Berger sees the chance of a one-time treatment that can hold the virus in check for good.

“I was totally inspired by the cancer work,” he says. “They cured leukemia, and we’ve borrowed it from them. The extension of those ideas for engineering the immune system against other things that ail people is a major front. I think HIV is the best candidate in infectious disease. If you talk to the HIV community, they are crying for a cure—a treatment that, ideally, you do once and never again.”

by Antonio Regalado",0.01% - 0.1%,"The article discusses the rapid progress being made in immune therapy, particularly in cancer treatment, and how this technology is now being applied to other areas such as autoimmune disorders and infectious diseases.

The article mentions several companies and researchers working on immune engineering, including Juno Therapeutics, Cellectis, and Google's life science unit, Verily. These companies are developing new technologies that allow for the rapid sequencing of DNA in cancer cells and the creation of personalized treatments using genetically engineered T cells.

One example mentioned is a young girl named Layla who was treated with genetically engineered T cells at Great Ormond Street Hospital in London. The treatment was successful in putting her leukemia into remission, and it has sparked interest in applying this technology to other types of cancer.

The article also mentions that researchers are now exploring the use of immune engineering for non-cancer applications, such as autoimmune disorders like diabetes and lupus, as well as infectious diseases like HIV. For example, a researcher at the National Institutes of Health is working on using genetically engineered T cells to create a ""functional cure"" for HIV.

Overall, the article suggests that immune engineering is a rapidly advancing field with significant potential for treating a range of diseases. However, it also notes that there are still many challenges and risks associated with this technology, and more research is needed to fully understand its potential and limitations.

Some key points from the article include:

* Immune engineering involves using genetic engineering to modify T cells, which are a type of immune cell.
* This technology has shown significant promise in treating cancer, particularly leukemia.
* Companies like Juno Therapeutics and Cellectis are working on developing this technology for other types of cancer.
* Researchers are also exploring the use of immune engineering for non-cancer applications, such as autoimmune disorders and infectious diseases.
* Google's life science unit, Verily, is also involved in this area, although its plans are not yet clear.
* The technology has the potential to revolutionize the treatment of many diseases, but it is still in the early stages and more research is needed.",Biotech,"The article discusses the rapid progress being made in immune therapy, specifically in the field of T-cell engineering, where scientists are learning how to modify a patient's own immune cells to attack cancer cells. This approach has shown promising results in treating leukemia and other types of cancer.

The article highlights several key developments in this field:

1. **Google's involvement**: Google is investing heavily in life sciences research, including a focus on T-cell engineering. The company held two summits at MIT with top immune oncologists and bioengineers to determine how to apply its technological expertise to the problem.
2. **Mount Sinai's software-based approach**: A team led by Jeffrey Hammerbacher at Mount Sinai is developing software that can interpret the DNA sequence of a patient's cancer and predict how to train T cells to attack it. This approach has led to a clinical trial that will start soon.
3. **Juno Therapeutics' acquisition of AbVitro**: Juno Therapeutics, a biotech company, paid $125 million to acquire AbVitro, which specializes in sequencing the DNA inside single T cells. This acquisition is expected to accelerate research in T-cell engineering.
4. **Pfizer's expansion into T-cell engineering**: Pfizer has appointed John Lin to head its San Francisco biotech unit, which will focus on developing cancer drugs and engineered T cells.

The article also discusses the potential applications of T-cell engineering beyond cancer treatment:

1. **Autoimmune disorders**: Researchers are exploring the use of T-cell engineering to treat autoimmune disorders such as diabetes, multiple sclerosis, and lupus.
2. **Infectious disease**: Scientists are working on using T-cell engineering to develop a ""functional cure"" for HIV.

However, the article also notes that there are still significant challenges to overcome in this field, including:

1. **Complexity of the technology**: The process of modifying T cells is complex and requires careful optimization.
2. **Risks associated with treatment**: Some protocols involving engineered T cells require patients to take drugs that temporarily kill off their own T cells, which carries risks.

Overall, the article suggests that T-cell engineering has the potential to revolutionize the way we treat cancer and other diseases, but it will likely take several years of research and development to overcome the technical challenges and make these treatments widely available."
Precise Gene Editing in Plants,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/precise-gene-editing-in-plants/,"Precise Gene Editing in Plants
CRISPR offers an easy, exact way to alter genes to create traits such as disease resistance and drought tolerance.
by David Talbot archive page
February 23, 2016

GRANT CORNETT
A new gene-editing method is providing a precise way to modify crops in hopes of making them yield more food and resist drought and disease more effectively. Research in the past year has shown that the resulting plants have no traces of foreign DNA, making it possible that they will not fall under existing regulations governing genetically modified organisms and will sidestep many of the consumer concerns over these GMOs.

The technology is known as CRISPR (see “10 Breakthrough Technologies 2014: Genome Editing”), and plants modified with it are sprouting in laboratory greenhouses around the world. Already, a lab in China has used it to create a fungus--resistant wheat; several groups in China are using the technique on rice in efforts to boost yields; and a group in the U.K. has used it to tweak a gene in barley that helps govern seed germination, which could aid efforts to produce drought-resistant varieties. Indeed, because it’s so easy to do and the plants could avoid the lengthy and expensive regulatory process associated with GMOs, the method is increasingly being used by research labs, small companies, and public plant breeders unwilling to take on the expense and risks of conventional genetic engineering.

The gene-editing technique could be critical in helping scientists keep up with the constantly evolving microbes that attack crops, says Sophien Kamoun, who leads a research group at the Sainsbury Lab in Norwich, England, that is applying the technology to potatoes, tomatoes, and other crops to fight fungal diseases. “It takes millions of dollars and many years of work to go through the regulatory process,” Kamoun says. “But the pathogens don’t sit and wait for you; they keep evolving and changing.”

Gene Editing in Plants

Breakthrough
The ability to cheaply and precisely edit plant genomes without leaving foreign DNA behind.
Why It Matters
We need to increase agricultural productivity to feed the world’s growing population, which is expected to reach 10 billion by 2050.
Key Players in Engineering Crops
- The Sainsbury Laboratory and John Innes Centre, Norwich, U.K.
- Seoul National University
- University of Minnesota
- Institute of Genetics and Developmental Biology, Beijing
A version of CRISPR he co-developed paved the way for recent work on barley and a broccoli-like plant at the John Innes Centre, a plant science research center also in Norwich. Kamoun and colleagues showed that the second generation of some of the edited plants contain none of the foreign DNA that had been used to create the first generation. (Though CRISPR doesn’t require inserting foreign genes, it does typically use bits of bacterial genetic material to target the editing.) Meanwhile, a group at Seoul National University has avoided leaving any foreign genetic material even in first-generation plants.

Big and small companies alike are jumping in. DuPont Pioneer has already invested in Caribou Biosciences, the CRISPR startup cofounded by Jennifer Doudna, one of the inventors of the technology, and is using it in experiments on corn, soybeans, wheat, and rice. It hopes to sell seeds bred with CRISPR technology in as little as five years.

The big question is whether CRISPR crops will be governed by the same regulations as GMOs. The U.S. Department of Agriculture has already said some examples of gene-edited corn, potatoes, and soybeans (edited using a different method, known as TALENs) don’t fall under existing regulations. But both the United States and the more restrictive European Union are now conducting reviews of today’s regulations. And Chinese authorities have not said whether they will allow the crops to be planted.

by David Talbot",0.05% - 0.5%,Positive,Biotech,Methodology
Conversational Interfaces,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/conversational-interfaces/,"Conversational Interfaces
Powerful speech technology from China’s leading Internet company makes it much easier to use a smartphone.
by Will Knight archive page
February 23, 2016

TOMI UM
Conversational Interfaces

Breakthrough
Combining voice recognition and natural language understanding to create effective speech interfaces for the world’s largest Internet market.
Why it matters
It can be time-consuming and frustrating to interact with computers by typing.
Key players
Baidu; Google; Apple; Nuance; Facebook
Stroll through Sanlitun, a bustling neighborhood in Beijing filled with tourists, karaoke bars, and luxury shops, and you’ll see plenty of people using the latest smartphones from Apple, Samsung, or Xiaomi. Look closely, however, and you might notice some of them ignoring the touch screens on these devices in favor of something much more efficient and intuitive: their voice.

A growing number of China’s 691 million smartphone users now regularly dispense with swipes, taps, and tiny keyboards when looking things up on the country’s most popular search engine, Baidu. China is an ideal place for voice interfaces to take off, because Chinese characters were hardly designed with tiny touch screens in mind. But people everywhere should benefit as Baidu advances speech technology and makes voice interfaces more practical and useful. That could make it easier for anyone to communicate with the machines around us.

“I see speech approaching a point where it could become so reliable that you can just use it and not even think about it,” says Andrew Ng, Baidu’s chief scientist and an associate professor at Stanford University. “The best technology is often invisible, and as speech recognition becomes more reliable, I hope it will disappear into the background.”

Voice interfaces have been a dream of technologists (not to mention science fiction writers) for many decades. But in recent years, thanks to some impressive advances in machine learning, voice control has become a lot more practical.

The systems offer a glimpse of a future in which there’s less need to learn a new interface for every device.
No longer limited to just a small set of predetermined commands, it now works even in a noisy environment like the streets of Beijing or when you’re speaking across a room. Voice-operated virtual assistants such as Apple’s Siri, Microsoft’s Cortana, and Google Now come bundled with most smartphones, and newer devices, like Amazon’s Alexa, offer a simple way to look up information, cue up songs, and build shopping lists with your voice. These systems are hardly perfect, sometimes mishearing and misinterpreting commands in comedic fashion, but they are improving steadily, and they offer a glimpse of a graceful future in which there’s less need to learn a new interface for every new device.


DATA SOURCES: CHINA INTERNET NETWORK INFORMATION CENTER
Baidu is making particularly impressive progress, especially with the accuracy of its voice recognition, and it has the scale to advance conversational interfaces even further. The company—founded in 2000 as China’s answer to Google, which is currently blocked there—dominates the country’s domestic search market, with 70 percent of all queries. And it has evolved into a purveyor of many services, from music and movie streaming to banking and insurance.

A more efficient mobile interface would come as a big help in China. Smartphones are far more common than desktops or laptops, and yet browsing the Web, sending messages, and doing other tasks can be painfully slow and frustrating. There are thousands of Chinese characters, and although a system called Pinyin allows them to be generated phonetically from Latin ones, many people (especially those over 50) do not know the system. It’s also common in China to use messaging apps such as WeChat to do all sorts of tasks, such as paying restaurant tabs. And yet in many of China’s poorer regions, where there is perhaps more opportunity for the Internet to have big social and economic effects, literacy levels are still low.

“It is a challenge and an opportunity,” says Ng, who was named one of MIT Technology Review’s Innovators Under 35 in 2008 for his work in AI and robotics at Stanford. “Rather than having to train people used to desktop computers to new behaviors appropriate for cell phones, many of them can learn the best ways to use a mobile device from the start.”

Ng believes that voice may soon be reliable enough to be used for interacting with all sorts of devices. Robots or home appliances, for example, could be easier to deal with if you could simply talk to them. The company has research teams at its headquarters in Beijing and at a facility in Silicon Valley that are dedicated to advancing the accuracy of speech recognition and working to make computers better at parsing the meaning of sentences.

Jim Glass, a senior research scientist at MIT who has been working on voice technology for the past few decades, agrees that the timing may finally be right for voice control. “Speech has reached a tipping point in our society,” he says. “In my experience, when people can talk to a device rather than via a remote control, they want to do that.”


Researchers at Baidu’s headquarters in Beijing are plugging away at a digital assistant that can hold a conversation.
QILAI SHEN | BLOOMBERG VIA GETTY IMAGES
Last November, Baidu reached an important landmark with its voice technology, announcing that its Silicon Valley lab had developed a powerful new speech recognition engine called Deep Speech 2. It consists of a very large, or “deep,” neural network that learns to associate sounds with words and phrases as it is fed millions of examples of transcribed speech. Deep Speech 2 can recognize spoken words with stunning accuracy. In fact, the researchers found that it can sometimes transcribe snippets of Mandarin speech more accurately than a person.

Baidu’s progress is all the more impressive because Mandarin is phonetically complex and uses tones that transform the meaning of a word. Deep Speech 2 is also striking because few of the researchers in the California lab where the technology was developed speak Mandarin, Cantonese, or any other variant of Chinese. The engine essentially works as a universal speech system, learning English just as well when fed enough examples.

Few of those behind Deep Speech 2 speak Mandarin or Cantonese. It’s a universal language engine.
Most of the voice commands that Baidu’s search engine hears today are simple queries—concerning tomorrow’s weather or pollution levels, for example. For these, the system is usually impressively accurate. Increasingly, however, users are asking more complicated questions. To take them on, last year the company launched its own voice assistant, called DuEr, as part of its main mobile app. DuEr can help users find movie show times or book a table at a restaurant.

The big challenge for Baidu will be teaching its AI systems to understand and respond intelligently to more complicated spoken phrases. Eventually, Baidu would like for DuEr to take part in a meaningful back-and-forth conversation, incorporating changing information into the discussion. To get there, a research group at Baidu’s Beijing offices is devoted to improving the system that interprets users’ queries. This involves using the kind of neural-network technology that Baidu has applied in voice recognition, but it also requires other tricks. And Baidu has hired a team to analyze the queries fed to DuEr and correct mistakes, thus gradually training the system to perform better.

“In the future, I would love for us to be able to talk to all of our devices and have them understand us,” Ng says. “I hope to someday have grandchildren who are mystified at how, back in 2016, if you were to say ‘Hi’ to your microwave oven, it would rudely sit there and ignore you.”

by Will Knight",1-5%,Positive,Software,"The article discusses the progress of Baidu, a Chinese technology company, in developing conversational interfaces, particularly in voice recognition. The company's advancements have made its voice recognition system, Deep Speech 2, capable of recognizing spoken words with high accuracy, even surpassing human transcription in some cases.

Baidu's progress is significant because Mandarin, the primary language it operates in, is phonetically complex and uses tones that transform the meaning of a word. The company's technology has also shown to be universal, learning English and other languages with equal proficiency.

The article highlights the potential benefits of Baidu's conversational interfaces, particularly in China where literacy levels are low, and many people rely on mobile devices for communication. A more efficient mobile interface would greatly improve user experience, especially for tasks such as browsing the web, sending messages, and making payments.

Andrew Ng, a renowned AI expert and former chief scientist at Baidu, is quoted in the article, stating that voice may soon be reliable enough to be used for interacting with various devices, including robots and home appliances. He also emphasizes the importance of developing conversational interfaces that can understand and respond intelligently to complex spoken phrases.

The article concludes by highlighting the challenges ahead for Baidu, particularly in teaching its AI systems to understand and respond intelligently to more complicated spoken phrases. However, with continued advancements in neural network technology and human correction, the company is poised to make significant progress in developing conversational interfaces that can revolutionize the way people interact with devices.

Some key points from the article include:

* Baidu's Deep Speech 2 voice recognition system has achieved high accuracy in recognizing spoken words, even surpassing human transcription in some cases.
* Mandarin is a phonetically complex language, but Baidu's technology has shown to be universal, learning English and other languages with equal proficiency.
* Conversational interfaces have the potential to greatly improve user experience, particularly for tasks such as browsing the web, sending messages, and making payments.
* Andrew Ng believes that voice may soon be reliable enough to be used for interacting with various devices, including robots and home appliances.
* Baidu faces challenges in teaching its AI systems to understand and respond intelligently to more complicated spoken phrases."
Reusable Rockets,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/reusable-rockets/,"Reusable Rockets
Rockets typically are destroyed on their maiden voyage. But now they can make an upright landing and be refueled for another trip, setting the stage for a new era in spaceflight.
by Brian Bergstein archive page
February 23, 2016

EMTECH AND SPACEX
Reusable Rockets

Breakthrough
Rockets that can launch payloads into orbit and then land safely.
Why it matters
Lowering the cost of flight would open the door to many new endeavors in space.
Key players
SpaceX; Blue Origin; United Launch Alliance
Thousands of rockets have flown into space, but not until 2015 did one return like this: it came down upright on a landing pad, steadily firing to control its descent, almost as if a movie of its launch were being played backward. If this can be done regularly and rockets can be refueled over and over, spaceflight could become a hundred times cheaper.

Two tech billionaires made it happen. Jeff Bezos’s Blue Origin first pulled off a landing in November; Elon Musk’s SpaceX did it in December. The companies are quite different—Blue Origin hopes to propel tourists in capsules on four-minute space rides, while SpaceX already launches satellites and space station supply missions—but both need reusable rockets to improve the economics of spaceflight.

Blasting things into space has been expensive because rockets cost tens of millions of dollars and fly once before burning up in a free fall back through the atmosphere. SpaceX and Blue Origin instead bring theirs down on fold-out legs, a trick that requires onboard software to fire thrusters and manipulate flaps that slow or nudge the rockets at precise moments.

SpaceX has the harder job because Blue Origin’s craft go half as fast and half as high and stay mostly vertical, whereas SpaceX’s rockets have to switch out of a horizontal position. A reminder of how many things can go wrong came in January, when SpaceX just missed a second landing because a rocket leg didn’t latch into place. Even so, it’s now clear that the future of spaceflight will be far more interesting than the Apollo-era hangover of the past 40 years.

by Brian Bergstein",0.01% - 0.1%,Positive,Hardware,methodology
Robots That Teach Each Other,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/robots-that-teach-each-other/,"Robots That Teach Each Other
What if robots could figure out more things on their own and share that knowledge among themselves?
by Amanda Schaffer archive page
February 23, 2016

KRISTIAN HAMMERSTAD
Many of the jobs humans would like robots to perform, such as packing items in warehouses, assisting bedridden patients, or aiding soldiers on the front lines, aren’t yet possible because robots still don’t recognize and easily handle common objects. People generally have no trouble folding socks or picking up water glasses, because we’ve gone through “a big data collection process” called childhood, says Stefanie Tellex, a computer science professor at Brown University. For robots to do the same types of routine tasks, they also need access to reams of data on how to grasp and manipulate objects. Where does that data come from? Typically it has come from painstaking programming. But ideally, robots could get some information from each other.

That’s the theory behind Tellex’s “Million Object Challenge.” The goal is for research robots around the world to learn how to spot and handle simple items from bowls to bananas, upload their data to the cloud, and allow other robots to analyze and use the information.

Tellex’s lab in Providence, Rhode Island, has the air of a playful preschool. On the day I visit, a Baxter robot, an industrial machine produced by Rethink Robotics, stands among oversized blocks, scanning a small hairbrush. It moves its right arm noisily back and forth above the object, taking multiple pictures with its camera and measuring depth with an infrared sensor. Then, with its two-pronged gripper, it tries different grasps that might allow it to lift the brush. Once it has the object in the air, it shakes it to make sure the grip is secure. If so, the robot has learned how to pick up one more thing.

The robot can work around the clock, frequently with a different object in each of its grippers. Tellex and her graduate student John Oberlin have gathered—and are now sharing—data on roughly 200 items, starting with such things as a child’s shoe, a plastic boat, a rubber duck, a garlic press and other cookware, and a sippy cup that originally belonged to her three-year-old son. Other scientists can contribute their robots’ own data, and Tellex hopes that together they will build up a library of information on how robots should handle a million different items. Eventually, robots confronting a crowded shelf will be able to “identify the pen in front of them and pick it up,” Tellex says.


Stefanie Tellex and a Baxter robot at Brown University.
LEONARD GRECO
Robots That Teach Each Other

Breakthrough
Robots that learn tasks and send that knowledge to the cloud for other robots to pick up later.
Why it matters
Progress in robotics could accelerate dramatically if each type of machine didn’t have to be programmed separately.
Key players
Ashutosh Saxena, Brain of Things; Stefanie Tellex, Brown University; Pieter Abbeel, Ken Goldberg, and Sergey Levine, University of California, Berkeley; Jan Peters, Technical University of Darmstadt, Germany
Projects like this are possible because many research robots use the same standard framework for programming, known as ROS. Once one machine learns a given task, it can pass the data on to others—and those machines can upload feedback that will in turn refine the instructions given to subsequent machines. Tellex says the data about how to recognize and grasp any given object can be compressed to just five to 10 megabytes, about the size of a song in your music library.

Tellex was an early partner in a project called RoboBrain, which demonstrated how one robot could learn from another’s experience. Her collaborator Ashutosh Saxena, then at Cornell, taught his PR2 robot to lift small cups and position them on a table. Then, at Brown, Tellex downloaded that information from the cloud and used it to train her Baxter, which is physically different, to perform the same task in a different environment.

Such progress might seem incremental now, but in the next five to 10 years, we can expect to see “an explosion in the ability of robots,” says Saxena, now CEO of a startup called Brain of Things. As more researchers contribute to and refine cloud-based knowledge, he says, “robots should have access to all the information they need, at their fingertips.”


Each time the robot determines the best way to grasp and hold something, it files that data away in a format other robots can use.
LEONARD GRECO
by Amanda Schaffer",0.01% - 0.1%,Positive,Software,project
DNA App Store,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/dna-app-store/,"DNA App Store
An online store for information about your genes will make it cheap and easy to learn more about your health risks and predispositions.
by Antonio Regalado archive page
February 23, 2016

JAVIER JAÉN
DNA App Store

Breakthrough
A new business model for DNA sequencing that will make genetic information widely accessible online.
Why it matters
Your genome determines a great deal about you, including your likelihood of getting certain diseases.
Key players
Helix; Illumina; Veritas Genetics
While driving and listening to National Public Radio one day, Justin Kao heard about the discovery of a “sweet tooth gene” that makes you more likely to crave sweets. “Oh my God,” thought Kao, who has always loved cookies. “I would pay $5 to know if I had that.”

Kao is hoping that millions of other people will be just as eager to spend a few bucks for tidbits revealed in their DNA. He is a cofounder of Helix, a San Francisco–based company that last summer secured more than $100 million in a quest to create the first “app store” for genetic information.

Our genomes hold information about our health risks, our physical traits, and whom we’re related to. Yet aside from ancestry tests that provide a limited genetic snapshot, there’s not a mass market for DNA data. Helix is a bet by Kao’s former employer, the buyout firm Warburg Pincus, and Illumina, the leading manufacturer of ultrafast DNA sequencing machines, that what’s been missing is the right business model.

Helix’s idea is to collect a spit sample from anyone who buys a DNA app, sequence and analyze the customers’ genes, and then digitize the findings so they can be accessed by software developers who want to sell other apps. Helix calls the idea “sequence once, query often.” (The company says customers will find these apps on websites and possibly in the Android and Apple app stores.)

With its ties to Illumina, Helix thinks it can decode the most important part of a person’s genome—all 20,000 genes and a few other bits—at a cost of about $100, about one-fifth of what it costs other companies. That’s why Helix can afford its second gambit: to generate and store this type of data for all customers, even if they initially make only one specific genetic query—such as whether they have the sweet tooth gene or a risk for a certain disease. Maybe two guys in a garage will write a $10 app that shows you how old you’ll look in 10 years, or which celebrity you are most closely related to. Kao says the tactic will make genetic information available to consumers “at an unprecedentedly low entry price.”

The engine to power the app store is being assembled a mile from Illumina’s San Diego headquarters, in a building where workmen were still bending sheet metal and laying floor tiles in January. Several miles of data cables strung through the ceiling will be connected to a large farm of sequencing machines, able to process the DNA from a million samples a year. Illumina’s CEO, Jay Flatley, also chairman of Helix, has said it could be the largest sequencing center anywhere.

Helix plans to launch the store this year or next. Customers will control their data by deciding who sees it. There’s even a “nuclear button” to erase every A, G, C, and T. But key details are still being sorted out. Will people be able to download their DNA information and take it elsewhere? Probably, though they might pay extra for the privilege.

One company working with Helix is Good Start Genetics, a startup in Cambridge, Massachusetts, that offers pre-conception testing. These DNA tests tell parents-to-be if they share a risk for passing on a serious genetic condition, such as cystic fibrosis. Jeffrey Luber, Good Start’s head of business development, says it hopes to reach a larger audience with an app that can report a few important risks. As with browsing on Amazon, he thinks, people will discover things they “didn’t know they needed but that [are] targeted to them, and that they want.”

A looming question mark is the U.S. Food and Drug Administration, which has kept close tabs on gene tests and will decide how much information Helix apps can reveal. Right now, says Keith Stewart, director of the Center for Individualized Medicine at the Mayo Clinic, most apps that return real medical information—your chance of cancer, say, not just how much Neanderthal is in your DNA—would need agency approval, or at least a doctor in the loop.

“The bottom line is going to be: What are the regulatory constraints on information that is truly useful?” says Mirza Cifric, CEO of Veritas Genetics. His company has been offering since last fall to sequence a person’s entire genome and is creating its own app to explore the data, complete with a button to get a FaceTime appointment with a genetic counselor. Cifric hasn’t decided whether to create an app with Helix, but he says he shares its core belief: “The genome is an asset that you have for life, and you’ll keep going back to it.”

by Antonio Regalado",<0.01% - 0.1%,Negative,Biotech,platform
SolarCity's Gigafactory,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/solarcitys-gigafactory/,"SolarCity’s Gigafactory
A $750 million solar facility in Buffalo will produce a gigawatt of high-efficiency solar panels per year and make the technology far more attractive to homeowners.
by Richard Martin archive page
February 23, 2016

In an industrial park near the shore of Lake Erie, hard by the Buffalo River, the future of the solar power industry is under construction. SolarCity’s sprawling Buffalo factory, built and paid for by the state of New York, is nearing completion and will soon begin producing some of the most efficient solar panels available commercially. Capable of making 10,000 solar panels a day, or one gigawatt of solar capacity a year, it will be the largest solar manufacturing plant in North America and one of the biggest in the world.

When production begins, SolarCity, already the leading installer of residential solar panels in the United States, will become a vertically integrated manufacturer and provider—doing everything from making the solar cells to putting them on rooftops. At a time when conventional silicon-based solar panels from China have never been cheaper, investing in a new type of solar technology is a risky undertaking. However, the potential benefits are huge. The new factory, says SolarCity chief technology officer Peter Rive, could transform both SolarCity’s business, which has consistently lost money, and the economics of residential solar power.

Solar panels installed by SolarCity cost the company $2.84 per watt (including sales and marketing plus overhead, in addition to the cost of the hardware), down from $4.73 in 2012. The combination of the new, highly efficient panels, the volume of product coming out of the new factory, and a simplified manufacturing process is a big reason why the company expects its costs for residential solar to fall well below $2.50 per watt by the end of 2017, when the Buffalo facility reaches full production.

Bolstered by federal solar subsidies and “net metering,” the rules that allow homeowners to sell excess power back to the grid at retail prices in many states, SolarCity is already leading the way in making residential systems financially attractive to many households, spurring an explosion in the popularity of the rooftop panels. The drop in installed costs could make residential solar even more popular.

“Right now we can sell you energy in 14 states at a rate lower than what you’re currently paying the utility,” says Rive. The Buffalo factory, he adds, “sets us up for a future where solar plus batteries is cheaper than fossil fuels.”

Key to the company’s ambitions is a technology it acquired when it bought a small solar company called Silevo in 2014. That technology, which allows it to make panels that are highly efficient at converting sunlight into electricity, traces its origins to the Australian solar power pioneer Martin Green in the late 1970s. It combines a standard crystalline-silicon solar cell with elements of a thin-film cell, along with a layer of a semiconductor oxide. Last October, SolarCity announced that test panels made at a small facility in Fremont, California, had tested at just over 22 percent efficiency. Today’s commodity silicon-based solar panels have efficiencies of between 16 and 18 percent. SolarCity competitor SunPower previously led the market with cells that can reach 21.5 percent.


Top image: The massive solar manufacturing facility, shown above in late December, is scheduled to begin full-scale production sometime next year.

Above: The gigafactory covers some 27 acres, making it the largest solar production facility in North America.
GUS POWELL
SolarCity’s Gigafactory

Breakthrough
Highly efficient solar panels made using a simplified, low-cost manufacturing process.
Why it matters
The solar industry needs cheaper and more efficient technology to be more competitive with fossil fuels.
Key players
SolarCity; SunPower; Panasonic
Efficiency matters because the panels themselves represent only 15 to 20 percent of the cost of the full installation. Much of the rest comes in what’s known as balance-of-system costs: inverters to connect to the grid, materials to house the array, nuts and bolts to attach it to the roof, the labor to install it, and so on. SolarCity’s installation, says the company, will require one-third fewer panels to produce the same amount of electricity as conventional installations. “Fewer panels means fewer bits and pieces, less wire, less days on the roof to install,” says Francis O’Sullivan, the director of research and analysis at the MIT Energy Initiative.

SolarCity uses a deposition manufacturing process that reduces the number of steps required to make the cells from two dozen or more to just six. It also replaces silver, one of the most expensive elements of conventional solar cells, with less expensive copper.

But the difference in performance between solar panels produced in a small facility like SolarCity’s Fremont plant and in a large factory like the Buffalo one could be significant. And scaling up production could be particularly tricky given -SolarCity’s lack of manufacturing experience. Rive acknowledges that there could be “small risks around the actual time line” in getting the products coming out of Buffalo to match the efficiencies achieved at small scale. Already, SolarCity has pushed back the target date for full production from the Buffalo factory from the first quarter of 2017 to later in the year.

But the real risk lies in the rapid advance of solar technology: a record-setting panel today might look relatively inefficient three or five years down the road. Soon after SolarCity showed off its high-efficiency panels last October, Panasonic topped its rival by claiming that its new panels would reach efficiencies of 22.5 percent. Meanwhile, efficiencies in the lab are even higher: researchers have made exotic solar-cell materials with efficiencies of up to 40 percent. “I think that within 10 years, most manufacturers will be producing panels over 20 percent efficiency, with the best commercial panels reaching over 23 percent,” Green says.

O’Sullivan adds: “For now, SolarCity is moving the boat out as far as it can with, generically speaking, contemporary technology. But we’re beginning to approach a choke point for the economics of any silicon-based technology”—including the new cells SolarCity is bringing online. Future advances, he says, will entail much lighter, flexible panels that offer much higher efficiencies and are even cheaper to install—and thus produce electricity at a much lower cost.

At that point, the solar panels coming out of the gigafactory may seem as conventional as commodity panels produced in China today. It is, however, SolarCity’s willingness to take on such risks that makes the Buffalo facility so ambitious. Over the last 10 years, the Silicon Valley company has made residential solar a popular choice for many consumers through smart marketing and attractive financing. Now it wants to transform solar manufacturing. Whether SolarCity succeeds or fails, it is once again pushing the possibilities of solar power.

by Richard Martin",0.01% to 0.1%,Positive,Hardware,platform
Slack,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/slack/,"Slack
A service built for the era of mobile phones and short text messages is changing the workplace.
by Lee Gomes archive page
February 23, 2016

MIGUEL PORLAN
The intra-office messaging system known as Slack is often described as the fastest-growing workplace software the world has ever seen. It surpassed two million daily users less than three years after its launch in 2013.

Slack gives you a centralized place to communicate with your colleagues through instant messages and in chat rooms, which can reduce the time you have to spend on e-mail. Whether you’re on a mobile device or a desktop computer, you can upload files, get and manipulate information stored in spreadsheets or other business applications, and easily search through past conversations. But many of the core features have been around since the 1990s. And there have been other “Facebook for the office” software packages that resemble Slack and have failed to generate anything close to the same level of enthusiasm.

But what, exactly, makes it so popular?

The reason for its success lies in part with big trends: more and more people now get work done on mobile devices, in collaboration with people who aren’t always in the same office at the same time. But Slack’s specific design choices have also been important. Gerald C. Kane, associate professor of information systems at Boston College’s Carroll School of Management, points out that Slack funnels messages into streams that everyone who works together can see. That “allows you to ‘overhear’ what is going on in an organization, which research has shown can lead to business impact,” he says. “It’s a kind of ambient awareness that you just don’t get from e-mail.”


Proportion of online time spent on mobile devices in the U.S.
MIGUEL PORLAN; DATA SOURCES: EMARKETER, GALLUP, RADICATI GROUP
Slack

Breakthrough
Easy-to-use communication software that is supplanting e-mail as a method of getting work done.
Why it matters
In many kinds of workplaces, the “water cooler” effect that lets people overhear their colleagues’ conversations can enhance productivity.
Key players
Slack; Quip; Hipchat; Microsoft
Kristina Lerman, a specialist in social computing at the Information Sciences Institute at the University of Southern California, notes that Slack messages tend to be short and casual, much more like the mobile text messages that people are increasingly favoring over e-mail in their personal life. This creates the perception that keeping in touch with coworkers is effortless. “You get the feeling that you are quickly responding to everything that is happening around you,” Lerman says.


Proportion of U.S. workers who telecommute
MIGUEL PORLAN; DATA SOURCES: EMARKETER, GALLUP, RADICATI GROUP
In fact, Slack makes it so easy to create messages that it might end up placing as many demands on people’s time as e-mail traditionally has, albeit with a hip and friendly interface. “There are limits to the amount of time that we have to interact with each other, and Slack doesn’t really cure that,” Lerman says. Software might take some of the friction out of getting work done, but it is still work.


Estimated average number of e-mails sent and received by business users each day
MIGUEL PORLAN; DATA SOURCES: EMARKETER, GALLUP, RADICATI GROUP
by Lee Gomes",0.5% - 1.5%,Positive,Software,product
Tesla Autopilot,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/tesla-autopilot/,"Tesla Autopilot
The electric-vehicle maker sent its cars a software update that suddenly made autonomous driving a reality.
by Ryan Bradley archive page
February 23, 2016

JULIAN BERMAN

Like many other features in the car, Autopilot can be activated or shut off from a touch screen. It also turns off with a tap on the brakes.
JULIAN BERMAN
Tesla Autopilot

Breakthrough
A car that drives itself safely in a variety of conditions.
Why it matters
Car crashes caused by human error kill thousands of people a day worldwide.
Key players
Ford Motor; General Motors; Google; Nissan; Mercedes; Tesla Motors; Toyota; Uber; Volvo
In October 2014, Elon Musk’s electric-car company began rolling out sedans with a dozen ultrasonic sensors discreetly placed around both bumpers and sides. For an additional $4,250, Tesla customers could purchase a “technology package” that used the sensors, as well as a camera, a front radar, and digitally controlled brakes, to help avoid collisions—essentially allowing the car to take over and stop before crashing. But mostly, the hardware sat there, waiting, waiting, and gathering reams of data. A year later, last October 14, the company sent a software update to the 60,000 sensor-laden cars it had sold in that time. The software update was officially named Tesla Version 7.0, but its nickname—Autopilot—was what stuck.

It did in fact give drivers something similar to what airline pilots employ in flight. The car could manage its speed, steer within and even change lanes, and park itself. Some of these features, like automatic parallel parking, were already on offer from other car companies (including Mercedes, BMW, and General Motors), but the self-steering was suddenly, overnight, via a software update, a giant leap toward full autonomy.

Tesla customers, delighted, posted videos of themselves on the highway, hands free, reading the paper, sipping coffee, and even, once, riding on the roof. Some of these are, it’s worth pointing out, illegal acts. Autopilot existed in a legal gray area,  but it was a grand gesture toward an ever nearing future, one that will reshape not just the car and our relationship with it but the road and our entire transportation infrastructure.

Which is why I jumped at the chance to borrow a car with Autopilot for a few days and drive it—or let it drive me—around Los Angeles.

Everyone wanted to know what it felt like, the strange surrender of allowing a car to take control. The only moments that seemed like magic were when the car parked itself or changed lanes, mostly because watching a steering wheel turn all on its own was unnatural and ghostly. Other than that, I was amazed by how quickly I got used to it, how inevitable it began to feel. As a Tesla engineer told me—on condition of anonymity, because the company won’t let anyone but Musk speak publicly these days—the thing that quickly becomes strange is driving a car without Autopilot. “You’ll feel like the car is not doing its job,” he said. 


Like many other features in the car, Autopilot can be activated or shut off from a touch screen. It also turns off with a tap on the brakes.
JULIAN BERMAN
The car can’t start in Autopilot; it requires a set of circumstances (good data, basically) before you can engage the setting. These include clear lane lines, a relatively constant speed, a sense of the cars around you, and a map of the area you’re traveling through—roughly in that order. L.A.’s abundant highway traffic is the ideal scenario for Autopilot, not simply because of all the data it makes available to the ultrasonic sensors—which use high-frequency sound waves to identify objects up to 16 feet away—but also because humans are awful in traffic. We are bad at estimating distances to begin with, and we are constantly trying to switch lanes when the next one looks faster, causing accidents in the process. With Autopilot, I no longer had to stare at the bumper ahead of me, and I could look around to see the variety of bad decisions drivers make, stopping and starting and stopping again. Meanwhile, my car accelerated and slowed more smoothly than it ever could have with me in charge.

With its incremental approach, Tesla stands in contrast to Google and other companies that have small test fleets gathering data in hopes of someday launching fully autonomous cars. For Tesla, its customers and their partially autonomous cars are a widely distributed test fleet. The hardware required for true autonomy is already in place, so the transition can play out in software updates. Musk has said that could be technically feasible—if not legally so—within two years.

The day after I returned the Tesla, my fiancée and I were on an L.A. freeway and saw someone, speeding, cross three lanes, cutting in front of several drivers. As the traffic stopped, the car behind us came in way too fast and crashed into our bumper, which fell right off. The future, I thought, was practically here, and it couldn’t arrive soon enough.

by Ryan Bradley",0.05% - 0.5%,Positive,Software,product
Power from the Air,2016.0,https://www.technologyreview.com/10-breakthrough-technologies/2016/,https://www.technologyreview.com/technology/power-from-the-air/,"Power from the Air
Internet devices powered by Wi-Fi and other telecommunications signals will make small computers and sensors more pervasive.
by Mark Harris archive page
February 23, 2016

BRENDAN MONROE; DANIEL BERMAN
Power from the Air

Breakthrough
Wireless gadgets that repurpose nearby radio signals, such as Wi-Fi, to power themselves and communicate.
Why it matters
Freeing Internet-­connected devices from the constraints of batteries and power cords will open up many new uses.
Key players
University of Washington; Texas Instruments; University of Massachusetts, Amherst
Even the smallest Internet-connected devices typically need a battery or power cord. Not for much longer. Technology that lets gadgets work and communicate using only energy harvested from nearby TV, radio, cell-phone, or Wi-Fi signals is headed toward commercialization. The University of Washington researchers who developed the technique have demonstrated Internet-connected temperature and motion sensors, and even a camera, powered that way.

Transferring power wirelessly is not a new trick. But getting a device without a conventional power source to communicate is harder, because generating radio signals is very power-intensive and the airwaves harvested from radio, TV, and other telecommunication technologies hold little energy.

Shyamnath Gollakota and his colleague Joshua Smith have proved that weak radio signals can indeed provide all an Internet gadget needs, using a principle called backscattering. Instead of generating original signals, one of their devices selectively reflects incoming radio waves to construct a new signal—a bit like an injured hiker sending an SOS message using the sun and a mirror. A gadget using the technique absorbs some energy from the signal it is modifying to power its own circuits.

“We can get communication for free,” says Gollakota. RFID chips for the contactless smart cards used in mass transit also rely on backscattering, but they require specialized reader devices and can communicate only within a few inches because the reflected signals are weak and the reader itself presents interference.

One version of the University of Washington technology, dubbed passive Wi-Fi, is being commercialized through a spin-off company, Jeeva Wireless. It lets battery-free gadgets connect with conventional devices such as computers and smartphones by backscattering Wi-Fi signals. In tests, prototype passive Wi-Fi devices have beamed data as far as 100 feet and made connections through walls. Doing that requires altering the software of a Wi-Fi access point to generate an extra signal for passive Wi-Fi devices to use, very slightly increasing its power consumption.

Smith says that passive Wi-Fi consumes just 1/10,000th as much power as existing Wi-Fi chipsets. It uses a thousandth as much power as the Bluetooth LE and ZigBee communications standards used by some small connected devices and has a longer range. A device using passive Wi-Fi to communicate—for example, a security camera—could power its other circuits using energy harvested from the Wi-Fi signals it is backscattering, or by feeding on other signals such as TV and radio broadcasts.

The researchers believe that tiny passive Wi-Fi devices could be extremely cheap to make, perhaps less than a dollar. In tomorrow’s smart home, security cameras, temperature sensors, and smoke alarms should never need to have their batteries changed.

by Mark Harris",0.00% - 0.01%,Positive,Nanotech,methodology
Reversing Paralysis,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/reversing-paralysis/,"Reversing Paralysis
Scientists are making remarkable progress at using brain implants to restore the freedom of movement that spinal cord injuries take away.
by Antonio Regalado archive page
February 22, 2017

ALAIN HERZOG | EPFL AND HILLARY SANCTUARY | EPFL
Reversing Paralysis

Breakthrough
Wireless brain-body electronic interfaces to bypass damage to the nervous system.
Why it matters
Thousands of people suffer paralyzing injuries every year.
Key players
École Polytechnique Fédérale de Lausanne; Wyss Center for Bio and Neuroengineering; University of Pittsburgh; Case Western Reserve University
Availability
10 to 15 years
“Go, go!” was the thought racing through Grégoire Courtine’s mind.

The French neuroscientist was watching a macaque monkey as it hunched aggressively at one end of a treadmill. His team had used a blade to slice halfway through the animal’s spinal cord, paralyzing its right leg. Now Courtine wanted to prove he could get the monkey walking again. To do it, he and colleagues had installed a recording device beneath its skull, touching its motor cortex, and sutured a pad of flexible electrodes around the animal’s spinal cord, below the injury. A wireless connection joined the two electronic devices.

The result: a system that read the monkey’s intention to move and then transmitted it immediately in the form of bursts of electrical stimulation to its spine. Soon enough, the monkey’s right leg began to move. Extend and flex. Extend and flex. It hobbled forward. “The monkey was thinking, and then boom, it was walking,” recalls an exultant Courtine, a professor with Switzerland’s École Polytechnique Fédérale de Lausanne.

In recent years, lab animals and a few people have controlled computer cursors or robotic arms with their thoughts, thanks to a brain implant wired to machines. Now researchers are taking a significant next step toward reversing paralysis once and for all. They are wirelessly connecting the brain-reading technology directly to electrical stimulators on the body, creating what Courtine calls a “neural bypass” so that people’s thoughts can again move their limbs. 


At Case Western Reserve University, in Cleveland, a middle-aged quadriplegic—he can’t move anything but his head and shoulder—agreed to let doctors place two recording implants in his brain, of the same type Courtine used in the monkeys. Made of silicon, and smaller than a postage stamp, they bristle with a hundred hair-size metal probes that can “listen” as neurons fire off commands.

To complete the bypass, the Case team, led by Robert Kirsch and Bolu Ajiboye, also slid more than 16 fine electrodes into the muscles of the man’s arm and hand. In videos of the experiment, the volunteer can be seen slowly raising his arm with the help of a spring-loaded arm rest, and willing his hand to open and close. He even raises a cup with a straw to his lips. Without the system, he can’t do any of that.

Just try sitting on your hands for a day. That will give you an idea of the shattering consequences of spinal cord injury. You can’t scratch your nose or tousle a child’s hair. “But if you have this,” says Courtine, reaching for a red espresso cup and raising it to his mouth with an actor’s exaggerated motion, “it changes your life.”

The Case results, pending publication in a medical journal, are a part of a broader effort to use implanted electronics to restore various senses and abilities. Besides treating paralysis, scientists hope to use so-called neural prosthetics to reverse blindness with chips placed in the eye, and maybe restore memories lost to Alzheimer’s disease (see “10 Breakthrough Technologies 2013: Memory Implants”).


Grégoire Courtine holds the two main parts of the brain-spine interface.
HILLARY SANCTUARY | EPFL
And they know it could work. Consider cochlear implants, which use a microphone to relay signals directly to the auditory nerve, routing around non-working parts of the inner ear. Videos of wide-eyed deaf children hearing their mothers for the first time go viral on the Internet every month. More than 250,000 cases of deafness have been treated.


In this video made by EPFL researchers, a monkey with a spinal cord injury that paralyzed its right leg is able to walk again.
But it’s been harder to turn neural prosthetics into something that helps paralyzed people. A patient first used a brain probe to move a computer cursor across a screen back in 1998. That and several other spectacular brain-control feats haven’t had any broader practical use. The technology remains too radical and too complex to get out of the lab. “Twenty years of work and nothing in the clinic!” Courtine exclaims, brushing his hair back. “We keep pushing the limits, but it is an important question if this entire field will ever have a product.”

Courtine’s laboratory is located in a vertiginous glass-and-steel building in Geneva that also houses a $100 million center that the Swiss billionaire Hansjörg Wyss funded specifically to solve the remaining technical obstacles to neurotechnologies like the spinal cord bypass. It’s hiring experts from medical-device makers and Swiss watch companies and has outfitted clean rooms where gold wires are printed onto rubbery electrodes that can stretch as our bodies do.


A close-up of a brain-reading chip, bristling with electrodes.
WYSS CENTER AND EPFL

Flexible electrodes developed to simulate the spinal cord.
WYSS CENTER AND EPFL
The head of the center is John Donoghue, an American who led the early development of brain implants in the U.S. (see “Implanting Hope”) and who moved to Geneva two years ago. He is now trying to assemble in one place the enormous technical resources and talent—skilled neuroscientists, technologists, clinicians—needed to create commercially viable systems.

Among Donoghue’s top priorities is a “neurocomm,” an ultra-compact wireless device that can collect data from the brain at Internet speed. “A radio inside your head,” Donoghue calls it, and “the most sophisticated brain communicator in the world.” The matchbox-size prototypes are made of biocompatible titanium with a sapphire window. Courtine used an earlier, bulkier version in his monkey tests.

As complex as they are, and as slow as progress has been, neural bypasses are worth pursuing because patients desire them, Donoghue says. “Ask someone if they would like to move their own arm,” he says. “People would prefer to be restored to their everyday self. They want to be reanimated.”


A model of a wireless neurocommunication device sits on a skull.
WYSS CENTER AND EPFL
by Antonio Regalado",0.01% - 0.1%,Positive,Biotech,project
Self-Driving Trucks,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/self-driving-trucks/,"Self-Driving Trucks
Tractor-trailers without a human at the wheel will soon barrel onto highways near you. What will this mean for the nation’s 1.7 million truck drivers?
by David H. Freedman archive page
February 22, 2017

ANDREW PAYNTER
Self-Driving Trucks

Breakthrough
Long-haul trucks that drive themselves for extended stretches on highways.
Why it matters
The technology might free truck drivers to complete routes more efficiently, but it could also erode their pay and eventually replace many of them altogether.
Key players
Otto; Volvo; Daimler; Peterbilt
Availability
5 to 10 years
Roman Mugriyev was driving his long-haul 18-wheeler down a two-lane Texas highway when he saw an oncoming car drift into his lane just a few hundred feet ahead. There was a ditch to his right and more oncoming cars to his left, so there was little for him to do but hit his horn and brake. “I could hear the man who taught me to drive telling me what he always said was rule number one: ‘Don’t hurt anybody,’” Mugriyev recalls.

But it wasn’t going to work out that way. The errant car collided with the front of Mugriyev’s truck. It shattered his front axle, and he struggled to keep his truck and the wrecked car now fused to it from hitting anyone else as it barreled down the road. After Mugriyev finally came to a stop, he learned that the woman driving the car had been killed in the collision.

Could a computer have done better at the wheel? Or would it have done worse?

We will probably find out in the next few years, because multiple companies are now testing self-driving trucks. Although many technical problems are still unresolved, proponents claim that self-driving trucks will be safer and less costly. “This system often drives better than I do,” says Greg Murphy, who’s been a professional truck driver for 40 years. He now serves as a safety backup driver during tests of self-driving trucks by Otto, a San Francisco company that outfits trucks with the equipment needed to drive themselves.


At first glance, the opportunities and challenges posed by self-driving trucks might seem to merely echo those associated with self-driving cars. But trucks aren’t just long cars. For one thing, the economic rationale for self-driving trucks might be even stronger than the one for driverless cars. Autonomous trucks can coördinate their movements to platoon closely together over long stretches of highway, cutting down on wind drag and saving on fuel. And letting the truck drive itself part of the time figures to help truckers complete their routes sooner.

But the technological obstacles facing autonomous trucks are higher than the ones for self-driving cars. Otto and other companies will need to demonstrate that sensors and code can match the situational awareness of a professional trucker—skills honed by years of experience and training in piloting an easily destabilized juggernaut, with the momentum of 25 Honda Accords, in the face of confusing road hazards, poor surface conditions, and unpredictable car drivers.

And perhaps most important, if self-driving trucks do take hold, they figure to be more controversial than self-driving cars. At a time when our politics and economy are already being upended by the threats that automation poses to jobs (see “The Relentless Pace of Automation”), self-driving trucks will affect an enormous number of blue-collar workers. There are 1.7 million trucking jobs in the U.S., according to the Bureau of Labor Statistics. Technology is unlikely to replace truckers entirely anytime soon. But it will almost certainly alter the nature of the job, and not necessarily in ways that all would welcome.

“We’re not waiting”

Otto’s headquarters, in the once-seedy South of Market section of San Francisco, isn’t much like many of the other tech startups that have transformed the area. Proudly oblivious to that neighborhood upgrade, it’s a barely renovated former furniture warehouse converted to a garage and machine shop, with semi trucks in various states of dismantlement hulking over benches of tools and computers. “No fancy, shiny offices here,” brags Eric Berdinis, Otto’s young and clean-cut-looking product manager.

Berdinis shows off the latest generation of the company’s fast-evolving technology, which is currently installed on Volvo semis. Unlike the bolted-on, kludgy-looking hardware that’s been on testing runs for the past year, the newer versions of the company’s sensor and processing arrays are more sleekly integrated throughout the Volvo cab. The equipment includes four forward-facing video cameras, radar, and a box of accelerometers that Berdinis boasts is “as close as the government allows you to get to missile-guidance quality.”

Particularly key to Otto’s technology is a lidar system, which uses a pulsed laser to amass detailed data about the truck’s surroundings. The current third-party lidar box costs Otto in the vicinity of $100,000 each. But the company has a team designing a proprietary version that could cost less than $10,000.

Inside the cab is a custom-built, liquid-cooled, breadbox-size micro-supercomputer that, Berdinis claims, provides the most computing muscle ever crammed into so small a package. It is needed to crunch the vast stream of sensor data and shepherd it through the guidance algorithms that adjust braking and steering commands to compensate for the truck’s load weight. Rounding out the hardware lineup is a drive-by-wire box to turn the computer’s output into physical truck-control signals. It does this through electromechanical actuators mounted to the truck’s mechanical steering, throttling, and braking systems. Two big red buttons in the cab—Otto calls them the Big Red Buttons—can cut off all self-driving activity. But even without them, the system is designed to yield to any urgent tugs on the steering wheel or heavy pumps of the pedals from anyone in the driver’s seat.

Otto was founded early in 2016 by Anthony Levandowski, who had been with Google’s self-driving-car effort, and Lior Ron, who headed up Google Maps, along with two others. It was a natural move to build on Google’s vast experience with its autonomous cars, which have driven more than two million miles on U.S. roads in several states, with an eye toward the four million trucks in the U.S. alone. Volvo Trucks, Daimler Trucks, and Peterbilt have been working on their own autonomous-truck technology.

Even if drivers stay on in the cab, it’s not clear the economics will work out in their favor.
Then, as further validation, Uber snatched Otto up for a reported $680 million last August. That deal has given Otto’s team access to roughly 500 engineers at Uber working on self-driving technology, according to Berdinis. Levandowski now heads that effort for Uber, which has said it envisions providing an overarching and largely automated transportation network for both goods and people.

Otto has only seven trucks on the road with its technology, but it hopes owners of many more trucks will eventually take on the equipment for free to test it out. Berdinis says the company is working to drive down the cost of the technology to the point where it offers a one- or two-year payback. That’s likely to mean something in the vicinity of $30,000 for a retrofit. “We expect the government to mandate this technology eventually, and for truck manufacturers to integrate it into their vehicles,” says Berdinis. “But new-truck development is on an eight-year cycle, and we’re not waiting.”


Greg Murphy, left,a longtime long-haul trucker, keeps an eye on things during tests of Otto trucks.

Roman Mugriyev, right, wonders how well self-driving trucks would handle dangerous situations.
OTTO
Pay cuts

Last October an Otto-outfitted self-driving truck carried 2,000 cases of Budweiser beer 200 kilometers down Interstate 25 in Colorado from Fort Collins to Colorado Springs—while the truck’s only human driver sat in the sleeper berth at the back of the cab without touching the vehicle’s controls.

That commercial delivery, the first ever to be handled by an autonomous heavy truck, illustrated the potential of the technology. But it also demonstrated the current limitations. The human driver piloted the truck to and from the highway the old-fashioned way, because the technology doesn’t drive on small rural roads or in cities. Even after it was on the highway, a car drove ahead of the truck to make sure the far right lane remained clear. Otto’s system is programmed to stay in that lane, because on many roads trucks are restricted to the far right and are generally considered safer there. And the truck was surrounded by several cars carrying Otto personnel and Colorado State Patrol staff.

In all other testing of Otto-equipped trucks, a professional driver like Greg Murphy sits in the driver’s seat, constantly ready to take the controls at a moment’s notice, even on the highway. Another Otto employee is in the cab as well. Murphy hits the Big Red Buttons when there’s debris on the road, or construction. “My hands are always on the wheel, and I have to concentrate pretty hard to be ready,” says Murphy. “It’s actually harder than normal driving.” (I was invited to sit in on an Otto test ride, but shortly before I was due to show up I was told there had been a scheduling miscommunication and a truck wouldn’t be available. I suspect the cancellation had more to do with that morning’s heavy rain—which can throw off autonomous vehicles—but Otto stuck to its story.)

In fact, Otto insists it has no plans to release products intended to operate trucks without a driver in the cab. “We’re at least a decade away from having trucks with no driver in it,” says Berdinis. But Otto does expect to free up the driver during highway cruising to remain in the back of the cab relaxing, working, or even napping. And therein lies the strongest part of the economic case for self-driving trucks. Drivers are legally restricted to 11 hours of driving a day and 60 hours a week. Given that a new big rig goes for about $150,000, and taking into account the vast delays that pulling over to rest injects into the movement of goods, trucks that can cruise nearly 24/7 could dramatically lower freight costs.

There are other anticipated savings from having trucks drive themselves across America’s 230,000 miles of highway. Fuel is about a third of the cost of operating a long-haul truck, and while drivers are capable of wringing maximum miles per gallon from their trucks, many are too heavy-footed on the pedals. (Berdinis says the best drivers are 30 percent more fuel-efficient than the worst ones.) Otto’s equipment is programmed to keep trucks pegged to optimal speeds and acceleration.


Otto says it has no intention of getting drivers out of the cab entirely—at least for the next decade.
OTTO
Then there’s the potential to cut down on accidents. Truck and bus crashes kill about 4,000 people a year in the U.S. and injure another 100,000. Driver fatigue is a factor in roughly one of seven fatal truck accidents. More than 90 percent of all accidents are caused at least in part by some form of driver error. We don’t yet know what fraction of those errors would be eliminated by autonomous technology—or what new errors might be introduced by it—but tests of self-driving cars suggest the technology will cut down on mistakes.

As long as self-driving trucks require a driver to remain on board, driving jobs seem safe. In some ways those jobs, which pay an average of about $40,000 a year, could even improve. For one thing, driving a truck 11 hours a day is stressful. “You get physically and mentally tired,” says Mugriyev, the driver in the Texas accident, which occurred in 2013. (He was not found to be at fault.) Besides being able to nap and relax in the cab while Otto does the driving, says Berdinis, drivers could use the time away from the wheel to catch up on trucking’s heavy paperwork, locate a “backhaul” load that would pay for the return trip, chat with family and friends, learn a second trade, or run a business. “And while they’re doing it, the drivers are still getting paid for driving,” he says.

These potential benefits could help with recruiting and training truck drivers—a key concern, because there’s actually a big shortage of drivers in both the U.S. and Europe. The American Trucking Associations pegs the current U.S. shortage at about 50,000 drivers and predicts that a total of nearly 900,000 new drivers will be needed over the next eight years. “We have customers calling us up saying they’ll buy 10 new trucks from us if we can provide the drivers, too,” says Carl Johan Almqvist, who heads product safety at Volvo Trucks.

One endorsement of the potential benefits of autonomous trucks to both trucking companies and drivers has come from the state government of Ohio, a trucking hub that’s home to more than 70,000 drivers. The state has committed $15 million to set up a 35-mile stretch of highway outside Columbus for testing self-driving trucks. The heads of both the American Trucking Associations and the Ohio Trucking Association have publicly suggested that autonomous trucks will be good for truckers.

However, the technology is not just a way to make the job more attractive to human drivers; it’s potentially a way for trucking companies to fill in for drivers who aren’t available. And if self-driving systems someday become accepted as capable of standing in for drivers, why keep human drivers on at all? After all, drivers account for a third of the per-mile costs of operating a truck.

Even if, as is likely for the foreseeable future, drivers stay on in the cab of self-driving trucks, it’s not clear the economics will work out in their favor. That’s because there’s currently no regulation that would require companies to pay drivers for the time they spend in the back of the cab. What’s more, freight companies are likely to be forced to convert the cost savings from always-rolling trucks into lower hauling charges in order to compete. Those dropping fees could put pressure on truckers’ pay. “If load prices get pushed down with this technology, the company will say, ‘You didn’t do as much driving, so you don’t make as much,’” says Mugriyev.

Safety questions

Is Otto’s technology up to safely piloting 80,000 pounds of truck down a busy highway? Having a driver in the cab won’t do much to make up for any shortcomings in the system, given that by Otto’s own reckoning it can take up to 30 seconds for a driver resting in the back to fully orient to the driver’s seat.

The extensive history racked up by Google’s self-driving cars is encouraging, with only 20 crashes over seven years and millions of miles. Only one of the crashes was found to be the fault of the car: a traffic merging situation of the sort that Otto hands off to the driver.

But that record doesn’t easily translate into a prediction for the safety of self-driving trucks. As Berdinis notes, trucks can’t swerve to avoid a hazard the way cars can. A fast, hard turn of the steering wheel at high speed would set the truck to fishtailing and possibly jackknifing. From the moment the brakes are applied in a truck going 55 miles per hour, it takes well over the length of a football field for the vehicle to stop. There are only six inches of lane on either side of a truck, meaning even small hazards at the side of the lane can’t be avoided without leaving the lane. “Many avoidance algorithms for self-driving cars just don’t apply to trucks,” says Berdinis.


A key detail not seen in most images of the Budweiser delivery: Otto staff and police riding nearby in cars to ensure safety.
OTTO
One advantage for trucks is that some of the sensors can be mounted at the top of the cab, providing a high-up view that can see over traffic far ahead. But even state-of-the-art sensors can struggle to provide accurate, unambiguous data. Bright sunlight can briefly blind cameras, computers can’t always differentiate between a car by the side of the road and a big sign, and systems can be thrown off by snow, ice, and sand. They also can’t interpret facial expressions and gestures of nearby drivers to predict the driving behavior of other vehicles. And few systems would be able to differentiate between a hitchhiker and a construction worker gesturing to pull over.

Self-driving cars have managed to do well in mostly city driving in spite of these limitations, but at highway speeds and with limited maneuverability, trucks may come up short more often. “We’re still having problems with these challenges,” says Volvo Trucks’ Almqvist. Heavy-truck drivers typically spend months in driving school, and go through thousands of miles of supervised driving, before taking full charge of a big rig. Thus, matching a human driver’s skill is harder for a self-driving truck than it is for a self-driving car. Mugriyev wonders, for example, if an autonomous system would be able to do what he did: wrestle to a safe stop a truck with a blown front axle and a smashed-up car pasted to its front.

Because of such safety concerns, Volvo has no current plans to field its autonomous trucks on public roads. Instead, it intends to limit them to private locations such as mines and ports. “On public roads, we’ll use the technology to support the driver, not to replace the driver,” says Almqvist. Volvo is still unsure about social acceptance of the technology. The company sometimes identifies the license plates of passing cars when testing its autonomous trucks, and then tracks the car owners down and surveys them about their perceptions.

Berdinis acknowledges the challenges, but he insists Otto’s technology is rapidly evolving to meet them. “We won’t ship until we’re confident there are no situations where we’d need a human to immediately take control of the truck,” he says.

Otto will also have to convince regulators its systems are ready for the highway. Unlike Uber, which has relied on the consumer popularity of its passenger service to take to the roads first and wrestle with regulations later, Otto will do everything strictly by the book, notes Berdinis.

Even Volvo’s Almqvist thinks the technology will make it to public roads in the not-too-distant future. But timing will be crucial, he adds: “If we do it too soon and have an accident, we’ll hurt the industry. And if you lose the public’s trust, it’s very difficult to regain it.”

by David H. Freedman",0.01% - 0.1%,"The article discusses the development of self-driving trucks by Otto, a company that aims to revolutionize the trucking industry. The technology involves equipping trucks with sensors and software that enable them to drive autonomously on highways. However, there are several challenges and concerns associated with this technology.

One of the main concerns is safety. While self-driving cars have been tested extensively, the same technology may not be suitable for large trucks, which have limited maneuverability and require a longer stopping distance. Additionally, sensors can struggle to provide accurate data in certain conditions, such as bright sunlight or snow.

Another concern is the impact on truck drivers' jobs. With self-driving trucks, drivers would no longer be needed to drive the vehicles, but they might still be required to ride in the cab and take control of the vehicle if necessary. However, this could lead to a reduction in pay for drivers, as companies may not need to compensate them for time spent driving.

Regulatory issues are also a concern. There is currently no regulation that requires companies to pay drivers for the time they spend in the back of the cab, which could lead to disputes over compensation. Additionally, the use of self-driving trucks on public roads would require regulatory approval, which could be a complex and time-consuming process.

Despite these challenges, Otto's CEO, Lior Ron, is confident that the technology will improve road safety and reduce costs for trucking companies. He notes that human drivers are responsible for the majority of accidents involving large trucks, and that self-driving trucks would eliminate this risk factor.

Other companies, such as Volvo Trucks, are also working on autonomous truck technology, but they are taking a more cautious approach. Volvo plans to limit its autonomous trucks to private locations, such as mines and ports, where the risks are lower.

Overall, the development of self-driving trucks is an exciting innovation that could transform the trucking industry. However, it will require careful consideration of safety, regulatory, and labor issues before it can become a reality on public roads.",Software,"The article discusses the development of self-driving trucks by companies like Otto and Volvo. While these trucks are expected to improve safety and efficiency on the roads, there are still several challenges that need to be addressed.

One concern is the potential impact on truck drivers' jobs. With self-driving trucks, drivers may no longer be needed to drive the vehicle, but they would still be required to ride in the cab to take control of the vehicle if necessary. This could lead to a reduction in driving hours and potentially lower pay for drivers.

Another issue is safety. Self-driving trucks are not yet proven to be as safe as human-driven vehicles, and there are concerns about their ability to handle complex situations on the road. For example, self-driving trucks may not be able to swerve to avoid hazards or stop quickly enough in emergency situations.

The article also notes that regulatory frameworks for self-driving trucks are still unclear, which could create challenges for companies looking to deploy this technology on public roads.

Despite these challenges, many experts believe that self-driving trucks will eventually become a reality. Volvo's approach is to use the technology to support drivers rather than replace them, at least in the short term. Otto is taking a more aggressive approach, aiming to develop a fully autonomous truck that can operate without human intervention.

Ultimately, the success of self-driving trucks will depend on their ability to demonstrate safety and reliability, as well as their potential to improve efficiency and reduce costs for the transportation industry.

Some key points from the article:

* Self-driving trucks are being developed by companies like Otto and Volvo.
* The technology has the potential to improve safety and efficiency on the roads.
* However, there are concerns about the impact on truck drivers' jobs and the potential for accidents.
* Regulatory frameworks for self-driving trucks are still unclear.
* Companies like Volvo are taking a cautious approach to deploying this technology on public roads.
* Otto is aiming to develop a fully autonomous truck that can operate without human intervention."
Paying with Your Face,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/paying-with-your-face/,"Paying with Your Face
Face-detecting systems in China now authorize payments, provide access to facilities, and track down criminals. Will other countries follow?
by Will Knight archive page
February 22, 2017

YOSHI SODEOKA
Paying with Your Face

Breakthrough
Face recognition technology that is finally accurate enough to be widely used in financial transactions and other everyday applications.
Why it matters
The technology offers a secure and extremely convenient method of payment but could raise privacy concerns.
Key players
Face++; Baidu; Alibaba
Availability
Now
Shortly after walking through the door at Face++, a Chinese startup valued at roughly a billion dollars, I see my face, unshaven and looking a bit jet-lagged, flash up on a large screen near the entrance.

Having been added to a database, my face now provides automatic access to the building. It can also be used to monitor my movements through each room inside. As I tour the offices of Face++ (pronounced “face plus plus”), located in a suburb of Beijing, I see it appear on several more screens, automatically captured from countless angles by the company’s software. On one screen a video shows the software tracking 83 different points on my face simultaneously. It’s a little creepy, but undeniably impressive.

Over the past few years, computers have become incredibly good at recognizing faces, and the technology is expanding quickly in China in the interest of both surveillance and convenience. Face recognition might transform everything from policing to the way people interact every day with banks, stores, and transportation services.

Technology from Face++ is already being used in several popular apps. It is possible to transfer money through Alipay, a mobile payment app used by more than 120 million people in China, using only your face as credentials. Meanwhile, Didi, China’s dominant ride-hailing company, uses the Face++ software to let passengers confirm that the person behind the wheel is a legitimate driver. (A “liveness” test, designed to prevent anyone from duping the system with a photo, requires people being scanned to move their head or speak while the app scans them.)


The technology figures to take off in China first because of the country’s attitudes toward surveillance and privacy. Unlike, say, the United States, China has a large centralized database of ID card photos. During my time at Face++, I saw how local governments are using its software to identify suspected criminals in video from surveillance cameras, which are omnipresent in the country. This is especially impressive—albeit somewhat dystopian—because the footage analyzed is far from perfect, and because mug shots or other images on file may be several years old.

Facial recognition has existed for decades, but only now is it accurate enough to be used in secure financial transactions. The new versions use deep learning, an artificial-intelligence technique that is especially effective for image recognition because it makes a computer zero in on the facial features that will most reliably identify a person (see “10 Breakthrough Technologies 2013: Deep Learning”).

“The face recognition market is huge,” says Shiliang Zhang, an assistant professor at Peking University who specializes in machine learning and image processing. Zhang heads a lab not far from the offices of Face++. When I arrived, his students were working away furiously in a dozen or so cubicles. “In China security is very important, and we also have lots of people,” he says. “Lots of companies are working on it.”


Employees simply show their face to gain entry to the company’s headquarters.
One such company is Baidu, which operates China’s most popular search engine, along with other services. Baidu researchers have published papers showing that their software rivals most humans in its ability to recognize a face. In January, the company proved this by taking part in a TV show featuring people who are remarkably good at identifying adults from their baby photos. Baidu’s system outshined them.

Now Baidu is developing a system that lets people pick up rail tickets by showing their face. The company is already working with the government of Wuzhen, a historic tourist destination, to provide access to many of its attractions without a ticket. This involves scanning tens of thousands of faces in a database to find a match, which Baidu says it can do with 99 percent accuracy.


Face++ pinpoints 83 points on a face. The distance between them provides a means of identification.
Jie Tang, an associate professor at Tsinghua University who advised the founders of Face++ as students, says the convenience of the technology is what appeals most to people in China. Some apartment complexes use facial recognition to provide access, and shops and restaurants are looking to the technology to make the customer experience smoother. Not only can he pay for things this way, he says, but the staff in some coffee shops are now alerted by a facial recognition system when he walks in: “They say, ‘Hello, Mr. Tang.’”

by Will Knight",-0.05% to -0.005%,Negative,Software,Product
Practical Quantum Computers,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/practical-quantum-computers/,"Practical Quantum Computers
Advances at Google, Intel, and several research groups indicate that computers with previously unimaginable power are finally within reach.
by Russ Juskalian archive page
February 22, 2017

MATHIJS LABADIE AND RUSS JUSKALIAN
Practical Quantum Computers

Breakthrough
The fabrication of stable qubits, the basic unit of quantum computers.
Why it matters
Quantum computers could be exponentially faster at running artificial-intelligence programs and handling complex simulations and scheduling problems. They could even create uncrackable encryption.
Key players
QuTech; Intel; Microsoft; Google; IBM
Availability
4-5 years
One of the labs at QuTech, a Dutch research institute, is responsible for some of the world’s most advanced work on quantum computing, but it looks like an HVAC testing facility. Tucked away in a quiet corner of the applied sciences building at Delft University of Technology, the space is devoid of people. Buzzing with resonant waves as if occupied by a swarm of electric katydids, it is cluttered by tangles of insulated tubes, wires, and control hardware erupting from big blue cylinders on three and four legs.


Inside the blue cylinders—essentially supercharged refrigerators—spooky quantum-mechanical things are happening where nanowires, semiconductors, and superconductors meet at just a hair above absolute zero. It’s here, down at the limits of physics, that solid materials give rise to so-called quasiparticles, whose unusual behavior gives them the potential to serve as the key components of quantum computers. And this lab in particular has taken big steps toward finally bringing those computers to fruition. In a few years they could rewrite encryption, materials science, pharmaceutical research, and artificial intelligence.

Every year quantum computing comes up as a candidate for this Breakthrough Technologies list, and every year we reach the same conclusion: not yet. Indeed, for years qubits and quantum computers existed mainly on paper, or in fragile experiments to determine their feasibility. (The Canadian company D-Wave Systems has been selling machines it calls quantum computers for a while, using a specialized technology called quantum annealing. The approach, skeptics say, is at best applicable to a very constrained set of computations and might offer no speed advantage over classical systems.) This year, however, a raft of previously theoretical designs are actually being built. Also new this year is the increased availability of corporate funding—from Google, IBM, Intel, and Microsoft, among others—for both research and the development of assorted technologies needed to actually build a working machine: microelectronics, complex circuits, and control software.

The project at Delft, led by Leo Kouwenhoven, a professor who was recently hired by Microsoft, aims to overcome one of the most long-standing obstacles to building quantum computers: the fact that qubits, the basic units of quantum information, are extremely susceptible to noise and therefore error. For qubits to be useful, they must achieve both quantum superposition (a property something like being in two physical states simultaneously) and entanglement (a phenomenon where pairs of qubits are linked so that what happens to one can instantly affect the other, even when they’re physically separated). These delicate conditions are easily upset by the slightest disturbance, like vibrations or fluctuating electric fields.


This blue refrigerator gets down to just above absolute zero, making quantum experiments possible on tiny chips deep inside it. In subsequent photos are scenes from the Delft lab where the experiments are prepared.
MATHIJS LABADIE AND RUSS JUSKALIAN
People have long wrestled with this problem in efforts to build quantum computers, which could make it possible to solve problems so complex they exceed the reach of today’s best computers. But now Kouwenhoven and his colleagues believe the qubits they are creating could eventually be inherently protected—as stable as knots in a rope. “Despite deforming the rope, pulling on it, whatever,” says Kouwenhoven, the knots remain and “you don’t change the information.” Such stability would allow researchers to scale up quantum computers by substantially reducing the computational power required for error correction.

Kouwenhoven’s work relies on manipulating unique quasiparticles that weren’t even discovered until 2012. And it’s just one of several impressive steps being taken. In the same lab, Lieven Vandersypen, backed by Intel, is showing how quantum circuits can be manufactured on traditional silicon wafers.

Quantum computers will be particularly suited to factoring large numbers (making it easy to crack many of today’s encryption techniques and probably providing uncrackable replacements), solving complex optimization problems, and executing machine-learning algorithms. And there will be applications nobody has yet envisioned.

Soon, however, we might have a better idea of what they can do. Until now, researchers have built fully programmable five-qubit computers and more fragile 10- to 20-qubit test systems. Neither kind of machine is capable of much. But the head of Google’s quantum computing effort, Harmut Neven, says his team is on target to build a 49-qubit system by as soon as a year from now. The target of around 50 qubits isn’t an arbitrary one. It’s a threshold, known as quantum supremacy, beyond which no classical supercomputer would be capable of handling the exponential growth in memory and communications bandwidth needed to simulate its quantum counterpart. In other words, the top supercomputer systems can currently do all the same things that five- to 20-qubit quantum computers can, but at around 50 qubits this becomes physically impossible.


MATHIJS LABADIE AND RUSS JUSKALIAN

MATHIJS LABADIE AND RUSS JUSKALIAN
All the academic and corporate quantum researchers I spoke with agreed that somewhere between 30 and 100 qubits—particularly qubits stable enough to perform a wide range of computations for longer durations—is where quantum computers start to have commercial value. And as soon as two to five years from now, such systems are likely to be for sale. Eventually, expect 100,000-qubit systems, which will disrupt the materials, chemistry, and drug industries by making accurate molecular-scale models possible for the discovery of new materials and drugs. And a million-physical-qubit system, whose general computing applications are still difficult to even fathom? It’s conceivable, says Neven, “on the inside of 10 years.”


MATHIJS LABADIE AND RUSS JUSKALIAN

MATHIJS LABADIE AND RUSS JUSKALIAN
What Is a Quantum Computer?

At the heart of quantum computing is the quantum bit, or qubit, a basic unit of information analogous to the 0s and 1s represented by transistors in your computer. Qubits have much more power than classical bits because of two unique properties: they can represent both 1 and 0 at the same time, and they can affect other qubits via a phenomenon known as quantum entanglement. That lets quantum computers take shortcuts to the right answers in certain types of calculations.

by Russ Juskalian",<0.01% to <0.1%,Positive,Hardware,product
The 360-Degree Selfie,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/the-360-degree-selfie/,"The 360-Degree Selfie
Inexpensive cameras that make spherical images are opening a new era in photography and changing the way people share stories.
by Elizabeth Woyke archive page
February 22, 2017

The 360-Degree Selfie

Breakthrough
Consumer cameras that produce 360° images, providing a realistic sense of events or places.
Why it matters
Photos and videos with this perspective could become the new standard for everything from news coverage to vacation shots.
Key players
Ricoh; Samsung; 360fly; JK Imaging (maker of Kodak Pixpro digital cameras); IC Real Tech (maker of the ALLie camera); Humaneyes Technologies
Availability
Now
Seasonal changes to vegetation fascinate Koen Hufkens. So last fall Hufkens, an ecological researcher at Harvard, devised a system to continuously broadcast images from a Massachusetts forest to a website called VirtualForest.io. And because he used a camera that creates 360° pictures, visitors can do more than just watch the feed; they can use their mouse cursor (on a computer) or finger (on a smartphone or tablet) to pan around the image in a circle or scroll up to view the forest canopy and down to see the ground. If they look at the image through a virtual-reality headset they can rotate the photo by moving their head, intensifying the illusion that they are in the woods.

Hufkens says the project will allow him to document how climate change is affecting leaf development in New England. The total cost? About $550, including $350 for the Ricoh Theta S camera that takes the photos.

We experience the world in 360 degrees, surrounded by sights and sounds. Until recently, there were two main options for shooting photos and video that captured that context: use a rig to position multiple cameras at different angles with overlapping fields of view or pay at least $10,000 for a special camera. The production process was just as cumbersome and generally took multiple days to complete. Once you shot your footage, you had to transfer the images to a computer; wrestle with complex, pricey software to fuse them into a seamless picture; and then convert the file into a format that other people could view easily.


Today, anyone can buy a decent 360° camera for less than $500, record a video within minutes, and upload it to Facebook or YouTube. Much of this amateur 360° content is blurry; some of it captures 360 degrees horizontally but not vertically; and most of it is mundane. (Watching footage of a stranger’s vacation is almost as boring in spherical view as it is in regular mode.) But the best user-generated 360° photos and videos—such as the Virtual Forest—deepen the viewer’s appreciation of a place or an event.

Journalists from the New York Times and Reuters are using $350 Samsung Gear 360 cameras to produce spherical photos and videos that document anything from hurricane damage in Haiti to a refugee camp in Gaza. One New York Times video that depicts people in Niger fleeing the militant group Boko Haram puts you in the center of a crowd receiving food from aid groups. You start by watching a man heaving sacks off a pickup truck and hearing them thud onto the ground. When you turn your head, you see the throngs that have gathered to claim the food and the makeshift carts they will use to transport it. The 360° format is so compelling that it could become a new standard for raw footage of news events—something that Twitter is trying to encourage by enabling live spherical videos in its Periscope app.


Kodak Pixpro SP360 4k
It can be mounted on a drone to produce aerial 360° videos.

Or consider the spherical videos of medical procedures that the Los Angeles startup Giblib makes to teach students about surgery. The company films the operations by attaching a $500 360fly 4K camera, which is the size of a baseball, to surgical lights above the patient. The 360° view enables students to see not just the surgeon and surgical site, but also the way the operating room is organized and how the operating room staff interacts.

Meanwhile, inexpensive 360° cameras such as Kodak’s $450 Pixpro SP360 4K are popping up on basketball backboards, football fields, and hockey nets during practice for professional and collegiate teams. Coaches say the resulting videos help players visualize the action and prepare for games in ways that conventional sideline and end-zone videos can’t.

Component innovations

These applications are feasible because of the smartphone boom and innovations in several technologies that combine images from multiple lenses and sensors. For instance, 360° cameras require more horsepower than regular cameras and generate more heat, but that is handled by the energy-efficient chips that power smartphones. Both the 360fly and the $499 ALLie camera use Qualcomm Snapdragon processors similar to those that run Samsung’s high-end handsets.

Camera companies also benefited in recent years from smartphone vendors’ continuous quest to integrate higher-quality imaging into their gadgets. The competition forced component makers like Sony to shrink image sensors and ensure that they offered both high resolution and good performance in low light. As the huge smartphone market helped bring down component prices, 360°-camera makers found it possible to price their devices accessibly, often at less than $500. “There are sensors that now cost $1 instead of $1,000 because they’re used in smartphones, which have incredible economies of scale,” says Jeffrey Martin, the CEO of a 360°-camera startup called Sphericam. Advances in optics played a part as well. Unlike traditional cameras, which have fairly narrow fields of view, 360° cameras sport exaggerated fish-eye lenses that require special optics to align and focus images across multiple points.

Most 360° cameras lack displays and viewfinders. To compensate, camera makers developed apps that you can download to your phone to compose shots and review the resulting images. The cameras connect to the apps wirelessly, and many of them allow you to upload photos and video directly from your phone to Facebook and YouTube. In turn, those sites have made it possible over the past year for people not just to post recorded 360° content but to live-stream 360° videos as well.

Because creating 360° content requires stitching together multiple images, doing it on the fly for live streaming represents an impressive technical achievement. Computer-vision algorithms have simplified the process so that it can  be done on the camera itself, which in turn allows people to live-stream video with minimal delays. (It helps that most consumer-grade cameras have only two lenses and thus one stitch line. Professional versions can have six to 24 lenses.) The ALLie camera supports fast stitching and live-streaming, as do Ricoh’s upcoming Ricoh R development kit camera and Kodak’s Orbit360 4K, which will be available later this year for $500.


Ricoh Theta S
Ricoh put the image sensors on the camera’s sides instead of behind its lenses, making its thin shape possible.

Spherical cameras represented 1 percent of worldwide consumer camera shipments in 2016 and are set to reach 4 percent in 2017, according to the research firm Futuresource Consulting. The popularity of these devices will benefit the virtual-reality industry as well as camera makers. You don’t need special VR gear to view spherical videos, but YouTube says many people look at them on smartphones slipped into VR headsets, such as Google’s Cardboard and Daydream devices. And more people experimenting with 360° cameras means more content for other people to watch in VR.

In fact, John Carmack, the chief technology officer of Facebook’s Oculus VR subsidiary, has predicted that people will spend less than 50 percent of their VR time playing games. Instead, they may don VR headsets to do things like virtually attend a wedding.


Samsung Gear 360
Samsung has given these cameras to New York Times and Reuters journalists who are producing 360° news coverage.
Once people discover spherical videos, research suggests, they shift their viewing behavior quickly. The company Humaneyes, which is developing an $800 camera that can produce 3-D spherical images, says people need to watch only about 10 hours of 360° content before they instinctively start trying to interact with all videos. When you see 360° imagery that truly transports you somewhere else, you want it more and more.

by Elizabeth Woyke",0.01% - 0.1%,Positive,Hardware,"The article discusses the growing trend of spherical cameras, also known as 360-degree cameras, which can capture immersive, panoramic views of a scene. These cameras are becoming increasingly popular among consumers, professionals, and even news organizations.

The article highlights several examples of how spherical cameras are being used in various fields, such as:

* News coverage: Journalists from The New York Times and Reuters are using Samsung's Gear 360 camera to produce immersive news coverage.
* Sports: Coaches and teams are using 360-degree cameras to analyze gameplay and prepare for matches.
* Education: Spherical videos of medical procedures are being used to teach students about surgery.
* Virtual reality (VR): Spherical cameras are being used to create immersive VR experiences, such as virtually attending a wedding.

The article attributes the growth of spherical cameras to several factors, including:

* Advances in smartphone technology: The development of energy-efficient chips and high-quality image sensors has made it possible for 360-degree cameras to be compact, affordable, and powerful.
* Improvements in optics: Specialized lenses and optics have been developed to align and focus images across multiple points.
* Increased accessibility: Camera makers have developed apps that allow users to compose shots, review images, and upload content directly from their phones.

The article also notes that creating 360-degree content requires stitching together multiple images, which can be done on the camera itself or through computer-vision algorithms. This process has been simplified, allowing for live-streaming of 360-degree videos with minimal delays.

According to research firm Futuresource Consulting, spherical cameras represented 1% of worldwide consumer camera shipments in 2016 and are expected to reach 4% in 2017. The popularity of these devices is also expected to benefit the VR industry, as more people experiment with 360-degree cameras and create content for VR headsets.

Overall, the article suggests that spherical cameras are becoming increasingly popular and are being used in a wide range of applications, from news coverage and sports analysis to education and virtual reality."
Hot Solar Cells,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/hot-solar-cells/,"Hot Solar Cells
By converting heat to focused beams of light, a new solar device could create cheap and continuous power.
by James Temple archive page
February 22, 2017

KEN RICHARDSO

The absorber-emitter layer is situated above an optical filter and photovoltaic cell, which is visible underneath.
KEN RICHARDSO
Hot Solar Cells

Breakthrough
A solar power device that could theoretically double the efficiency of conventional solar cells.
Why it matters
The new design could lead to inexpensive solar power that keeps working after the sun sets.
Key players
David Bierman, Marin Soljacic, and Evelyn Wang, MIT - Vladimir Shalaev, Purdue University; Andrej Lenert, University of Michigan; Ivan Celanovic, MIT
Availability
10 to 15 years
Solar panels cover a growing number of rooftops, but even decades after they were first developed, the slabs of silicon remain bulky, expensive, and inefficient. Fundamental limitations prevent these conventional photovoltaics from absorbing more than a fraction of the energy in sunlight.


But a team of MIT scientists has built a different sort of solar energy device that uses inventive engineering and advances in materials science to capture far more of the sun’s energy. The trick is to first turn sunlight into heat and then convert it back into light, but now focused within the spectrum that solar cells can use. While various researchers have been working for years on so-called solar thermophotovoltaics, the MIT device is the first one to absorb more energy than its photovoltaic cell alone, demonstrating that the approach could dramatically increase efficiency.

Standard silicon solar cells mainly capture the visual light from violet to red. That and other factors mean that they can never turn more than around 32 percent of the energy in sunlight into electricity. The MIT device is still a crude prototype, operating at just 6.8 percent efficiency—but with various enhancements it could be roughly twice as efficient as conventional photovoltaics.

The key step in creating the device was the development of something called an absorber-emitter. It essentially acts as a light funnel above the solar cells. The absorbing layer is built from solid black carbon nanotubes that capture all the energy in sunlight and convert most of it into heat. As temperatures reach around 1,000 °C, the adjacent emitting layer radiates that energy back out as light, now mostly narrowed to bands that the photovoltaic cells can absorb. The emitter is made from a photonic crystal, a structure that can be designed at the nanoscale to control which wavelengths of light flow through it. Another critical advance was the addition of a highly specialized optical filter that transmits the tailored light while reflecting nearly all the unusable photons back. This “photon recycling” produces more heat, which generates more of the light that the solar cell can absorb, improving the efficiency of the system.


Black carbon nanotubes sit on top of the absorber-emitter layer, collecting energy across the solar spectrum and converting it to heat.
KEN RICHARDSO
There are some downsides to the MIT team’s approach, including the relatively high cost of certain components. It also currently works only in a vacuum. But the economics should improve as efficiency levels climb, and the researchers now have a clear path to achieving that. “We can further tailor the components now that we’ve improved our understanding of what we need to get to higher efficiencies,” says Evelyn Wang, an associate professor who helped lead the effort.

The researchers are also exploring ways to take advantage of another strength of solar thermophotovoltaics. Because heat is easier to store than electricity, it should be possible to divert excess amounts generated by the device to a thermal storage system, which could then be used to produce electricity even when the sun isn’t shining. If the researchers can incorporate a storage device and ratchet up efficiency levels, the system could one day deliver clean, cheap—and continuous—solar power.


Concentrated light from a solar simulator shines through the window of a vacuum chamber, where it reaches the solar thermophotovoltaic device and generates electricity.
KEN RICHARDSO
by James Temple",0.01% - 0.1%,Positive.,Hardware,product
Gene Therapy 2.0,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/gene-therapy-2-0/,"Gene Therapy 2.0
Scientists have solved fundamental problems that were holding back cures for rare hereditary disorders. Next we’ll see if the same approach can take on cancer, heart disease, and other common illnesses.
by Emily Mullin archive page
February 22, 2017

ARMANDO VEVE
Gene Therapy 2.0

Breakthrough
First gene therapies on track for approval in the U.S. More are on the way.
Why it matters
Thousands of diseases stem from an error in a single gene. New treatments could cure them.
Key players
Spark Therapeutics; BioMarin; BlueBird Bio; GenSight Biologics; UniQure
Availability
Now
When Kala Looks gave birth to fraternal twin boys in January 2015, she and her husband, Philip, had no idea that one of them was harboring a deadly mutation in his genes.

At three months old, their son Levi was diagnosed with severe combined immune deficiency, or SCID, which renders the body defenseless against infections. Levi’s blood had only a few immune cells essential to fighting disease. Soon he would lose them and have no immune system at all.

Kala and Philip frantically began sanitizing their home to keep Levi alive. They got rid of the family cat, sprayed every surface with Lysol, and boiled the twins’ toys in hot water. Philip would strap on a surgical mask when he came home from work.

At first, Kala and Philip thought their only option was to get Levi a bone marrow transplant, but they couldn’t find a match for him. Then they learned about an experimental gene therapy at Boston Children’s Hospital. It was attempting to treat children like Levi by replacing the gene responsible for destroying his immune system.

“I thought, this isn’t real,” Kala says. “There’s no way this could work.”

Nonetheless, the Lookses flew from their home in Michigan to Boston in May 2015. Days later, Levi got an infusion of the therapy into his veins. He has been a normal boy ever since—and he has even grown larger than his twin brother. Babies born with SCID typically didn’t survive past two years old. Now, a one-time treatment offers a cure for patients like Levi Looks.

Researchers have been chasing the dream of gene therapy for decades. The idea is elegant: use an engineered virus to deliver healthy copies of a gene into patients with defective versions. But until recently it had produced more disappointments than successes. The entire field was slowed in 1999 when an 18-year-old patient with a liver disease, Jesse Gelsinger, died in a gene-therapy experiment.

Gene-Therapy Time Line

But now, crucial puzzles have been solved and gene therapies are on the verge of curing devastating genetic disorders. Two gene therapies for inherited diseases—Strimvelis for a form of SCID and Glybera for a disorder that makes fat build up in the bloodstream—have won regulatory approval in Europe. In the United States, Spark Therapeutics could be the first to market; it has a treatment for a progressive form of blindness. Other gene therapies in development point to a cure for hemophilia and relief from an incapacitating skin disorder called epidermolysis bullosa.

Fixing rare diseases, impressive in its own right, could be just the start. Researchers are studying gene therapy in clinical trials for about 40 to 50 different diseases, says Maria-Grazia Roncarolo, a pediatrician and scientist at Stanford University who led early gene-therapy experiments in Italy that laid the foundation for Strimvelis. That’s up from just a few conditions 10 years ago. And in addition to treating disorders caused by malfunctions in single genes, researchers are looking to engineer these therapies for more common diseases, like Alzheimer’s, diabetes, heart failure, and cancer. Harvard geneticist George Church has said that someday, everyone may be able to take gene therapy to combat the effects of aging.


Early gene therapies failed in part because of the delivery mechanism. In 1990, a four-year-old girl with a form of SCID was treated by scientists at the National Institutes of Health, who extracted white blood cells from her, inserted normal copies of her faulty gene into them, then injected her with the corrected cells. But patients later treated for a different type of SCID went on to develop leukemia. The new genetic material and the virus used to carry it into cells were delivered to the wrong part of the genome, which switched on cancer-causing genes in some patients. In Gelsinger’s case, the virus used to transport functioning genes into his cells made his immune system go into overdrive, leading to multiple organ failure and brain death.

Gene-therapy researchers have surmounted many of those early problems by using viruses that are more efficient at transporting new genetic material into cells.

But several challenges remain. While gene therapies have been developed for several relatively rare diseases, creating such treatments for more common diseases that have complex genetic causes will be far more difficult. In diseases like SCID and hemophilia, scientists know the precise genetic mutation that is to blame. But diseases like Alzheimer’s, diabetes, and heart failure involve multiple genes—and the same ones aren’t all involved in all people with those conditions.

Nonetheless, for Kala and Philip Looks, the success of gene therapy is already real. A treatment they had never heard of rid their child of a horrific disease.

by Emily Mullin",0.05% - 0.5%,Positive,Biotech,Methodology
The Cell Atlas,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/the-cell-atlas/,"The Cell Atlas
Biology’s next mega-project will find out what we’re really made of.
by Steve Connor archive page
February 22, 2017

FRED TOMASELLI AND JAMES COHAN, NEW YORK
The Cell Atlas

Breakthrough
A master catalog of every cell type in the human body.
Why it matters
Super-accurate models of human physiology will speed up the discovery and testing of new drugs.
Key players
Broad Institute; Sanger Institute; Chan Zuckerberg Biohub
Availability
5 years
In 1665, Robert Hooke peered down his microscope at a piece of cork and discovered little boxes that reminded him of rooms in a monastery. Being the first scientist to describe cells, Hooke would be amazed by biology’s next mega-project: a scheme to individually capture and scrutinize millions of cells using the most powerful tools in modern genomics and cell biology. 

The objective is to construct the first comprehensive “cell atlas,” or map of human cells, a technological marvel that should comprehensively reveal, for the first time, what human bodies are actually made of and provide scientists a sophisticated new model of biology that could speed the search for drugs.

To perform the task of cataloguing the 37.2 trillion cells of the human body, an international consortium of scientists from the U.S., U.K., Sweden, Israel, the Netherlands, and Japan is being assembled to assign each a molecular signature and also give each type a zip code in the three-dimensional space of our bodies.

“We will see some things that we expect, things we know to exist, but I’m sure there will be completely novel things,” says Mike Stubbington, head of the cell atlas team at the Sanger Institute in the U.K. “I think there will be surprises.”


Previous attempts at describing cells, from the hairy neurons that populate the brain and spinal cord to the glutinous fat cells of the skin, suggest there are about 300 variations in total. But the true figure is undoubtedly larger. Analyzing molecular differences between cells has already revealed, for example, two new types of retinal cells that escaped decades of investigation of the eye; a cell that forms the first line of defense against pathogens and makes up four in every 10,000 blood cells; and a newly spotted immune cell that uniquely produces a steroid that appears to suppress the immune response.

Three technologies are coming together to make this new type of mapping possible. The first is known as “cellular microfluidics.” Individual cells are separated, tagged with tiny beads, and manipulated in droplets of oil that are shunted like cars down the narrow, one-way streets of artificial capillaries etched into a tiny chip, so they can be corralled, cracked open, and studied one by one.

The second is the ability to identify the genes active in single cells by decoding them in superfast and efficient sequencing machines at a cost of just a few cents per cell. One scientist can now process 10,000 cells in a single day.

The third technology uses novel labeling and staining techniques that can locate each type of cell—on the basis of its gene activity—at a specific zip code in a human organ or tissue.

Behind the cell atlas are big-science powerhouses including Britain’s Sanger Institute, the Broad Institute of MIT and Harvard, and a new “Biohub” in California funded by Facebook CEO Mark Zuckerberg. In September Zuckerberg and his wife, Priscilla Chan, made the cell atlas the inaugural target of a $3 billion donation to medical research.  

by Steve Connor",0.01% - 0.1%,Positive,Biotech,Project
Botnets of Things,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/botnets-of-things/,"Botnets of Things
The relentless push to add connectivity to home gadgets is creating dangerous side effects that figure to get even worse.
by Bruce Schneier archive page
February 22, 2017

ROBERT BEATTY
Botnets of Things

Breakthrough
Malware that takes control of webcams, video recorders, and other consumer devices to cause widespread Internet outages.
Why it matters
Botnets based on this software are disrupting larger and larger swaths of the Internet—and getting harder to stop.
Key players
Whoever created the Mirai botnet software; Anyone who runs a poorly secured device online—including you?
Availability
Now
Botnets have existed for at least a decade. As early as 2000, hackers were breaking into computers over the Internet and controlling them en masse from centralized systems. Among other things, the hackers used the combined computing power of these botnets to launch distributed denial-of-service attacks, which flood websites with traffic to take them down.

But now the problem is getting worse, thanks to a flood of cheap webcams, digital video recorders, and other gadgets in the “Internet of things.” Because these devices typically have little or no security, hackers can take them over with little effort. And that makes it easier than ever to build huge botnets that take down much more than one site at a time.

In October, a botnet made up of 100,000 compromised gadgets knocked an Internet infrastructure provider partially offline. Taking down that provider, Dyn, resulted in a cascade of effects that ultimately caused a long list of high-profile websites, including Twitter and Netflix, to temporarily disappear from the Internet. More attacks are sure to follow: the botnet that attacked Dyn was created with publicly available malware called Mirai that largely automates the process of coöpting computers.


The best defense would be for everything online to run only secure software, so botnets couldn’t be created in the first place. This isn’t going to happen anytime soon. Internet of things devices are not designed with security in mind and often have no way of being patched. The things that have become part of Mirai botnets, for example, will be vulnerable until their owners throw them away. Botnets will get larger and more powerful simply because the number of vulnerable devices will go up by orders of magnitude over the next few years.

What do hackers do with them? Many things.

Botnets are used to commit click fraud. Click fraud is a scheme to fool advertisers into thinking that people are clicking on, or viewing, their ads. There are lots of ways to commit click fraud, but the easiest is probably for the attacker to embed a Google ad in a Web page he owns. Google ads pay a site owner according to the number of people who click on them. The attacker instructs all the computers on his botnet to repeatedly visit the Web page and click on the ad. Dot, dot, dot, PROFIT! If the botnet makers figure out more effective ways to siphon revenue from big companies online, we could see the whole advertising model of the Internet crumble.

Similarly, botnets can be used to evade spam filters, which work partly by knowing which computers are sending millions of e-mails. They can speed up password guessing to break into online accounts, mine bitcoins, and do anything else that requires a large network of computers. This is why botnets are big businesses. Criminal organizations rent time on them.

But the botnet activities that most often make headlines are denial-of-service attacks. Dyn seems to have been the victim of some angry hackers, but more financially motivated groups use these attacks as a form of extortion. Political groups use them to silence websites they don’t like. Such attacks will certainly be a tactic in any future cyberwar.


This map shows the extent of some of the Internet outages caused by denial-of-service attacks on Dyn on October 21, 2016. Dyn operates domain-name servers that connect end users to websites.
OUTAGE DATA FROM DOWNDETECTOR
Once you know a botnet exists, you can attack its command-and-control system. When botnets were rare, this tactic was effective. As they get more common, this piecemeal defense will become less so. You can also secure yourself against the effects of botnets. For example, several companies sell defenses against denial-of-service attacks. Their effectiveness varies, depending on the severity of the attack and the type of service.

But overall, the trends favor the attacker. Expect more attacks like the one against Dyn in the coming year.

by Bruce Schneier",0.1% to 1.0%,Negative,Hardware,Concept
Reinforcement Learning,2017.0,https://www.technologyreview.com/10-breakthrough-technologies/2017/,https://www.technologyreview.com/technology/reinforcement-learning/,"Reinforcement Learning
By experimenting, computers are figuring out how to do things that no programmer could teach them.
by Will Knight archive page
February 22, 2017

SEYMOUR CHWAST
Reinforcement Learning

Breakthrough
An approach to artificial intelligence that gets computers to learn like people, without explicit instruction.
Why it matters
Progress in self-­driving cars and other forms of automation will slow dramatically unless machines can hone skills through experience.
Key players
DeepMind; Mobileye; OpenAI; Google; Uber
Availability
1 to 2 years
Inside a simple computer simulation, a group of self-driving cars are performing a crazy-looking maneuver on a four-lane virtual highway. Half are trying to move from the right-hand lanes just as the other half try to merge from the left. It seems like just the sort of tricky thing that might flummox a robot vehicle, but they manage it with precision.

I’m watching the driving simulation at the biggest artificial-intelligence conference of the year, held in Barcelona this past December. What’s most amazing is that the software governing the cars’ behavior wasn’t programmed in the conventional sense at all. It learned how to merge, slickly and safely, simply by practicing. During training, the control software performed the maneuver over and over, altering its instructions a little with each attempt. Most of the time the merging happened way too slowly and cars interfered with each other. But whenever the merge went smoothly, the system would learn to favor the behavior that led up to it.

This approach, known as reinforcement learning, is largely how AlphaGo, a computer developed by a subsidiary of Alphabet called DeepMind, mastered the impossibly complex board game Go and beat one of the best human players in the world in a high-profile match last year. Now reinforcement learning may soon inject greater intelligence into much more than games. In addition to improving self-driving cars, the technology can get a robot to grasp objects it has never seen before, and it can figure out the optimal configuration for the equipment in a data center.

Reinforcement learning copies a very simple principle from nature. The psychologist Edward Thorndike documented it more than 100 years ago. Thorndike placed cats inside boxes from which they could escape only by pressing a lever. After a considerable amount of pacing around and meowing, the animals would eventually step on the lever by chance. After they learned to associate this behavior with the desired outcome, they eventually escaped with increasing speed.


Some of the very earliest artificial-intelligence researchers believed that this process might be usefully reproduced in machines. In 1951, Marvin Minsky, a student at Harvard who would become one of the founding fathers of AI as a professor at MIT, built a machine that used a simple form of reinforcement learning to mimic a rat learning to navigate a maze. Minsky’s Stochastic Neural Analogy Reinforcement Computer, or SNARC, consisted of dozens of tubes, motors, and clutches that simulated the behavior of 40 neurons and synapses. As a simulated rat made its way out of a virtual maze, the strength of some synaptic connections would increase, thereby reinforcing the underlying behavior.

There were few successes over the next few decades. In 1992, Gerald Tesauro, a researcher at IBM, demonstrated a program that used the technique to play backgammon. It became skilled enough to rival the best human players, a landmark achievement in AI. But reinforcement learning proved difficult to scale to more complex problems. “People thought it was a cool idea that didn’t really work,” says David Silver, a researcher at DeepMind in the U.K. and a leading proponent of reinforcement learning today.

That view changed dramatically in March 2016, however. That’s when AlphaGo, a program trained using reinforcement learning, destroyed one of the best Go players of all time, South Korea’s Lee Sedol. The feat was astonishing, because it is virtually impossible to build a good Go-playing program with conventional programming. Not only is the game extremely complex, but even accomplished Go players may struggle to say why certain moves are good or bad, so the principles of the game are difficult to write into code. Most AI researchers had expected that it would take a decade for a computer to play the game as well as an expert human.

Jostling for position

Silver, a mild-mannered Brit who became fascinated with artificial intelligence as an undergraduate at the University of Cambridge, explains why reinforcement learning has recently become so formidable. He says that the key is combining it with deep learning, a technique that involves using a very large simulated neural network to recognize patterns in data (see “10 Breakthrough Technologies 2013: Deep Learning”).

Reinforcement learning works because researchers figured out how to get a computer to calculate the value that should be assigned to, say, each right or wrong turn that a rat might make on its way out of its maze. Each value is stored in a large table, and the computer updates all these values as it learns. For large and complicated tasks, this becomes computationally impractical. In recent years, however, deep learning has proved an extremely efficient way to recognize patterns in data, whether the data refers to the turns in a maze, the positions on a Go board, or the pixels shown on screen during a computer game.

In fact, it was in games that DeepMind made its name. In 2013 it published details of a program capable of learning to play various Atari video games at a superhuman level, leading Google to acquire the company for more than $500 million in 2014. These and other feats have in turn inspired other researchers and companies to turn to reinforcement learning. A number of industrial-robot makers are testing the approach as a way to train their machines to perform new tasks without manual programming. And researchers at Google, also an Alphabet subsidiary, worked with DeepMind to use deep reinforcement learning to make its data centers more energy efficient. It is difficult to figure out how all the elements in a data center will affect energy usage, but a reinforcement-learning algorithm can learn from collated data and experiment in simulation to suggest, say, how and when to operate the cooling systems.


These images are from the Mobileye vision system for cars, which will benefit from reinforcement learning.
MOBILEYE
But the setting where you will probably most notice this software’s remarkably humanlike behavior is in self-driving cars. Today’s driverless vehicles often falter in complex situations that involve interacting with human drivers, such as traffic circles or four-way stops. If we don’t want them to take unnecessary risks, or to clog the roads by being overly hesitant, they will need to acquire more nuanced driving skills, like jostling for position in a crowd of other cars.

The highway merging software was demoed in Barcelona by Mobileye, an Israeli automotive company that makes vehicle safety systems used by dozens of carmakers, including Tesla Motors (see “50 Smartest Companies 2016”). After screening the merging clip, Shai Shalev-Shwartz, Mobileye’s vice president for technology, shows some of the challenges self-driving cars will face: a bustling roundabout in Jerusalem; a frenetic intersection in Paris; and a hellishly chaotic scene from a road in India. “If a self-driving car follows the law precisely, then during rush hour I might wait in a merge situation for an hour,” Shalev-Shwartz says.

Mobileye plans to test the software on a fleet of vehicles in collaboration with BMW and Intel later this year. Both Google and Uber say they are also testing reinforcement learning for their self-driving vehicles.

Reinforcement learning is being applied in a growing number of areas, says Emma Brunskill, an assistant professor at Stanford University who specializes in the approach. But she says it is well suited to automated driving because it enables “good sequences of decisions.” Progress would proceed much more slowly if programmers had to encode all such decisions into cars in advance.

But there are challenges to overcome, too. Andrew Ng, chief scientist at the Chinese company Baidu, warns that the approach requires a huge amount of  data, and that many of its successes have come when a computer could practice relentlessly in simulations. Indeed, researchers are still figuring out just how to make reinforcement learning work in complex situations in which there is more than one objective. Mobileye has had to tweak its protocols so a self-driving car that is adept at avoiding accidents won’t be more likely to cause one for someone else.


Reinforcement learning led to AlphaGo’s stunning victory over a human Go champion last year.
DEEPMIND
When you watch the outlandish merging demo, it looks as though the company has succeeded, at least so far. But later this year, perhaps on a highway near you, reinforcement learning will get its most dramatic and important tests to date. 

by Will Knight",0.1% - 1.0%,Positive,Software,"The article discusses the recent advancements in artificial intelligence (AI) through reinforcement learning, a technique that enables machines to learn from their experiences and make decisions based on trial and error. The approach has been successful in various areas, including playing complex games like Go and Atari video games.

The article highlights the work of DeepMind, a UK-based AI startup acquired by Google in 2014, which developed an algorithm that defeated a human world champion in Go last year. This achievement was significant because Go is a highly complex game that requires intuition and creativity, making it difficult for computers to master.

Reinforcement learning has also been applied to other areas, such as autonomous driving. Mobileye, an Israeli automotive company, demonstrated a system that enables self-driving cars to merge onto highways and navigate complex intersections using reinforcement learning.

The article notes that reinforcement learning is well-suited for autonomous driving because it allows the car to learn from its experiences and make decisions in real-time. However, the approach requires a large amount of data and can be challenging to implement in complex situations with multiple objectives.

Experts, including Andrew Ng, chief scientist at Baidu, and Emma Brunskill, an assistant professor at Stanford University, comment on the potential of reinforcement learning and its applications in various areas.

The article concludes by noting that reinforcement learning will face its most significant tests later this year when it is deployed in self-driving cars. The success or failure of these tests will have a significant impact on the future of autonomous driving and AI research.

Some key points from the article:

* Reinforcement learning is a technique that enables machines to learn from their experiences and make decisions based on trial and error.
* DeepMind's algorithm defeated a human world champion in Go last year, demonstrating the power of reinforcement learning.
* Mobileye demonstrated a system that enables self-driving cars to merge onto highways and navigate complex intersections using reinforcement learning.
* Reinforcement learning is well-suited for autonomous driving because it allows the car to learn from its experiences and make decisions in real-time.
* The approach requires a large amount of data and can be challenging to implement in complex situations with multiple objectives."
3-D Metal Printing,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"3-D Metal Printing

DEREK BRAHNEY
3-D Metal Printing

Breakthrough
Now printers can make metal objects quickly and cheaply.
Why it matters
The ability to make large and complex metal ­objects on demand could transform manufacturing.
Key players
Markforged, Desktop Metal, GE
Availability
Now
While 3-D printing has been around for decades, it has remained largely in the domain of hobbyists and designers producing one-off prototypes. And printing objects with anything other than plastics—in particular, metal—has been expensive and painfully slow.

Now, however, it’s becoming cheap and easy enough to be a potentially practical way of manufacturing parts. If widely adopted, it could change the way we mass-produce many products.

In the short term, manufacturers wouldn’t need to maintain large inventories—they could simply print an object, such as a replacement part for an aging car, whenever someone needs it.

In the longer term, large factories that mass-produce a limited range of parts might be replaced by smaller ones that make a wider variety, adapting to customers’ changing needs.

The technology can create lighter, stronger parts, and complex shapes that aren’t possible with conventional metal fabrication methods. It can also provide more precise control of the microstructure of metals. In 2017, researchers from the Lawrence Livermore National Laboratory announced they had developed a 3-D-printing method for creating stainless-steel parts twice as strong as traditionally made ones. 

Also in 2017, 3-D-printing company Markforged, a small startup based outside Boston, released the first 3-D metal printer for under $100,000.

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Another Boston-area startup, Desktop Metal, began to ship its first metal prototyping machines in December 2017. It plans to begin selling larger machines, designed for manufacturing, that are 100 times faster than older metal printing methods.

The printing of metal parts is also getting easier. Desktop Metal now offers software that generates designs ready for 3-D printing. Users tell the program the specs of the object they want to print, and the software produces a computer model suitable for printing.   

GE, which has long been a proponent of using 3-D printing in its aviation products (see “10 Breakthrough Technologies of 2013: Additive Manufacturing”), has a test version of its new metal printer that is fast enough to make large parts. The company plans to begin selling the printer in 2018.

by Erin Winick",0.05% - 0.5%,Positive,Hardware,Platform
Babel-fish Earbuds,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Babel-Fish Earbuds

GOOGLE
Babel-Fish Earbuds

Breakthrough
Near-real-time translation now works for a large number of languages and is easy to use.
Why it matters
In an increasingly global world, language is still a barrier to communication.
Key players
Google and Baidu
Availability
Now
In the cult sci-fi classic The Hitchhiker’s Guide to the Galaxy, you slide a yellow Babel fish into your ear to get translations in an instant. In the real world, Google has come up with an interim solution: a $159 pair of earbuds, called Pixel Buds. These work with its Pixel smartphones and Google Translate app to produce practically real-time translation.

One person wears the earbuds, while the other holds a phone. The earbud wearer speaks in his or her language—English is the default—and the app translates the talking and plays it aloud on the phone. The person holding the phone responds; this response is translated and played through the earbuds.

Google Translate already has a conversation feature, and its iOS and Android apps let two users speak as it automatically figures out what languages they’re using and then translates them. But background noise can make it hard for the app to understand what people are saying, and also to figure out when one person has stopped speaking and it’s time to start translating.

Pixel Buds get around these problems because the wearer taps and holds a finger on the right earbud while talking. Splitting the interaction between the phone and the earbuds gives each person control of a microphone and helps the speakers maintain eye contact, since they’re not trying to pass a phone back and forth.

The Pixel Buds were widely panned for subpar design. They do look silly, and they may not fit well in your ears. They can also be hard to set up with a phone.

Clunky hardware can be fixed, though. Pixel Buds show the promise of mutually intelligible communication between languages in close to real time. And no fish required.

by Rachel Metz",0.01% - 0.1%,Positive,Software,product
The Sensing City,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Sensing City

SIDEWALK TORONTO
Sensing City

Breakthrough
A Toronto neighborhood aims to be the first place to successfully integrate cutting-edge urban design with state-of-the-art digital technology.
Why it matters
Smart cities could make urban areas more affordable, livable, and environmentally friendly.
Key players
Sidewalk Labs and Waterfront Toronto
Availability
Project announced in October 2017; construction could begin in 2019
Numerous smart-city schemes have run into delays, dialed down their ambitious goals, or priced out everyone except the super-wealthy. A new project in Toronto, called Quayside, is hoping to change that pattern of failures by rethinking an urban neighborhood from the ground up and rebuilding it around the latest digital technologies.

Alphabet’s Sidewalk Labs, based in New York City, is collaborating with the Canadian government on the high-tech project, slated for Toronto’s industrial waterfront.

One of the project’s goals is to base decisions about design, policy, and technology on information from an extensive network of sensors that gather data on everything from air quality to noise levels to people’s activities.

The plan calls for all vehicles to be autonomous and shared. Robots will roam underground doing menial chores like delivering the mail. Sidewalk Labs says it will open access to the software and systems it’s creating so other companies can build services on top of them, much as people build apps for mobile phones.

The company intends to closely monitor public infrastructure, and this has raised concerns about data governance and privacy. But Sidewalk Labs believes it can work with the community and the local government to alleviate those worries.

“What’s distinctive about what we’re trying to do in Quayside is that the project is not only extraordinarily ambitious but also has a certain amount of humility,” says Rit Aggarwala, the executive in charge of Sidewalk Labs’ urban-systems planning. That humility may help Quayside avoid the pitfalls that have plagued previous smart-city initiatives.

Other North American cities are already clamoring to be next on Sidewalk Labs’ list, according to Waterfront Toronto, the public agency overseeing Quayside’s development. “San Francisco, Denver, Los Angeles, and Boston have all called asking for introductions,” says the agency’s CEO, Will Fleissig.

by Elizabeth Woyke",1.5% - 15.4%,Positive,Hardware,Concept
AI for Everyone,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"AI for Everybody

MIGUEL PORLAN
AI for Everybody

Breakthrough
Cloud-based AI is making the technology cheaper and easier to use.
Why it matters
Right now the use of AI is dominated by a relatively few companies, but as a cloud-based service, it could be widely available to many more, giving the economy a boost.
Key players
Amazon; Google; Microsoft
Availability
Now
Artificial intelligence has so far been mainly the plaything of big tech companies like Amazon, Baidu, Google, and Microsoft, as well as some startups. For many other companies and parts of the economy, AI systems are too expensive and too difficult to implement fully.

What’s the solution? Machine-learning tools based in the cloud are bringing AI to a far broader audience. So far, Amazon dominates cloud AI with its AWS subsidiary. Google is challenging that with TensorFlow, an open-source AI library that can be used to build other machine-learning software. Recently Google announced Cloud AutoML, a suite of pre-trained systems that could make AI simpler to use.

Microsoft, which has its own AI-powered cloud platform, Azure, is teaming up with Amazon to offer Gluon, an open-source deep-learning library. Gluon is supposed to make building neural nets—a key technology in AI that crudely mimics how the human brain learns—as easy as building a smartphone app.

It is uncertain which of these companies will become the leader in offering AI cloud services.  But it is a huge business opportunity for the winners.

These products will be essential if the AI revolution is going to spread more broadly through different parts of the economy.

Currently AI is used mostly in the tech industry, where it has created efficiencies and produced new products and services. But many other businesses and industries have struggled to take advantage of the advances in artificial intelligence. Sectors such as medicine, manufacturing, and energy could also be transformed if they were able to implement the technology more fully, with a huge boost to economic productivity.

Most companies, though, still don’t have enough people who know how to use cloud AI. So Amazon and Google are also setting up consultancy services. Once the cloud puts the technology within the reach of almost everyone, the real AI revolution can begin.

by Jackie Snow",1-10%,Positive,Software,Concept
Dueling Neural Networks,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Dueling Neural Networks

ILLUSTRATION BY DEREK BRAHNEY | DIAGRAM COURTESY OF MICHAEL NIELSEN, “NEURAL NETWORKS AND DEEP LEARNING”, DETERMINATION PRESS, 2015
Dueling Neural Networks

Breakthrough
Two AI systems can spar with each other to create ultra-realistic original images or sounds, something machines have never been able to do before.
Why it matters
This gives machines something akin to a sense of imagination, which may help them become less reliant on humans—but also turns them into alarmingly powerful tools for digital fakery.
Key players
Google Brain, DeepMind, Nvidia
Availability
Now
Artificial intelligence is getting very good at identifying things: show it a million pictures, and it can tell you with uncanny accuracy which ones depict a pedestrian crossing a street. But AI is hopeless at generating images of pedestrians by itself. If it could do that, it would be able to create gobs of realistic but synthetic pictures depicting pedestrians in various settings, which a self-driving car could use to train itself without ever going out on the road.

The problem is, creating something entirely new requires imagination—and until now that has perplexed AIs.

The solution first occurred to Ian Goodfellow, then a PhD student at the University of Montreal, during an academic argument in a bar in 2014. The approach, known as a generative adversarial network, or GAN, takes two neural networks—the simplified mathematical models of the human brain that underpin most modern machine learning—and pits them against each other in a digital cat-and-mouse game.

Both networks are trained on the same data set. One, known as the generator, is tasked with creating variations on images it’s already seen—perhaps a picture of a pedestrian with an extra arm. The second, known as the discriminator, is asked to identify whether the example it sees is like the images it has been trained on or a fake produced by the generator—basically, is that three-armed person likely to be real?

Over time, the generator can become so good at producing images that the discriminator can’t spot fakes. Essentially, the generator has been taught to recognize, and then create, realistic-looking images of pedestrians.

The technology has become one of the most promising advances in AI in the past decade, able to help machines produce results that fool even humans.

GANs have been put to use creating realistic-sounding speech and photorealistic fake imagery. In one compelling example, researchers from chipmaker Nvidia primed a GAN with celebrity photographs to create hundreds of credible faces of people who don’t exist. Another research group made not-unconvincing fake paintings that look like the works of van Gogh. Pushed further, GANs can reimagine images in different ways—making a sunny road appear snowy, or turning horses into zebras.

The results aren’t always perfect: GANs can conjure up bicycles with two sets of handlebars, say, or faces with eyebrows in the wrong place. But because the images and sounds are often startlingly realistic, some experts believe there’s a sense in which GANs are beginning to understand the underlying structure of the world they see and hear. And that means AI may gain, along with a sense of imagination, a more independent ability to make sense of what it sees in the world.

by Jamie Condliffe",0.05% - 0.5%,Negative,Software,Methodology
Materials' Quantum Leap,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Materials’ Quantum Leap

JEREMY LIEBMAN
Materials’ Quantum Leap

Breakthrough
IBM has simulated the electronic structure of a small molecule, using a seven-qubit quantum computer.
Why it matters
Understanding molecules in exact detail will allow chemists to design more effective drugs and better materials for generating and distributing energy.
Key players
IBM; Google; Harvard’s Alán Aspuru-Guzik
Availability
5 to 10 years
The prospect of powerful new quantum computers comes with a puzzle. They’ll be capable of feats of computation inconceivable with today’s machines, but we haven’t yet figured out what we might do with those powers.

One likely and enticing possibility: precisely designing molecules.

Chemists are already dreaming of new proteins for far more effective drugs, novel electrolytes for better batteries, compounds that could turn sunlight directly into a liquid fuel, and much more efficient solar cells.

We don’t have these things because molecules are ridiculously hard to model on a classical computer. Try simulating the behavior of the electrons in even a relatively simple molecule and you run into complexities far beyond the capabilities of today’s computers.

But it’s a natural problem for quantum computers, which instead of digital bits representing 1s and 0s use “qubits” that are themselves quantum systems. Recently, IBM researchers used a quantum computer with seven qubits to model a small molecule made of three atoms.

It should become possible to accurately simulate far larger and more interesting molecules as scientists build machines with more qubits and, just as important, better quantum algorithms.

by David Rotman",0.01% - 1.0%,Positive.,Nanotech,project
Zero-carbon Natural Gas,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Zero-Carbon Natural Gas

MIGUEL PORLAN
Zero-Carbon Natural Gas

Breakthrough
A power plant efficiently and cheaply captures carbon released by burning natural gas, avoiding greenhouse-gas emissions.
Why it matters
Around 32 percent of US electricity is produced with natural gas, accounting for around 30 percent of the power sector’s carbon emissions.
Key players
8 Rivers Capital; Exelon Generation; CB&I
Availability
3 to 5 years
The world is probably stuck with natural gas as one of our primary sources of electricity for the foreseeable future. Cheap and readily available, it now accounts for more than 30 percent of US electricity and 22 percent of world electricity. And although it’s cleaner than coal, it’s still a massive source of carbon emissions.

A pilot power plant just outside Houston, in the heart of the US petroleum and refining industry, is testing a technology that could make clean energy from natural gas a reality. The company behind the 50-megawatt project, Net Power, believes it can generate power at least as cheaply as standard natural-gas plants and capture essentially all the carbon dioxide released in the process.

If so, it would mean the world has a way to produce carbon-free energy from a fossil fuel at a reasonable cost. Such natural-gas plants could be cranked up and down on demand, avoiding the high capital costs of nuclear power and sidestepping the unsteady supply that renewables generally provide.

Net Power is a collaboration between technology development firm 8 Rivers Capital, Exelon Generation, and energy construction firm CB&I. The company is in the process of commissioning the plant and has begun initial testing. It intends to release results from early evaluations in the months ahead.

The plant puts the carbon dioxide released from burning natural gas under high pressure and heat, using the resulting supercritical CO2 as the “working fluid” that drives a specially built turbine. Much of the carbon dioxide can be continuously recycled; the rest can be captured cheaply.

A key part of pushing down the costs depends on selling that carbon dioxide. Today the main use is in helping to extract oil from petroleum wells. That’s a limited market, and not a particularly green one. Eventually, however, Net Power hopes to see growing demand for carbon dioxide in cement manufacturing and in making plastics and other carbon-based materials.

Net Power’s technology won’t solve all the problems with natural gas, particularly on the extraction side. But as long as we’re using natural gas, we might as well use it as cleanly as possible. Of all the clean-energy technologies in development, Net Power’s is one of the furthest along to promise more than a marginal advance in cutting carbon emissions.

by James Temple",-0.01% to -0.1%,Positive,Climate Tech,Methodology
Perfect Online Privacy,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Perfect Online Privacy

MIGUEL PORLAN
Perfect Online Privacy

Breakthrough
Computer scientists are perfecting a cryptographic tool for proving something without revealing the information underlying the proof.
Why it matters
If you need to disclose personal information to get something done online, it will be easier to do so without risking your privacy or exposing yourself to identity theft.
Key players
Zcash; JPMorgan Chase; ING
Availability
Now
True internet privacy could finally become possible thanks to a new tool that can—for instance—let you prove you’re over 18 without revealing your date of birth, or prove you have enough money in the bank for a financial transaction without revealing your balance or other details. That limits the risk of a privacy breach or identity theft.

The tool is an emerging cryptographic protocol called a zero-­knowledge proof. Though researchers have worked on it for decades, interest has exploded in the past year, thanks in part to the growing obsession with cryptocurrencies, most of which aren’t private.

Much of the credit for a practical zero-knowledge proof goes to Zcash, a digital currency that launched in late 2016. Zcash’s developers used a method called a zk-SNARK (for “zero-knowledge succinct non-interactive argument of knowledge”) to give users the power to transact anonymously.

That’s not normally possible in Bitcoin and most other public blockchain systems, in which transactions are visible to everyone. Though these transactions are theoretically anonymous, they can be combined with other data to track and even identify users. Vitalik Buterin, creator of Ethereum, the world’s second-most-popular blockchain network, has described zk-SNARKs as an “absolutely game-changing technology.”

For banks, this could be a way to use blockchains in payment systems without sacrificing their clients’ privacy. Last year, JPMorgan Chase added zk-SNARKs to its own blockchain-based payment system.

For all their promise, though, zk-SNARKs are computation-heavy and slow. They also require a so-called “trusted setup,” creating a cryptographic key that could compromise the whole system if it fell into the wrong hands. But researchers are looking at alternatives that deploy zero-knowledge proofs more efficiently and don’t require such a key.

by Mike Orcutt",-0.1% to -0.01%,positive,Software,concept
Artificial Embryos,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Artificial Embryos

UNIVERSITY OF CAMBRIDGE
Artificial Embryos

Breakthrough
Without using eggs or sperm cells, researchers have made embryo-like structures from stem cells alone, providing a whole new route to creating life.
Why it matters
Artificial embryos will make it easier for researchers to study the mysterious beginnings of a human life, but they’re stoking new bioethical debates.
Key players
University of Cambridge; University of Michigan; Rockefeller University
Availability
Now
In a breakthrough that redefines how life can be created, embryologists working at the University of Cambridge in the UK have grown realistic-looking mouse embryos using only stem cells. No egg. No sperm. Just cells plucked from another embryo.

The researchers placed the cells carefully in a three-dimensional scaffold and watched, fascinated, as they started communicating and lining up into the distinctive bullet shape of a mouse embryo several days old.

“We know that stem cells are magical in their powerful potential of what they can do. We did not realize they could self-organize so beautifully or perfectly,” Magdelena Zernicka­-Goetz, who headed the team, told an interviewer at the time.

Zernicka-Goetz says her “synthetic” embryos probably couldn’t have grown into mice. Nonetheless, they’re a hint that soon we could have mammals born without an egg at all.

That isn’t Zernicka-Goetz’s goal. She wants to study how the cells of an early embryo begin taking on their specialized roles. The next step, she says, is to make an artificial embryo out of human stem cells, work that’s being pursued at the University of Michigan and Rockefeller University.

Synthetic human embryos would be a boon to scientists, letting them tease apart events early in development. And since such embryos start with easily manipulated stem cells, labs will be able to employ a full range of tools, such as gene editing, to investigate them as they grow.

Artificial embryos, however, pose ethical questions. What if they turn out to be indistinguishable from real embryos? How long can they be grown in the lab before they feel pain? We need to address those questions before the science races ahead much further, bioethicists say.

by Antonio Regalado",0.00% - 0.01%,Negative,Biotech,Concept
Genetic Fortune-Telling,2018.0,https://www.technologyreview.com/10-breakthrough-technologies/2018/,,"Genetic Fortune-Telling

DEREK BRAHNEY
Genetic Fortune-Telling

Breakthrough
Scientists can now use your genome to predict your chances of getting heart disease or breast cancer, and even your IQ.
Why it matters
DNA-based predictions could be the next great public health advance, but they will increase the risks of genetic discrimination.
Key players
Helix; 23andMe; Myriad Genetics; UK Biobank; Broad Institute
Availability
Now
One day, babies will get DNA report cards at birth. These reports will offer predictions about their chances of suffering a heart attack or cancer, of getting hooked on tobacco, and of being smarter than average.

The science making these report cards possible has suddenly arrived, thanks to huge genetic studies—some involving more than a million people.

It turns out that most common diseases and many behaviors and traits, including intelligence, are a result of not one or a few genes but many acting in concert. Using the data from large ongoing genetic studies, scientists are creating what they call “polygenic risk scores.”

Though the new DNA tests offer probabilities, not diagnoses, they could greatly benefit medicine. For example, if women at high risk for breast cancer got more mammograms and those at low risk got fewer, those exams might catch more real cancers and set off fewer false alarms.

Pharmaceutical companies can also use the scores in clinical trials of preventive drugs for such illnesses as Alzheimer’s or heart disease. By picking volunteers who are more likely to get sick, they can more accurately test how well the drugs work.

The trouble is, the predictions are far from perfect. Who wants to know they might develop Alzheimer’s? What if someone with a low risk score for cancer puts off being screened, and then develops cancer anyway?

Polygenic scores are also controversial because they can predict any trait, not only diseases. For instance, they can now forecast about 10 percent of a person’s performance on IQ tests. As the scores improve, it’s likely that DNA IQ predictions will become routinely available. But how will parents and educators use that information?

To behavioral geneticist Eric ­Turk­heimer, the chance that genetic data will be used for both good and bad is what makes the new technology “simultaneously exciting and alarming.”

by Antonio Regalado",-0.01% to -0.001%,Negative,Biotech,Methodology
Robot Dexterity,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Robot dexterity

NICOLAS ORTEGA
Robot dexterity

Why it matters
If robots could learn to deal with the messiness of the real world, they could do many more tasks.
Key players
OpenAI, Carnegie Mellon University, University of Michigan, UC Berkeley
Availability
3-5 years
Robots are teaching themselves to handle the physical world.

For all the talk about machines taking jobs, industrial robots are still clumsy and inflexible. A robot can repeatedly pick up a component on an assembly line with amazing precision and without ever getting bored—but move the object half an inch, or replace it with something slightly different, and the machine will fumble ineptly or paw at thin air.

But while a robot can’t yet be programmed to figure out how to grasp any object just by looking at it, as people do, it can now learn to manipulate the object on its own through virtual trial and error.

One such project is Dactyl, a robot that taught itself to flip a toy building block in its fingers. Dactyl, which comes from the San Francisco nonprofit OpenAI, consists of an off-the-shelf robot hand surrounded by an array of lights and cameras. Using what’s known as reinforcement learning, neural-network software learns how to grasp and turn the block within a simulated environment before the hand tries it out for real. The software experiments, randomly at first, strengthening connections within the network over time as it gets closer to its goal.

It usually isn’t possible to transfer that type of virtual practice to the real world, because things like friction or the varied properties of different materials are so difficult to simulate. The OpenAI team got around this by adding randomness to the virtual training, giving the robot a proxy for the messiness of reality.

We’ll need further breakthroughs for robots to master the advanced dexterity needed in a real warehouse or factory. But if researchers can reliably employ this kind of learning, robots might eventually assemble our gadgets, load our dishwashers, and even help Grandma out of bed.

by Will Knight",0.1% - 1.0%,Positive,Hardware,project
New-wave Nuclear Power,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"New-wave nuclear power

BOB MUMGAARD/PLASMA SCIENCE AND FUSION CENTER/MIT
Advanced fusion and fission reactors are edging closer to reality. 

New nuclear designs that have gained momentum in the past year are promising to make this power source safer and cheaper. Among them are generation IV fission reactors, an evolution of traditional designs; small modular reactors; and fusion reactors, a technology that has seemed eternally just out of reach. Developers of generation IV fission designs, such as Canada’s Terrestrial Energy and Washington-based TerraPower, have entered into R&D partnerships with utilities, aiming for grid supply (somewhat optimistically, maybe) by the 2020s.

Small modular reactors typically produce in the tens of megawatts of power (for comparison, a traditional nuclear reactor produces around 1,000 MW). Companies like Oregon’s NuScale say the miniaturized reactors can save money and reduce environmental and financial risks.

There has even been progress on fusion. Though no one expects delivery before 2030, companies like General Fusion and Commonwealth Fusion Systems, an MIT spinout, are making some headway. Many consider fusion a pipe dream, but because the reactors can’t melt down and don’t create long-lived, high-level waste, it should face much less public resistance than conventional nuclear. (Bill Gates is an investor in TerraPower and Commonwealth Fusion Systems.)

by Leigh Phillips",-0.01% to -0.001%,Positive,Hardware,concept
Predicting Preemies,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Predicting preemies

NENOV | GETTY
Predicting preemies

Why it matters
15 million babies are born prematurely every year; it’s the leading cause of death for children under age five
Key players
Akna Dx
Availability
A test could be offered in doctor’s offices within five years
A simple blood test can predict if a pregnant woman is at risk of giving birth prematurely.

Our genetic material lives mostly inside our cells. But small amounts of “cell-free” DNA and RNA also float in our blood, often released by dying cells. In pregnant women, that cell-free material is an alphabet soup of nucleic acids from the fetus, the placenta, and the mother.

Stephen Quake, a bioengineer at Stanford, has found a way to use that to tackle one of medicine’s most intractable problems: the roughly one in 10 babies born prematurely.

Free-floating DNA and RNA can yield information that previously required invasive ways of grabbing cells, such as taking a biopsy of a tumor or puncturing a pregnant woman’s belly to perform an amniocentesis. What’s changed is that it’s now easier to detect and sequence the small amounts of cell-free genetic material in the blood. In the last few years researchers have begun developing blood tests for cancer (by spotting the telltale DNA from tumor cells) and for prenatal screening of conditions like Down syndrome.

The tests for these conditions rely on looking for genetic mutations in the DNA. RNA, on the other hand, is the molecule that regulates gene expression—how much of a protein is produced from a gene. By sequencing the free-floating RNA in the mother’s blood, Quake can spot fluctuations in the expression of seven genes that he singles out as associated with preterm birth. That lets him identify women likely to deliver too early. Once alerted, doctors can take measures to stave off an early birth and give the child a better chance of survival.

The technology behind the blood test, Quake says, is quick, easy, and less than $10 a measurement. He and his collaborators have launched a startup, Akna Dx, to commercialize it.

by Bonnie Rochman",0.0001% - 0.001%,Positive,Biotech,product
Gut Probe in a Pill,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Gut probe in a pill

BRUCE PETERSON
Gut probe in a pill

Why it matters
The device makes it easier to screen for and study gut diseases, including one that keeps millions of children in poor countries from growing properly
Key players
Massachusetts General Hospital
Availability
Now used in adults; testing in infants begins in 2019
A small, swallowable device captures detailed images of the gut without anesthesia, even in infants and children. 

Environmental enteric dysfunction (EED) may be one of the costliest diseases you’ve never heard of. Marked by inflamed intestines that are leaky and absorb nutrients poorly, it’s widespread in poor countries and is one reason why many people there are malnourished, have developmental delays, and never reach a normal height. No one knows exactly what causes EED and how it could be prevented or treated.

Practical screening to detect it would help medical workers know when to intervene and how. Therapies are already available for infants, but diagnosing and studying illnesses in the guts of such young children often requires anesthetizing them and inserting a tube called an endoscope down the throat. It’s expensive, uncomfortable, and not practical in areas of the world where EED is prevalent.

So Guillermo Tearney, a pathologist and engineer at Massachusetts General Hospital (MGH) in Boston, is developing small devices that can be used to inspect the gut for signs of EED and even obtain tissue biopsies. Unlike endoscopes, they are simple to use at a primary care visit.

Tearney’s swallowable capsules contain miniature microscopes. They’re attached to a flexible string-like tether that provides power and light while sending images to a briefcase-like console with a monitor. This lets the health-care worker pause the capsule at points of interest and pull it out when finished, allowing it to be sterilized and reused. (Though it sounds gag-­inducing, Tearney’s team has developed a technique that they say doesn’t cause discomfort.) It can also carry technologies that image the entire surface of the digestive tract at the resolution of a single cell or capture three-dimensional cross sections a couple of millimeters deep.

The technology has several applications; at MGH it’s being used to screen for Barrett’s esophagus, a precursor of esophageal cancer. For EED, Tearney’s team has developed an even smaller version for use in infants who can’t swallow a pill. It’s been tested on adolescents in Pakistan, where EED is prevalent, and infant testing is planned for 2019.

The little probe will help researchers answer questions about EED’s development—such as which cells it affects and whether bacteria are involved—and evaluate interventions and potential treatments.

by Courtney Humphries",0.0001% - 0.001%,Positive,Biotech,product
Custom Cancer Vaccines,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Custom cancer vaccines

PAPER BOAT CREATIVE | GETTY
Custom cancer vaccines

Why it matters
Conventional chemotherapies take a heavy toll on healthy cells and aren’t always effective against tumors
Key players
BioNTech, Genentech
Availability
In human testing
The treatment incites the body’s natural defenses to destroy only cancer cells by identifying mutations unique to each tumor

Scientists are on the cusp of commercializing the first personalized cancer vaccine. If it works as hoped, the vaccine, which triggers a person’s immune system to identify a tumor by its unique mutations, could effectively shut down many types of cancers.

By using the body’s natural defenses to selectively destroy only tumor cells, the vaccine, unlike conventional chemotherapies, limits damage to healthy cells. The attacking immune cells could also be vigilant in spotting any stray cancer cells after the initial treatment.

The possibility of such vaccines began to take shape in 2008, five years after the Human Genome Project was completed, when geneticists published the first sequence of a cancerous tumor cell.

Soon after, investigators began to compare the DNA of tumor cells with that of healthy cells—and other tumor cells. These studies confirmed that all cancer cells contain hundreds if not thousands of specific mutations, most of which are unique to each tumor.

A few years later, a German startup called BioNTech provided compelling evidence that a vaccine containing copies of these mutations could catalyze the body’s immune system to produce T cells primed to seek out, attack, and destroy all cancer cells harboring them.

In December 2017, BioNTech began a large test of the vaccine in cancer patients, in collaboration with the biotech giant Genentech. The ongoing trial is targeting at least 10 solid cancers and aims to enroll upwards of 560 patients at sites around the globe.

The two companies are designing new manufacturing techniques to produce thousands of personally customized vaccines cheaply and quickly. That will be tricky because creating the vaccine involves performing a biopsy on the patient’s tumor, sequencing and analyzing its DNA, and rushing that information to the production site. Once produced, the vaccine needs to be promptly delivered to the hospital; delays could be deadly.

by Adam Piore",0.01% to 0.1%,Positive,Biotech,product
The Cow-free Burger,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"The cow-free burger

BRUCE PETERSON/STYLING: MONICA MARIANO
The cow-free burger

Why it matters
Livestock production causes catastrophic deforestation, water pollution, and greenhouse-gas emissions
Key players
Beyond Meat, Impossible Foods
Availability
5-10 years
Both lab-grown and plant-based alternatives approximate the taste and nutritional value of real meat without the environmental devastation.

The UN expects the world to have 9.8 billion people by 2050. And those people are getting richer. Neither trend bodes well for climate change—especially because as people escape poverty, they tend to eat more meat.

By that date, according to the predictions, humans will consume 70% more meat than they did in 2005. And it turns out that raising animals for human consumption is among the worst things we do to the environment.

Depending on the animal, producing a pound of meat protein with Western industrialized methods requires 4 to 25 times more water, 6 to 17 times more land, and 6 to 20 times more fossil fuels than producing a pound of plant protein.

The problem is that people aren’t likely to stop eating meat anytime soon. Which means lab-grown and plant-based alternatives might be the best way to limit the destruction.

Making lab-grown meat involves extracting muscle tissue from animals and growing it in bioreactors. The end product looks much like what you’d get from an animal, although researchers are still working on the taste. Researchers at Maastricht University in the Netherlands, who are working to produce lab-grown meat at scale, believe they'll have a lab-grown burger available by next year. One drawback of lab-grown meat is that the environmental benefits are still sketchy at best—a recent World Economic Forum report says the emissions from lab-grown meat would be only around 7% less than emissions from beef production.

The better environmental case can be made for plant-based meats from companies like Beyond Meat and Impossible Foods (Bill Gates is an investor in both companies), which use pea proteins, soy, wheat, potatoes, and plant oils to mimic the texture and taste of animal meat.

Beyond Meat has a new 26,000-square-foot (2,400-square-meter) plant in California and has already sold upwards of 25 million burgers from 30,000 stores and restaurants. According to an analysis by the Center for Sustainable Systems at the University of Michigan, a Beyond Meat patty would probably generate 90% less in greenhouse-gas emissions than a conventional burger made from a cow.

by Markkus Rovito",0.01% - 0.1%,Positive,Biotech,product
Carbon Dioxide Catcher,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Carbon dioxide catcher

NICO ORTEGA
Carbon dioxide catcher

Why it matters
Removing CO2 from the atmosphere might be one of the last viable ways to stop catastrophic climate change
Key players
Carbon Engineering, Climeworks, Global Thermostat
Availability
5-10 years
Practical and affordable ways to capture carbon dioxide from the air can soak up excess greenhouse-gas emissions.

Even if we slow carbon dioxide emissions, the warming effect of the greenhouse gas can persist for thousands of years. To prevent a dangerous rise in temperatures, the UN’s climate panel now concludes, the world will need to remove as much as 1 trillion tons of carbon dioxide from the atmosphere this century.

In a surprise finding last summer, Harvard climate scientist David Keith calculated that machines could, in theory, pull this off for less than $100 a ton, through an approach known as direct air capture. That’s an order of magnitude cheaper than earlier estimates that led many scientists to dismiss the technology as far too expensive—though it will still take years for costs to fall to anywhere near that level.

But once you capture the carbon, you still need to figure out what to do with it.

Carbon Engineering, the Canadian startup Keith cofounded in 2009, plans to expand its pilot plant to ramp up production of its synthetic fuels, using the captured carbon dioxide as a key ingredient. (Bill Gates is an investor in Carbon Engineering.)

Zurich-based Climeworks’s direct air capture plant in Italy will produce methane from captured carbon dioxide and hydrogen, while a second plant in Switzerland will sell carbon dioxide to the soft-drinks industry. So will Global Thermostat of New York, which finished constructing its first commercial plant in Alabama last year.

Still, if it’s used in synthetic fuels or sodas, the carbon dioxide will mostly end up back in the atmosphere. The ultimate goal is to lock greenhouse gases away forever. Some could be nested within products like carbon fiber, polymers, or concrete, but far more will simply need to be buried underground, a costly job that no business model seems likely to support.

In fact, pulling CO2 out of the air is, from an engineering perspective, one of the most difficult and expensive ways of dealing with climate change. But given how slowly we’re reducing emissions, there are no good options left.

by James Temple",-0.05% to -0.005%,Positive,Climate Tech,Methodology
An ECG on Your Wrist,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"An ECG on your wrist

BRUCE PETERSON
Regulatory approval and technological advances are making it easier for people to continuously monitor their hearts with wearable devices.

Fitness trackers aren’t serious medical devices. An intense workout or loose band can mess with the sensors that read your pulse. But an electrocardiogram—the kind doctors use to diagnose abnormalities before they cause a stroke or heart attack— requires a visit to a clinic, and people often fail to take the test in time.

ECG-enabled smart watches, made possible by new regulations and innovations in hardware and software, offer the convenience of a wearable device with something closer to the precision of a medical one.

An Apple Watch–compatible band from Silicon Valley startup AliveCor that can detect atrial fibrillation, a frequent cause of blood clots and stroke, received clearance from the FDA in 2017. Last year, Apple released its own FDA-cleared ECG feature, embedded in the watch itself.

The health-device company Withings also announced plans for an ECG-equipped watch shortly after.
Current wearables still employ only a single sensor, whereas a real ECG has 12. And no wearable can yet detect a heart attack as it’s happening.

But this might change soon. Last fall, AliveCor presented preliminary results to the American Heart Association on an app and two-­sensor system that can detect a certain type of heart attack.

by Karen Hao",0.01% - 0.1%,Positive,Hardware,product
Sanitation Without Sewers,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Sanitation without sewers

THEDMAN | GETTY
Sanitation without sewers

Why it matters
2.3 billion people lack safe sanitation, and many die as a result
Key players
Duke University, University of South Florida, Biomass Controls, California Institute of Technology
Availability
1-2 years
Energy-efficient toilets can operate without a sewer system and treat waste on the spot.

About 2.3 billion people don’t have good sanitation. The lack of proper toilets encourages people to dump fecal matter into nearby ponds and streams, spreading bacteria, viruses, and parasites that can cause diarrhea and cholera. Diarrhea causes one in nine child deaths worldwide.

Now researchers are working to build a new kind of toilet that’s cheap enough for the developing world and can not only dispose of waste but treat it as well.

In 2011 Bill Gates created what was essentially the X Prize in this area—the Reinvent the Toilet Challenge. Since the contest’s launch, several teams have put prototypes in the field. All process the waste locally, so there’s no need for large amounts of water to carry it to a distant treatment plant.

Most of the prototypes are self-contained and don’t need sewers, but they look like traditional toilets housed in small buildings or storage containers. The NEWgenerator toilet, designed at the University of South Florida, filters out pollutants with an anaerobic membrane, which has pores smaller than bacteria and viruses. Another project, from Connecticut-based Biomass Controls, is a refinery the size of a shipping container; it heats the waste to produce a carbon-rich material that can, among other things, fertilize soil.

One drawback is that the toilets don’t work at every scale. The Biomass Controls product, for example, is designed primarily for tens of thousands of users per day, which makes it less well suited for smaller villages. Another system, developed at Duke University, is meant to be used only by a few nearby homes.

So the challenge now is to make these toilets cheaper and more adaptable to communities of different sizes. “It’s great to build one or two units,” says Daniel Yeh, an associate professor at the University of South Florida, who led the NEWgenerator team. “But to really have the technology impact the world, the only way to do that is mass-produce the units.”

by Erin Winick",0.01% - 0.1%,Positive,Climate Tech,Concept
Smooth-talking AI Assistants,2019.0,https://www.technologyreview.com/10-breakthrough-technologies/2019/,,"Smooth-talking AI assistants

BRUCE PETERSON
Smooth-talking AI assistants

Why it matters
AI assistants can now perform conversation-based tasks like booking a restaurant reservation or coordinating a package drop-off rather than just obey simple commands
Key players
Google, Alibaba, Amazon
Availability
1-2 years
New techniques that capture semantic relationships between words are making machines better at understanding natural language.

We’re used to AI assistants—Alexa playing music in the living room, Siri setting alarms on your phone—but they haven’t really lived up to their alleged smarts. They were supposed to have simplified our lives, but they’ve barely made a dent. They recognize only a narrow range of directives and are easily tripped up by deviations.

But some recent advances are about to expand your digital assistant’s repertoire. In June 2018, researchers at OpenAI developed a technique that trains an AI on unlabeled text to avoid the expense and time of categorizing and tagging all the data manually. A few months later, a team at Google unveiled a system called BERT that learned how to predict missing words by studying millions of sentences. In a multiple-choice test, it did as well as humans at filling in gaps.

These improvements, coupled with better speech synthesis, are letting us move from giving AI assistants simple commands to having conversations with them. They’ll be able to deal with daily minutiae like taking meeting notes, finding information, or shopping online.

Some are already here. Google Duplex, the eerily human-like upgrade of Google Assistant, can pick up your calls to screen for spammers and telemarketers. It can also make calls for you to schedule restaurant reservations or salon appointments.

In China, consumers are getting used to Alibaba’s AliMe, which coordinates package deliveries over the phone and haggles about the price of goods over chat.

But while AI programs have gotten better at figuring out what you want, they still can’t understand a sentence. Lines are scripted or generated statistically, reflecting how hard it is to imbue machines with true language understanding. Once we cross that hurdle, we’ll see yet another evolution, perhaps from logistics coordinator to babysitter, teacher—or even friend?

by Karen Hao",0.5% - 1.5%,Positive,Software,product
Unhackable Internet,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Unhackable internet

YOSHI SODEOKA
Unhackable internet

Why it matters
The internet is increasingly vulnerable to hacking; a quantum one would be unhackable.
Key players
Delft University of Technology, Quantum Internet Alliance, University of Science and Technology of China
Availability
5 years
Later this year, Dutch researchers will complete a quantum internet between Delft and the Hague.

An internet based on quantum physics will soon enable inherently secure communication. A team led by Stephanie Wehner, at Delft University of Technology, is building a network connecting four cities in the Netherlands entirely by means of quantum technology. Messages sent over this network will be unhackable.

In the last few years, scientists have learned to transmit pairs of photons across fiber-optic cables in a way that absolutely protects the information encoded in them. A team in China used a form of the technology to construct a 2,000-kilometer network backbone between Beijing and Shanghai—but that project relies partly on classical components that periodically break the quantum link before establishing a new one, introducing the risk of hacking.

The Delft network, in contrast, will be the first to transmit information between cities using quantum techniques from end to end.

The technology relies on a quantum behavior of atomic particles called entanglement. Entangled photons can’t be covertly read without disrupting their content.

But entangled particles are difficult to create, and harder still to transmit over long distances. Wehner’s team has demonstrated it can send them more than 1.5 kilometers (0.93 miles), and they are confident they can set up a quantum link between Delft and the Hague by around the end of this year. Ensuring an unbroken connection over greater distances will require quantum repeaters that extend the network.

Such repeaters are currently in design at Delft and elsewhere. The first should be completed in the next five to six years, says Wehner, with a global quantum network following by the end of the decade.

by Russ Juskalian",0.01% - 0.1%,Positive,Hardware,concept
Hyper-personalized Medicine,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Hyper-personalized medicine

JULIA DUFOSSÉ
Hyper-personalized medicine

Why it matters
Genetic medicine tailored to a single patient means hope for people whose ailments were previously uncurable.
Key players
A-T Children’s Project, Boston Children’s Hospital, Ionis Pharmaceuticals, US Food & Drug Administration
Availability
Now
Novel drugs are being designed to treat unique genetic mutations.

Here’s a definition of a hopeless case: a child with a fatal disease so exceedingly rare that not only is there no treatment, there’s not even anyone in a lab coat studying it. “Too rare to care,” goes the saying.

That’s about to change, thanks to new classes of drugs that can be tailored to a person’s genes. If an extremely rare disease is caused by a specific DNA mistake—as several thousand are—there’s now at least a fighting chance for a genetic fix.

One such case is that of Mila Makovec, a little girl suffering from a devastating illness caused by a unique genetic mutation, who got a drug manufactured just for her. Her case made the New England Journal of Medicine in October, after doctors moved from a readout of her genetic error to a treatment in just a year. They called the drug milasen, after her.

The treatment hasn’t cured Mila. But it seems to have stabilized her condition: it has reduced her seizures, and she has begun to stand and walk with assistance.

Mila’s treatment was possible because creating a gene medicine has never been faster or had a better chance of working. The new medicines might take the form of gene replacement, gene editing, or antisense (the type Mila received), a sort of molecular eraser, which erases or fixes erroneous genetic messages. What the treatments have in common is that they can be programmed, in digital fashion and with digital speed, to correct or compensate for inherited diseases, letter for DNA letter.

How many stories like Mila’s are there? So far, just a handful.

But more are on the way. Where researchers would have once seen obstacles and said “I’m sorry,” they now see solutions in DNA and think maybe they can help.

The real challenge for “n-of-1” treatments (a reference to the number of people who get the drug) is that they defy just about every accepted notion of how pharmaceuticals should be developed, tested, and sold. Who will pay for these drugs when they help one person, but still take large teams to design and manufacture?

by Antonio Regalado",0.05% - 0.5%,Positive,Biotech,product
Digital Money,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Digital money
The rise of digital currency has massive ramifications for financial privacy.


Digital money

Why it matters
As the use of physical cash declines, so does the freedom to transact without an intermediary. Meanwhile, digital currency technology could be used to splinter the global financial system.
Key players
People’s Bank of China, Facebook
Availability
This year
Last June Facebook unveiled a “global digital currency” called Libra. The idea triggered a backlash and Libra may never launch, at least not in the way it was originally envisioned. But it’s still made a difference: just days after Facebook’s announcement, an official from the People’s Bank of China implied that it would speed the development of its own digital currency in response. Now China is poised to become the first major economy to issue a digital version of its money, which it intends as a replacement for physical cash.

China’s leaders apparently see Libra, meant to be backed by a reserve that will be mostly US dollars, as a threat: it could reinforce America’s disproportionate power over the global financial system, which stems from the dollar’s role as the world’s de facto reserve currency. Some suspect China intends to promote its digital renminbi internationally.

Now Facebook’s Libra pitch has become geopolitical. In October, CEO Mark Zuckerberg promised Congress that Libra “will extend America’s financial leadership as well as our democratic values and oversight around the world.” The digital money wars have begun.

by Mike Orcutt",1-10%,Negative,Software,Concept
Anti-aging Drugs,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Anti-aging drugs

YOSHI SODEOKA
Anti-aging drugs

Why it matters
A number of different diseases, including cancer, heart disease, and dementia, could potentially be treated by slowing aging.
Key players
Unity Biotechnology, Alkahest, Mayo Clinic, Oisín Biotechnologies, Siwa Therapeutics
Availability
Less than 5 years
Drugs that try to treat ailments by targeting a natural aging process in the body have shown promise.

The first wave of a new class of anti-aging drugs have begun human testing. These drugs won’t let you live longer (yet) but aim to treat specific ailments by slowing or reversing a fundamental process of aging.

The drugs are called senolytics—they work by removing certain cells that accumulate as we age. Known as “senescent” cells, they can create low-level inflammation that suppresses normal mechanisms of cellular repair and creates a toxic environment for neighboring cells.

In June, San Francisco–based Unity Biotechnology reported initial results in patients with mild to severe osteoarthritis of the knee. Results from a larger clinical trial are expected in the second half of 2020. The company is also developing similar drugs to treat age-related diseases of the eyes and lungs, among other conditions.

Senolytics are now in human tests, along with a number of other promising approaches  targeting the biological processes that lie at the root of aging and various diseases.

A company called Alkahest injects patients with components found in young people’s blood and says it hopes to halt cognitive and functional decline in patients suffering from mild to moderate Alzheimer’s disease. The company also has drugs for Parkinson’s and dementia in human testing. 

And in December, researchers at Drexel University College of Medicine even tried to see if a cream including the immune-suppressing drug rapamycin could slow aging   in human skin.

The tests reflect researchers’ expanding efforts to learn if the many diseases associated with getting older—such as heart diseases, arthritis, cancer, and dementia—can be hacked to delay their onset.

by Adam Piore",0.01% - 1%,positive,Biotech,Product
AI-discovered Molecules,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"AI-discovered molecules
Scientists have used AI to discover promising drug-like compounds.


AI-discovered molecules

Why it matters
Commercializing a new drug costs around $2.5 billion on average. One reason is the difficulty of finding promising molecules.
Key players
Insilico Medicine, Kebotix, Atomwise, University of Toronto, BenevolentAI, Vector Institute
Availability
3-5 years
The universe of molecules that could be turned into potentially life-saving drugs is mind-boggling in size: researchers estimate the number at around 1060. That’s more than all the atoms in the solar system, offering virtually unlimited chemical possibilities—if only chemists could find the worthwhile ones.

Now machine-learning tools can explore large databases of existing molecules and their properties, using the information to generate new possibilities. This could make it faster and cheaper to discover new drug candidates.

In September, a team of researchers at Hong Kong–based Insilico Medicine and the University of Toronto took a convincing step toward showing that the strategy works by synthesizing several drug candidates found by AI algorithms.

Using techniques like deep learning and generative models similar to the ones that allowed a computer to beat the world champion at the ancient game of Go, the researchers identified some 30,000 novel molecules with desirable properties. They selected six to synthesize and test. One was particularly active and proved promising in animal tests.

Chemists in drug discovery often dream up new molecules—an art honed by years of experience and, among the best drug hunters, by a keen intuition. Now these scientists have a new tool to expand their imaginations.

by David Rotman",0.1-1%,Positive,Biotech,Concept
Satellite Mega-constellations,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"AI-discovered molecules
Scientists have used AI to discover promising drug-like compounds.


AI-discovered molecules

Why it matters
Commercializing a new drug costs around $2.5 billion on average. One reason is the difficulty of finding promising molecules.
Key players
Insilico Medicine, Kebotix, Atomwise, University of Toronto, BenevolentAI, Vector Institute
Availability
3-5 years
The universe of molecules that could be turned into potentially life-saving drugs is mind-boggling in size: researchers estimate the number at around 1060. That’s more than all the atoms in the solar system, offering virtually unlimited chemical possibilities—if only chemists could find the worthwhile ones.

Now machine-learning tools can explore large databases of existing molecules and their properties, using the information to generate new possibilities. This could make it faster and cheaper to discover new drug candidates.

In September, a team of researchers at Hong Kong–based Insilico Medicine and the University of Toronto took a convincing step toward showing that the strategy works by synthesizing several drug candidates found by AI algorithms.

Using techniques like deep learning and generative models similar to the ones that allowed a computer to beat the world champion at the ancient game of Go, the researchers identified some 30,000 novel molecules with desirable properties. They selected six to synthesize and test. One was particularly active and proved promising in animal tests.

Chemists in drug discovery often dream up new molecules—an art honed by years of experience and, among the best drug hunters, by a keen intuition. Now these scientists have a new tool to expand their imaginations.

by David Rotman
Share story on linkedin
Share story on twitter
Share story on facebook
Share story on email
Satellite mega-constellations

JULIA DUFOSSÉ
Satellite mega-constellations

Why it matters
These systems can blanket the globe with high-speed internet—or turn Earth’s orbit into a junk-ridden minefield.
Key players
SpaceX, OneWeb, Amazon, Telesat
Availability
Now
We can now affordably build, launch, and operate tens of thousands of satellites in orbit at once.

Satellites that can beam a broadband connection to internet terminals. As long as these terminals have a clear view of the sky, they can deliver internet to any nearby devices. SpaceX alone wants to send more than 4.5 times more satellites into orbit this decade than humans have ever launched since Sputnik.

These mega-constellations are feasible because we have learned how to build smaller satellites and launch them more cheaply. During the space shuttle era, launching a satellite into space cost roughly $24,800 per pound. A small communications satellite that weighed four tons cost nearly $200 million to fly up.

Today a SpaceX Starlink satellite weighs about 500 pounds (227 kilograms). Reusable architecture and cheaper manufacturing mean we can strap dozens of them onto rockets to greatly lower the cost; a SpaceX Falcon 9 launch today costs about $1,240 per pound.

The first 120 Starlink satellites went up last year, and the company planned to launch batches of 60 every two weeks starting in January 2020. OneWeb will launch over 30 satellites later this year. We could soon see thousands of satellites working in tandem to supply internet access for even the poorest and most remote populations on the planet.

But that’s only if things work out. Some researchers are livid because they fear these objects will disrupt astronomy research. Worse is the prospect of a collision that could cascade into a catastrophe of millions of pieces of space debris, making satellite services and future space exploration next to impossible. Starlink’s near-miss with an ESA weather satellite in September was a jolting reminder that the world is woefully unprepared to manage this much orbital traffic. What happens with these mega-constellations this decade will define the future of orbital space.

by Neel V. Patel",0.01% - 0.1%,Negative,Hardware,platform
Quantum Supremacy,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Quantum supremacy

YOSHI SODEOKA
Quantum supremacy

Why it matters
Eventually, quantum computers will be able to solve problems no classical machine can manage.
Key players
Google, IBM, Microsoft, Rigetti, D-Wave, IonQ, Zapata Computing, Quantum Circuits
Availability
5-10+ years
Google has provided the first clear proof of a quantum computer outperforming a classical one.

Quantum computers store and process data in a way completely differently from the ones we’re all used to. In theory, they could tackle certain classes of problems that even the most powerful classical supercomputer imaginable would take millennia to solve, like breaking today’s cryptographic codes or simulating the precise behavior of molecules to help discover new drugs and materials.

There have been working quantum computers for several years, but it’s only under certain conditions that they outperform classical ones, and in October Google claimed the first such demonstration of “quantum supremacy.” A computer with 53 qubits—the basic unit of quantum computation—did a calculation in a little over three minutes that, by Google’s reckoning, would have taken the world’s biggest supercomputer 10,000 years, or 1.5 billion times as long. IBM challenged Google’s claim, saying the speedup would be a thousandfold at best; even so, it was a milestone, and each additional qubit will make the computer twice as fast.

However, Google’s demo was strictly a proof of concept—the equivalent of doing random sums on a calculator and showing that the answers are right. The goal now is to build machines with enough qubits to solve useful problems. This is a formidable challenge: the more qubits you have, the harder it is to maintain their delicate quantum state. Google’s engineers believe the approach they’re using can get them to somewhere between 100 and 1,000 qubits, which may be enough to do something useful—but nobody is quite sure what.

And beyond that? Machines that can crack today’s cryptography will require millions of qubits; it will probably take decades to get there. But one that can model molecules should be easier to build.

by Gideon Lichfield",0.01% - 0.1%,Positive,Hardware,Concept
Tiny AI,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Tiny AI

JULIA DUFOSSÉ
Tiny AI

Why it matters
Our devices no longer need to talk to the cloud for us to benefit from the latest AI-driven features.
Key players
Google, IBM, Apple, Amazon
Availability
Now
We can now run powerful AI algorithms on our phones.

AI has a problem: in the quest to build more powerful algorithms, researchers are using ever greater amounts of data and computing power, and relying on centralized cloud services. This not only generates alarming amounts of carbon emissions but also limits the speed and privacy of AI applications.

But a countertrend of tiny AI is changing that. Tech giants and academic researchers are working on new algorithms to shrink existing deep-learning models without losing their capabilities. Meanwhile, an emerging generation of specialized AI chips promises to pack more computational power into tighter physical spaces, and train and run AI on far less energy.

These advances are just starting to become available to consumers. Last May, Google announced that it can now run Google Assistant on users’ phones without sending requests to a remote server. As of iOS 13, Apple runs Siri’s speech recognition capabilities and its QuickType keyboard locally on the iPhone. IBM and Amazon now also offer developer platforms for making and deploying tiny AI.

All this could bring about many benefits. Existing services like voice assistants, autocorrect, and digital cameras will get better and faster without having to ping the cloud every time they need access to a deep-learning model. Tiny AI will also make new applications possible, like mobile-based medical-image analysis or self-driving cars with faster reaction times. Finally, localized AI is better for privacy, since your data no longer needs to leave your device to improve a service or a feature.

But as the benefits of AI become distributed, so will all its challenges. It could become harder to combat surveillance systems or deepfake videos, for example, and discriminatory algorithms could also proliferate. Researchers, engineers, and policymakers need to work together now to develop technical and policy checks on these potential harms.

by Karen Hao",1.5% - 15%,Positive,Software,Concept
Differential Privacy,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Differential privacy
A technique to measure the privacy of a crucial data set.


Differential privacy

Why it matters
It is increasingly difficult for the US Census Bureau to keep the data it collects private. A technique called differential privacy could solve that problem, build trust, and also become a model for other countries.
Key players
US Census Bureau, Apple, Facebook
Availability
Its use in the 2020 US Census will be the biggest-scale application yet.
In 2020, the US government has a big task: collect data on the country’s 330 million residents while keeping their identities private. The data is released in statistical tables that policymakers and academics analyze when writing legislation or conducting research. By law, the Census Bureau must make sure that it can’t lead back to any individuals. 

But there are tricks to “de-anonymize” individuals, especially if the census data is combined with other public statistics.

So the Census Bureau injects inaccuracies, or “noise,” into the data. It might make some people younger and others older, or label some white people as black and vice versa, while keeping the totals of each age or ethnic group the same. The more noise you inject, the harder de-anonymization becomes.

Differential privacy is a mathematical technique that makes this process rigorous by measuring how much privacy increases when noise is added. The method is already used by Apple and Facebook to collect aggregate data without identifying particular users.

But too much noise can render the data useless. One analysis showed that a differentially private version of the 2010 Census included households that supposedly had 90 people.

If all goes well, the method will likely be used by other federal agencies. Countries like Canada and the UK are watching too.

by Angela Chen",0.01% - 0.1%,Positive,Software,methodology
Climate Change Attribution,2020.0,https://www.technologyreview.com/10-breakthrough-technologies/2020/,,"Climate change attribution

YOSHI SODEOKA
Climate change attribution

Why it matters
It’s providing a clearer sense of how climate change is worsening the weather, and what we’ll need to do to prepare.
Key players
World Weather Attribution, Royal Netherlands Meteorological Institute, Red Cross Red Crescent Climate Centre, University of Oxford
Availability
Now
Researchers can now spot climate change’s role in extreme weather.

Ten days after Tropical Storm Imelda began flooding neighborhoods across the Houston area last September, a rapid-response research team announced that climate change almost certainly played a role.

The group, World Weather Attribution, had compared high-resolution computer simulations of worlds where climate change did and didn’t occur. In the former, the world we live in, the severe storm was as much as 2.6 times more likely—and up to 28% more intense.

Earlier this decade, scientists were reluctant to link any specific event to climate change. But many more extreme-weather attribution studies have been done in the last few years, and rapidly improving tools and techniques have made them more reliable and convincing.

This has been made possible by a combination of advances. For one, the lengthening record of detailed satellite data is helping us understand natural systems. Also, increased computing power means scientists can create higher-resolution simulations and conduct many more virtual experiments.

These and other improvements have allowed scientists to state with increasing statistical certainty that yes, global warming is often fueling more dangerous weather events. 

By disentangling the role of climate change from other factors, the studies are telling us what kinds of risks we need to prepare for, including how much flooding to expect and how severe heat waves will get as global warming becomes worse. If we choose to listen, they can help us understand how to rebuild our cities and infrastructure for a climate-changed world.

by James Temple",-1% to -10%,Positive,Other,methodology
Messenger RNA Vaccines,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Messenger RNA vaccines

We got very lucky. The two most effective vaccines against the coronavirus are based on messenger RNA, a technology that has been in the works for 20 years. When the covid-19 pandemic began last January, scientists at several biotech companies were quick to turn to mRNA as a way to create potential vaccines; in late December 2020, at a time when more than 1.5 million had died from covid-19 worldwide, the vaccines were approved in the US, marking the beginning of the end of the pandemic.

The new covid vaccines are based on a technology never before used in therapeutics, and it could transform medicine, leading to vaccines against various infectious diseases, including malaria. And if this coronavirus keeps mutating, mRNA vaccines can be easily and quickly modified. Messenger RNA also holds great promise as the basis for cheap gene fixes to sickle-cell disease and HIV. Also in the works: using mRNA to help the body fight off cancers. Antonio Regalado explains the history and medical potential of the exciting new science of messenger RNA.",2-20%,Positive,Biotech,Product
GPT-3,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"GPT-3

Large natural-language computer models that learn to write and speak are a big step toward AI that can better understand and interact with the world. GPT-3 is by far the largest—and most literate—to date. Trained on the text of thousands of books and most of the internet, GPT-3 can mimic human-written text with uncanny—and at times bizarre—realism, making it the most impressive language model yet produced using machine learning.

conceptual photograph of unicorns
SIERRA LENNY
But GPT-3 doesn’t understand what it’s writing, so sometimes the results are garbled and nonsensical. It takes an enormous amount of computation power, data, and money to train, creating a large carbon footprint and restricting the development of similar models to those labs with extraordinary resources. And since it is trained on text from the internet, which is filled with misinformation and prejudice, it often produces similarly biased passages. Will Douglas Heaven shows off a sample of GPT-3’s clever writing and explains why some are ambivalent about its achievements.",0.01% to 0.1%,Negative,Software,product
Data Trusts,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Data trusts

Technology companies have proven to be poor stewards of our personal data. Our information has been leaked, hacked, and sold and resold more times than most of us can count. Maybe the problem isn’t with us, but with the model of privacy to which we’ve long adhered—one in which we, as individuals, are primarily responsible for managing and protecting our own privacy.

Data trusts offer one alternative approach that some governments are starting to explore. A data trust is a legal entity that collects and manages people’s personal data on their behalf. Though the structure and function of these trusts are still being defined, and many questions remain, data trusts are notable for offering a potential solution to long-standing problems in privacy and security. Anouk Ruhaak describes the powerful potential of this model and a few early examples that show its promise.",0.05-0.5%,Positive,Software,concept
Lithium-metal Batteries,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Lithium-metal batteries

Electric vehicles come with a tough sales pitch; they’re relatively expensive, and you can drive them only a few hundred miles before they need to recharge—which takes far longer than stopping for gas. All these drawbacks have to do with the limitations of lithium-ion batteries. A well-funded Silicon Valley startup now says it has a battery that will make electric vehicles far more palatable for the mass consumer.

It’s called a lithium-metal battery and is being developed by QuantumScape. According to early test results, the battery could boost the range of an EV by 80% and can be rapidly recharged. The startup has a deal with VW, which says it will be selling EVs with the new type of battery by 2025.

The battery is still just a prototype that’s much smaller than one needed for a car. But if QuantumScape and others working on lithium-metal batteries succeed, it could finally make EVs attractive to millions of consumers. James Temple describes how a lithium-metal battery works, and why scientists are so excited by recent results.",0.05% - 0.5%,Positive,Hardware,product
Digital Contact Tracing,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Digital contact tracing

As the coronavirus began to spread around the world, it felt at first as if digital contact tracing might help us. Smartphone apps could use GPS or Bluetooth to create a log of people who had recently crossed paths. If one of them later tested positive for covid, that person could enter the result into the app, and it would alert others who might have been exposed.

But digital contact tracing largely failed to make much impact on the virus’s spread. Apple and Google quickly pushed out features like exposure notifications to many smartphones, but public health officials struggled to persuade residents to use them. The lessons we learn from this pandemic could not only help us prepare for the next pandemic but also carry over to other areas of health care. Lindsay Muscato explores why digital contact tracing failed to slow covid-19 and offers ways we can do better next time.",-0.05% to -0.5%,Negative,Software,methodology
Hyper-accurate Positioning,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Hyper-accurate positioning

We all use GPS every day; it has transformed our lives and many of our businesses. But while today’s GPS is accurate to within 5 to 10 meters, new hyper-accurate positioning technologies have accuracies within a few centimeters or millimeters. That’s opening up new possibilities, from landslide warnings to delivery robots and self-driving cars that can safely navigate streets.

China’s BeiDou (Big Dipper) global navigation system was completed in June 2020 and is part of what’s making all this possible. It provides positioning accuracy of 1.5 to two meters to anyone in the world. Using ground-based augmentation, it can get down to millimeter-level accuracy. Meanwhile, GPS, which has been around since the early 1990s, is getting an upgrade: four new satellites for GPS III launched in November and more are expected in orbit by 2023. Ling Xin reports on how the greatly increased accuracy of these systems is already proving useful.",0.01% - 0.1%,Positive,Hardware,platform
Remote Everything,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Remote everything

The covid pandemic forced the world to go remote. Getting that shift right has been especially critical in health care and education. Some places around the world have done a particularly good job at getting remote services in these two areas to work well for people.

Snapask, an online tutoring company, has more than 3.5 million users in nine Asian countries, and Byju’s, a learning app based in India, has seen the number of its users soar to nearly 70 million. Unfortunately, students in many other countries are still floundering with their online classes.

Meanwhile, telehealth efforts in Uganda and several other African countries have extended health care to millions during the pandemic. In a part of the world with a chronic lack of doctors, remote health care has been a life saver. Sandy Ong reports on the remarkable success of online learning in Asia and the spread of telemedicine in Africa.",5-50%,Positive,Software,concept
Multi-skilled AI,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Multi-skilled AI

Despite the immense progress in artificial intelligence in recent years, AI and robots are still dumb in many ways, especially when it comes to solving new problems or navigating unfamiliar environments. They lack the human ability, found even in young children, to learn how the world works and apply that general knowledge to new situations.

One promising approach to improving the skills of AI is to expand its senses; currently AI with computer vision or audio recognition can sense things but cannot “talk” about what it sees and hears using natural-language algorithms. But what if you combined these abilities in a single AI system? Might these systems begin to gain human-like intelligence? Might a robot that can see, feel, hear, and communicate be a more productive human assistant? Karen Hao explains how AIs with multiple senses will gain a greater understanding of the world around them, achieving a much more flexible intelligence.",2-20%,Positive,Software,concept
TikTok Recommendation Algorithms,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"TikTok recommendation algorithms

Since its launch in China in 2016, TikTok has become one of the world’s fastest-growing social networks. It’s been downloaded billions of times and attracted hundreds of millions of users. Why? Because the algorithms that power TikTok’s “For You” feed have changed the way people become famous online.

While other platforms are geared more toward highlighting content with mass appeal, TikTok’s algorithms seem just as likely to pluck a new creator out of obscurity as they are to feature a known star. And they’re particularly adept at feeding relevant content to niche communities of users who share a particular interest or identity.

The ability of new creators to get a lot of views very quickly—and the ease with which users can discover so many kinds of content—have contributed to the app’s stunning growth. Other social media companies are now scrambling to reproduce these features on their own apps. Abby Ohlheiser profiles a TikTok creator who was surprised by her own success on the platform.   ",0.05-0.5%,Positive,Software,product
Green Hydrogen,2021.0,https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/,,"Green hydrogen

Related Story

Voestalpine&#039;s H2FUTURE plant.
How falling solar costs have renewed clean hydrogen hopes
As nations do the hard math on how to meet their climate goals, green hydrogen increasingly appears essential.
Hydrogen has always been an intriguing possible replacement for fossil fuels. It burns cleanly, emitting no carbon dioxide; it’s energy dense, so it’s a good way to store power from on-and-off renewable sources; and you can make liquid synthetic fuels that are drop-in replacements for gasoline or diesel. But most hydrogen up to now has been made from natural gas; the process is dirty and energy intensive.

The rapidly dropping cost of solar and wind power means green hydrogen is now cheap enough to be practical. Simply zap water with electricity, and presto, you’ve got hydrogen. Europe is leading the way, beginning to build the needed infrastructure. Peter Fairley argues that such projects are just a first step to an envisioned global network of electrolysis plants that run on solar and wind power, churning out clean hydrogen.",0.01% - 0.1%,Positive,Climate Tech,Concept
End of Passwords (New Authentication Forms),2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044953/password-login-cybersecurity/,"The end of passwords
Companies are finally shifting away from notoriously insecure alphanumerics to other methods of authentication.
By Mat Honanarchive page
February 23, 2022
The end of passwords conceptual image of phone, computer, and hardware key.
ANDREA D'AQUINO
KEY PLAYERS
Microsoft, Google, Okta, Duo
 
AVAILABILITY
Now
In the early 1960s, MIT professor Fernando Corbató was developing a new kind of shared computer system and wanted a way for people to be able to protect their private files. His solution was a password. Over the years, Corbató’s fix won out over other means of authentication and became the standard way we log on to pretty much everything, everywhere. 

The catch? Passwords are inherently insecure. They can be stolen, guessed, or brute-forced. But mostly, people just use bad ones. (And, worse, reuse them.) 

Password managers like Dashlane and 1Password can track all those various alphanumerics for you and even replace the weak ones. But password management is a half measure when it comes to security. The real action is in eliminating passwords altogether. 

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
The process is already underway. Enterprise-oriented companies like Okta and Duo, as well as personal identity providers like Google, offer ways for people to log in to apps and services without having to enter a password. Apple’s facial recognition system has taken biometric login mainstream. Most notably, Microsoft announced in March 2021 that some of its customers could go completely passwordless, and it followed up in September by telling people to delete their passwords altogether. Those other methods of authentication? They’re finally winning.


You’ve probably experienced this for yourself. You go to log in to a site or fire up an app, and instead of being asked to enter a password you get a prompt to enter a six-digit code from your authenticator app, tap a notification on your phone, or click a link sent to your email. Or maybe you just need to raise your phone to your face. Easy peasy. 

As for typing in a bunch of characters and symbols you have to recall, write down, or store in a database? Good riddance to all that.

Curious about how hackers do what they do? Check out The Extortion Economy, our five-part podcast series about the ransomware crisis.",0.05-0.5%,Positive,Software,Concept
COVID Variant Tracking,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044975/covid-19-variant-tracking-scientists/,"Covid variant tracking
The SARS-CoV-2 virus is the most sequenced organism on Earth, allowing scientists to quickly spot new variants as they spread.
By Antonio Regaladoarchive page
February 23, 2022
Concept illustration of covid variant tracking
ANDREA D'AQUINO
KEY PLAYERS
GISAID, Nextstrain, Illumina
 
AVAILABILITY
Now
Among all the nose swabs that have ever come up positive for covid-19 over the course of the pandemic, about two in a hundred have been sent for extra analysis in a gene sequencing machine. The objective: to create a fresh map of the SARS-CoV-2 virus’s genome—all 30,000 or so letters—and see what’s changed.  

Such genetic surveillance has allowed scientists to quickly spot and warn about new variants like alpha, delta, and mostly recently omicron. It’s an unprecedented effort that’s turned SARS-CoV-2 into history’s most sequenced organism—blasting past the flu, HIV, or even our own human genome. Open databases now display more than 7 million maps of the germ’s genes.

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Omicron is the most dramatic evolution yet. A lab in South Africa first sent up a warning flare in November after its sequencers spotted a virus genome with more than 50 mutations. Almost instantly, computers in Seattle, Boston, and London were using the data to spit out predictions: omicron was trouble, a variant likely to evade antibodies. 

One thing the sequencers can’t yet tell us is how, exactly, SARS-CoV-2 will evolve next. That’s why some say we should be tracking the germ even more closely. The majority of sequences are being generated in places like the UK, the US, and Denmark. But the virus can still evolve, unwitnessed, in regions without the capacity to sequence. Luckily, the quick work in South Africa to spot omicron and track its spread gave the world an early warning.

As part of our 10 Breakthrough Technologies series, meet the scientist who warned the world about omicron.

CORRECTION: GISAID is an organization that promotes rapid sharing of flu and coronavirus genetic data and Nextstrain is a project developing software for analyzing such data. An earlier version of this article incorrectly referred to both as databases.",-1% to -3%,Positive,Biotech,methodology
Long-Lasting Grid Battery,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044962/grid-battery-iron-clean-energy/,"Long-lasting grid battery
Cheap, long-lasting iron-based batteries could help even out renewable energy supplies and expand the use of clean power.
By Casey Crownhartarchive page
February 23, 2022
Concept illustration of a long-lasting grid battery
ANDREA D'AQUINO
KEY PLAYERS
ESS, Form Energy
 
AVAILABILITY
Now
For a few seconds on a sunny afternoon last April, renewables broke a record for California’s main electric grid, providing enough power to supply 94.5% of demand. The moment was hailed as a milestone on the path to decarbonization. But what happens when the sun sets and the breeze stops?

Handling the fluctuating power production of renewables will require cheap storage for hours or even days at a time. New types of iron-based batteries might be up to the task.

Oregon-based ESS, whose batteries can store energy for between four and 12 hours, launched its first grid-scale projects in 2021. Massachusetts-based Form Energy, which raised $240 million in 2021, has batteries that store power for up to 100 hours. Its first installation will be a one-megawatt pilot plant in Minnesota, slated to be completed in 2023. 

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Both companies rely on batteries that use iron, one of the most abundant materials on the planet. This means that their offerings could eventually be cheaper than other grid storage candidates, like lithium-ion and vanadium flow batteries. Form says its batteries could ultimately cost just $20 per kilowatt-hour, lower than even optimistic projections for lithium-ion batteries in the next several decades. 

There are still challenges to address: iron batteries typically have low efficiency, meaning a good fraction of the energy that’s put into them can’t be recovered. Unwanted side reactions can also degrade them over time. But if iron-based batteries can be deployed widely, at a low enough cost, they could help power more of the world with renewable energy.

As part of our 10 Breakthrough Technologies series, learn about ESS's ambitious plans to install iron batteries for grid storage around the world.",0.1% - 1%,Positive,Hardware,product
AI for Protein Folding,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044957/ai-protein-folding-deepmind/,"AI for protein folding
DeepMind has opened new paths for drug discovery and design by solving a 50-year-old problem in biology.
By Will Douglas Heavenarchive page
February 23, 2022
Concept illustration of using AI for predicting protein folding
ANDREA D'AQUINO, PROTEIN MODEL COURTESY OF ALPHAFOLD
KEY PLAYERS
DeepMind, Isomorphic Labs, Baker Lab
 
AVAILABILITY
Now
By the end of 2020, DeepMind, the UK-based artificial-intelligence lab, had already produced many impressive achievements in AI. Still, when the group’s program for predicting protein folding was released in November of that year, biologists were shocked by how well it worked. 

Nearly everything your body does, it does with proteins. Understanding what individual proteins do is therefore crucial for most drug development and for understanding many diseases. And what a protein does is determined by its three-dimensional shape.

A protein is made up of a ribbon of amino acids, which folds up into a knot of complex twists and twirls. Determining that shape—and thus the protein’s function—can take months in a lab. For years, scientists have tried computerized prediction methods to make the process easier. But no technique ever came close to matching the accuracy achieved by humans. 

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
That changed with DeepMind’s AlphaFold2. The software, which uses an AI technique called deep learning, can predict the shape of proteins to the nearest atom, the first time a computer has matched the slow but accurate techniques used in the lab. 


Scientific teams around the world have started using it for research on cancer, antibiotic resistance, and covid-19. DeepMind has also set up a public database that it’s filling with protein structures as AlphaFold2 predicts them. It currently has around 800,000 entries, and DeepMind says it will add more than 100 million—nearly every protein known to science—in the next year.  

DeepMind has spun off this work into a company called Isomorphic Labs, which it says will collaborate with existing biotech and pharma companies. The true impact of AlphaFold2 may take a year or two to be clear, but its potential is rapidly unfolding in labs around the world.

As part of our 10 Breakthrough Technologies series, explore how DeepMind shifted from playing games to solving one of the hardest problems in science.",0.01% - 0.1%,Positive.,Biotech,Concept
Malaria Vaccine,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044969/malaria-vaccine/,"Malaria vaccine
Malaria kills hundreds of thousands of children a year. Combined with other measures, the vaccine could reduce deaths by as much as 70%.
By Adam Piorearchive page
February 23, 2022
Concept illustration of the malaria vaccine
ANDREA D'AQUINO
KEY PLAYERS
GlaxoSmithKline, World Health Organization
 
AVAILABILITY
Now (limited)
The malaria parasite, a notoriously deadly foe, has evolved countless ways to evade immune detection and thrive in human hosts. Mainly concentrated in sub-Saharan Africa, which accounts for roughly 95% of cases, malaria kills more than 600,000 people a year, a majority of them children younger than five years old. 

Last October, after years of development, the World Health Organization finally approved the world’s first vaccine to combat the deadly mosquito-borne disease. 

GlaxoSmithKline’s vaccine, known as RTS,S or Mosquirix, is not a particularly effective one. It requires three doses in children between five months and 17 months old, and a fourth dose given 12 to 15 months after that. Given to more than 800,000 children in Kenya, Malawi, and Ghana, the vaccine had an efficacy of about 50% against severe malaria in the first year, and its effectiveness dropped dramatically over time. 

Even so, public health officials are hailing the vaccine, which has been in testing since 1987, as a “game changer” in Africa. When combined with other malaria control measures, which include insecticide-treated bed nets and preventative drugs administered during the rainy season, it is expected to reduce malaria deaths by as much 70%, compared with the death rate in children given existing drugs.


NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Mosquirix also has a broader significance: it is the first vaccine approved for a parasitic disease. Parasites are complex multicellular organisms, with genomes 500 to 1,000 times larger than those found in most viruses and bacteria. This complexity allows them to mutate in myriad ways when challenged by the immune response. Glaxo’s vaccine consists of copies of a single protein that dots the surface of the parasite early in its life, paired with a series of molecules designed to set off alarm bells in the immune system and catalyze the production of antibodies that will protect potential hosts against the real thing. 

Public health officials say the approval is likely to encourage innovation. Second-generation malaria vaccines—as well as vaccines for other parasitic diseases—are already in the pipeline.

As part of our 10 Breakthrough Technologies series, examine how the malaria vaccine came about and where we go from here.",-0.001% to -0.01%,Positive,Biotech,product
Proof of Stake for Cryptocurrencies,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044960/proof-of-stake-cryptocurrency/,"Proof of stake
This alternative method for securing digital currency could end crypto’s energy consumption woes.
By Siobhan Robertsarchive page
February 23, 2022
Concept illustration of proof of stake
ANDREA D'AQUINO
KEY PLAYERS
Cardano, Solana, Algorand, Ethereum
 
AVAILABILITY
2022 for Ethereum
Cryptocurrencies such as Bitcoin use huge amounts of electricity. In 2021, the Bitcoin network consumed upwards of 100 terawatt-hours, more than the typical annual energy budget of Finland. 

Proof of stake offers a way to set up such a network without requiring so much energy. And if all goes as planned, Ethereum, which runs all sorts of applications in addition to the world’s second-largest cryptocurrency, will transition to it in the first half of 2022. The shift has been projected to cut energy use by 99.95%.

Cryptocurrencies run on blockchains—digital ledgers of transactions that must be secure from cheats, fraudsters, and hacks. Bitcoin and Ethereum currently ensure that security using proof-of-work algorithms: “miners” solve cryptographic puzzles, competing for the right to verify a new block of transactions. Successful miners are rewarded for their work with cryptocurrency. 

Finding solutions to proof-of-work puzzles requires massive amounts of computing power and, thus, electricity. 


NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
With proof of stake, validators don’t have to vie against one another, spending big on energy and computing hardware. Instead, their cache, or stake, of cryptocurrency allows them to enter a lottery. Those who are chosen gain the authority to verify a set of transactions (and so earn more cryptocurrency). In some networks, validators who display bad behavior are penalized—they lose some portion of their stake. 

Ethereum will be the largest network to use proof of stake. It has built a new blockchain for the system, which has been running in parallel. All that needs to happen now is “The Merge,” transferring over the layer that actually executes transactions and holds users’ assets (and dropping proof of work in the process).

If successful, Ethereum’s proof-of-stake blockchain could set the stage for wider adoption of the energy-saving technology. Other networks have contemplated a switch, but they seem to be taking a wait-and-see approach.

As part of our 10 Breakthrough Technologies series, learn how Ethereum will switch to proof of stake—and what could go wrong.",0.0001% - 0.001%,Positive,Software,Methodology
COVID Pill,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044936/covid-19-pill-pandemic/,"A pill for covid
Easy to take pills that prevent severe covid-19 could work against the next pandemic, too.
By Antonio Regaladoarchive page
February 23, 2022
Concept illustration of the covid pill
ANDREA D'AQUINO
KEY PLAYERS
Merck, Pfizer, Pardes Biosciences
 
AVAILABILITY
Now
Remember hydroxychloroquine, a malaria treatment taken by Donald Trump, and ivermectin, the horse dewormer that sent people to poison control? These drugs weren’t effective against covid-19. But people desperately wanted them to be. The dream was a pill you could swallow to make the virus go away. 

Now the real thing is here: pills designed from the ground up to block the covid-19 virus. And they work. Given to people within a few days of infection, an antiviral from Pfizer slashes the chance of hospitalization by 89%. The US government has already placed orders for $10 billion worth of the new drug, called Paxlovid.

The new pill was no lucky stab in the dark. Chemists designed it to mess with the virus’s ability to copy itself. The medicine locks onto and blocks a protein, called a protease, that’s at the core of covid’s menacing replication machinery. 

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Similar protease enzymes exist in other types of coronavirus. That means Pfizer’s drug could also be a ready defense against the next pandemic. And scientists are sure that more pathogens like SARS-CoV-2 lurk in bat caves and industrial animal farms.


The new antiviral drugs—there’s also one from Merck that targets a different mechanism in the virus’s replication—took longer than covid vaccines to design, synthesize, and test. But they still set records. Never before has an entirely new molecule to defeat a disease gone so quickly from a chemist’s bench into the mouths of volunteers and gained approval from the US Food and Drug Administration. The CEO of Pfizer, Albert Bourla, said he was “in tears” when he got the news in November that the drug worked.

The pill will prevent many people from dying of covid-19, including people with weak immune systems for whom vaccines don’t work. And if a new variant turns up that defeats vaccines, antivirals could be our last resort.

As part of our 10 Breakthrough Technologies series, discover how covid pills were developed and what to expect next.",1-10%,Positive,Biotech,Product
Practical Fusion Reactors,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044948/practical-fusion-reactors-carbon-free/,"Practical fusion reactors
Fusion promises to generate cheap, carbon-free, always-on energy, with no meltdowns and little radioactive waste.
By James Templearchive page
February 23, 2022
Concept illustration of practical fusion reactors
ANDREA D'AQUINO
KEY PLAYERS
Commonwealth Fusion Systems, ITER, Lawrence Livermore’s National Ignition Facility, Helion Energy, Tokamak Energy, General Fusion
 
AVAILABILITY
About 10 years
Last September, researchers at Commonwealth Fusion Systems slowly charged a 10-ton D-shaped magnet, pushing up the field strength until it surpassed 20 tesla—a record for a magnet of its kind. The company’s founders say the feat addressed the major engineering challenge required to develop a compact, inexpensive fusion reactor.

Fusion power has been a dream of physicists for decades. At temperatures well above 100 million degrees, as in the sun, atomic nuclei mash together, releasing a massive amount of energy in the process. If researchers can bring about these reactions in a controlled and sustained way here on Earth, it could provide a crucial source of cheap, always-on, carbon-free electricity, using nearly limitless fuel sources. 

In one approach, magnets are used to confine a gas of ions and electrons, known as a plasma, within doughnut-­shaped reactors. More powerful magnets mean less heat escapes and more fusion reactions can occur within a smaller, cheaper facility. And not by just a little: doubling the strength of the magnetic field reduces the volume of the plasma needed to generate the same amount of power by a factor of 16.

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
Despite decades of research and billions of dollars’ investment in the past, nobody has yet built a fusion plant that can produce more energy than it consumes. But Commonwealth and its backers are hopeful, and other fusion startups and research efforts have reported recent progress as well. 

Commonwealth is building a factory to mass-produce the magnets and laying the groundwork for a prototype reactor. If all goes as hoped, the startup plans to deliver fusion energy to the electric grid by the early 2030s.

As part of our 10 Breakthrough Technologies series, follow Commonwealth Fusion Systems’ attempts to make nuclear fusion a practical power source.",0% - 1%,Positive,Hardware,product
Synthetic Data for AI,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044965/ai-synthetic-data-2/,"Synthetic data for AI
The benefits of AI are concentrated in areas where data is available.
Synthetic data promises to fill the gaps.
By Will Douglas Heavenarchive page
February 23, 2022
Concept illustration of synthetic data
ANDREA D'AQUINO
KEY PLAYERS
Synthetic Data Vault, Syntegra, Datagen, Synthesis AI
 
AVAILABILITY
Now
Last year, researchers at Data Science Nigeria noted that engineers looking to train computer-vision algorithms could choose from a wealth of data sets featuring Western clothing, but there were none for African clothing. The team addressed the imbalance by using AI to generate artificial images of African fashion—a whole new data set from scratch. 

Such synthetic data sets—computer-generated samples with the same statistical characteristics as the genuine article—are growing more and more common in the data-hungry world of machine learning. These fakes can be used to train AIs in areas where real data is scarce or too sensitive to use, as in the case of medical records or personal financial data. 

The idea of synthetic data isn’t new: driverless cars have been trained on virtual streets. But in the last year the technology has become widespread, with a raft of startups and universities offering such services. Datagen and Synthesis AI, for example, supply digital human faces on demand. Others provide synthetic data for finance and insurance. And the Synthetic Data Vault, a project launched in 2021 by MIT’s Data to AI Lab, provides open-source tools for creating a wide range of data types.

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
This boom in synthetic data sets is driven by generative adversarial networks (GANs), a type of AI that is adept at generating realistic but fake examples, whether of images or medical records.

Proponents claim that synthetic data avoids the bias that is rife in many data sets. But it will only be as unbiased as the real data used to generate it. A GAN trained on fewer Black faces than white, for example, may be able to create a synthetic data set with a higher proportion of Black faces, but those faces may end up being less lifelike given the limited original data.

Join us March 29-30 at EmTech Digital, our signature AI conference, to hear Unity’s Danny Lange talk about how the video game maker is using synthetic data.",0.05% - 0.5%,Positive.,Software,methodology
Carbon Removal Factory in Iceland,2022.0,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,https://www.technologyreview.com/2022/02/23/1044972/carbon-removal-factory-climate-change/,"Carbon removal factory

A large plant that captures carbon from the air could help create an industry the world needs to avoid dangerous levels of warming this century.
orca plant in iceland
KRISTJÁN MAACK
by James Temple archive pageCasey Crownhart archive page
February 23, 2022
KEY PLAYERS
Climeworks, Carbon Engineering, Carbon Collect
 
AVAILABILITY
Now
In September, Climeworks flipped the switch on Orca, the largest plant to date that is designed to remove carbon dioxide from the air. 

The facility, outside Reykjavik, Iceland, can capture 4,000 metric tons of carbon dioxide every year. Large fans suck air through a filter, where materials bind with CO2 molecules. The company’s partner, Carbfix, then mixes the carbon dioxide with water and pumps it underground, where it reacts with basalt rock and eventually turns into stone. The facility runs entirely on carbon-free electricity, mainly from a nearby geothermal power plant. 

Advertisement

workers at the orca plant
Each module at Orca is made up of a dozen carbon removal units. Air passes through grates and over a filter that traps carbon dioxide with adsorbent chemicals. When the filters are full, grates close across the front of the unit and pipes pump heat into the enclosed space, releasing CO2 from the filters.The carbon dioxide is then pumped to an area where it's prepared for storage before the gates open again to restart the process.
KRISTJáN MAACK
To be sure, 4,000 tons isn’t that much. It’s less than the annual emissions of 900 cars. And it’s a tiny fraction of the billions of tons of carbon dioxide the world will likely need to pull out of the atmosphere to prevent global warming from soaring past 2 °C over preindustrial levels, according to a variety of studies.

orca module
 Orca is made of modules that can be combined in different ways, making the plant easier to replicate around the world. Climeworks put its first plant in Iceland in part to take advantage of the country’s abundant geothermal power, so the facility can minimize the emissions it generates in the process of capturing carbon dioxide.
KRISTJáN MAACK
material covering fans at orca plant
On the back of each module, fans blow filtered air back into the atmosphere. Flexible covers on the fans ensure that loose bits from the filters aren’t blown away by the strong Icelandic winds. These covers will likely be needed just for the first year, while the units are tested.
KRISTJáN MAACK
Carbfix employee holds basalt rock
After being captured, carbon dioxide is transformed into solid carbonate minerals, seen here as light spots in a matrix of dark basalt rock.
KRISTJáN MAACK
Far larger facilities are in the works as well. Carbon Engineering, based in Squamish, British Columbia, plans to start construction this year on a plant in the US Southwest with the capacity to remove 1 million tons of carbon dioxide a year. With various partners, it’s also begun engineering or design work on plants in Scotland and Norway that would capture 500,000 to 1 million tons per year.

NEVER MISS A BREAKTHROUGH
Sign up to receive the latest emerging tech stories in your inbox, every weekday.
Enter your email

Sign up
Get updates and offers from MIT Technology Review
By signing up, you agree to our Privacy Policy.
pipes across landscape in Iceland
Carbon dioxide is pressurized and mixed with water before being transferred through massive pipes to Carbfix, the company that pumps it underground to be stored.
KRISTJáN MAACK

Injection wells operated by Carbfix pump carbon dioxide 1,000 meters underground, where it reacts with basalt rock and is locked into mineral form within two years.
KRISTJáN MAACK
The hope is that building more and larger plants to capture carbon from the air will help companies figure out how to optimize operations, drive down the costs, and realize economies of scale. Climeworks estimates it will reduce costs between $600 and $800 per ton of carbon, to around $100 to $150, by the late 2030s. 

A growing number of individuals and companies, including Microsoft, Stripe, and Square, are already paying today’s high costs to suck carbon out of the air as they strive to cancel out their emissions. That’s providing crucial early revenue.",-0.0001% to -0.001%,Positive,Climate Tech,product
CRISPR for High Cholesterol,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064857/crispr-high-cholesterol-10-breakthrough-technologies-2023/,"CRISPR for high cholesterol: 10 Breakthrough Technologies 2023
New forms of the gene-editing tool could enable treatments for common diseases.
By Jessica Hamzelouarchive page
January 9, 2023
&quot;&quot;
AMRITA MARINO
WHO
Verve Therapeutics, Beam Therapeutics, Prime Medicine, Broad Institute

WHEN
10 to 15 years

Advertisement

Last year, a New Zealand woman became the first to receive a gene-editing treatment to permanently lower her cholesterol. The woman had heart disease, along with an inherited risk for high cholesterol. But scientists behind the experimental treatment think it could help pretty much anyone.  

The trial is a potential turning point for CRISPR, the editing tool they used. Since the technology was first programmed to edit genomes about a decade ago, we’ve seen CRISPR move from scientific labs to clinics. But the first experimental treatments have focused on rare genetic disorders. A high-cholesterol treatment has wider potential. 


The cholesterol-lowering treatment, developed by Verve Therapeutics, relies on a form of gene editing called base editing, or “CRISPR 2.0.” It’s a more targeted approach—instead of simply making cuts to shut off specific genes, scientists can now swap a single DNA base for another. In theory, this should be safer because you’re less likely to cut an important gene by mistake, and you can avoid potential errors that may occur when DNA repairs itself after being cut. 

An even newer form of CRISPR could take things further still. Prime editing—or “CRISPR 3.0”—allows scientists to insert chunks of DNA into a genome. If it works in people, it could let scientists replace disease-causing genes.  

Together, these newer forms of CRISPR could broaden the scope of gene editing to take on many conditions—not all of them genetic. Someday, people may have the option to add genes thought to protect against high blood pressure, or certain diseases, to their genetic code.   

All CRISPR treatments are experimental at this point, and we don’t know if they’re safe. Some argue we should focus on treating those with severe diseases in the meantime. But if these new forms of CRISPR do work, they could help many others.

Read more about how CRISPR might be ready to go mainstream.",0.01% - 0.1%,Positive,Biotech,methodology
AI that Makes Images,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064864/image-making-ai-10-breakthrough-technologies-2023/,"AI that makes images: 10 Breakthrough Technologies 2023

AI models that generate stunning imagery from simple phrases are evolving into powerful creative and commercial tools.
By Will Douglas Heavenarchive page
January 9, 2023
&quot;&quot;
ERIK CARTER VIA DALL-E 2
WHO
OpenAI, Stability AI, Midjourney, Google

WHEN
Now

Advertisement

OpenAI introduced a world of weird and wonderful mash-ups when its text-to-image model DALL-E was released in 2021. Type in a short description of pretty much anything, and the program spat out a picture of what you asked for in seconds. DALL-E 2, unveiled in April 2022, was a massive leap forward. Google also launched its own image-making AI, called Imagen. 

Yet the biggest game-changer was Stable Diffusion, an open-source text-to-image model released for free by UK-based startup Stability AI in August. Not only could Stable Diffusion produce some of the most stunning images yet, but it was designed to run on a (good) home computer.


By making text-to-image models accessible to all, Stability AI poured fuel on what was already an inferno of creativity and innovation. Millions of people have created tens of millions of images in just a few months. But there are problems, too. Artists are caught in the middle of one of the biggest upheavals in a decade. And, just like language models, text-to-image generators can amplify the biased and toxic associations buried in training data scraped from the internet.

The tech is now being built into commercial software, such as Photoshop. Visual-effects artists and video-game studios are exploring how it can fast-track development pipelines. And text-to-image technology has already advanced to text-to-video. The AI-generated video clips demoed by Google, Meta, and others in the last few months are only seconds long, but that will change. One day movies could be made just by feeding a script into a computer.

Nothing else in AI grabbed people’s attention more last year—for the best and worst reasons. Now we wait to see what lasting impact these tools will have on creative industries—and the entire field of AI.

No one knows where the rise of generative AI will leave us. Read more here.",0.05% - 0.5%,Negative,Software,Methodology
A Chip Design that Changes Everything (RISC-V),2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064876/riscv-computer-chips-10-breakthough-technologies-2023/,"A chip design that changes everything: 10 Breakthrough Technologies 2023
Computer chip designs are expensive and hard to license. That’s all about to change thanks to the popular open standard known as RISC-V.
By Sophia Chenarchive page
January 9, 2023
&quot;&quot;
NICK LITTLE
WHO
RISC-V International, Intel, SiFive, SemiFive, China RISC-V Industry Alliance

WHEN
Now

Ever wonder how your smartphone connects to your Bluetooth speaker, given they were made by different companies? Well, Bluetooth is an open standard, meaning its design specifications, such as the required frequency and its data encoding protocols, are publicly available. Software and hardware based on open standards—Ethernet, Wi-Fi, PDF—have become household names. 

Now an open standard known as RISC-V (pronounced “risk five”) could change how companies create computer chips. 


Chip companies such as Intel and Arm have long kept their blueprints proprietary. Customers would buy off-the-shelf chips, which may have had capabilities irrelevant to their product, or pay more for a custom design. Since RISC-V is an open standard, anyone can use it to design a chip, free of charge. 

RISC-V specifies design norms for a computer chip’s instruction set. The instruction set describes the basic operations that a chip can do to change the values its transistors represent—for example, how to add two numbers. RISC-V’s simplest design has just 47 instructions. But RISC-V also offers other design norms for companies seeking chips with more complex capabilities. 

About 3,100 members worldwide, including companies and academic institutions, are now collaborating via the nonprofit RISC-V International to establish and develop these norms. In February 2022, Intel announced a $1 billion fund that will, in part, support companies building RISC-V chips.

RISC-V chips have already begun to pop up in earbuds, hard drives, and AI processors, with 10 billion cores already shipped. Companies are also working on RISC-V designs for data centers and spacecraft. In a few years, RISC-V proponents predict, the chips will be everywhere. 

Read about how RISC-V is rewriting the economics of chip design and shaking up the tech sector’s power dynamics.",2-20%,Positive,Hardware,platform
Mass-market Military Drones,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064892/mass-market-military-drones-tb2-10-breakthrough-technologies-2023/,"Mass-market military drones: 10 Breakthrough Technologies 2023
Turkish-made aircraft like the TB2 have dramatically expanded the role of drones in warfare.
By Kelsey D. Athertonarchive page
January 9, 2023
&quot;&quot;
DAVID ALABO
WHO
Baykar Technologies, Shahed Aviation Industries

WHEN
Now

For decades, high-end precision-strike American aircraft, such as the Predator and Reaper, dominated drone warfare. The war in Ukraine, however, has been defined by low-budget models made in China, Iran, or Turkey. Their widespread use has changed how drone combat is waged and who can wage it. 

Some of these new drones are off-the-shelf quadcopters, like those from DJI, used for both reconnaissance and close-range attacks. Others, such as the $30,000 Iranian-made exploding Shahed drones, which Russia has used to attack civilians in Kyiv, are capable of longer-range missions. But the most notable is the $5 million Bayraktar TB2, made by Turkey’s Baykar corporation.


The TB2 is a collection of good-enough parts put together in a slow-flying body. It travels at speeds up to 138 miles per hour and has a communication range of around 186 miles. Baykar says it can stay aloft for 27 hours. But when combined with cameras that can share video with ground stations, the TB2 becomes a powerful tool for both targeting the laser-guided bombs carried on its wings and helping direct artillery barrages from the ground.

Most important is simply its availability. US-made drones like the Reaper are more capable but costlier and subject to stiff export controls. The TB2 is there for any country that wants it. 

Turkey’s military used the drones against Kurds in 2016. Since then, they’ve been used in Libya, Syria, and Ethiopia, and by Azerbaijan during its war against Armenia. Ukraine bought six in 2019 for military operations in the Donbas, but the drones caught the world’s attention in early 2022, when they helped thwart Russian invaders. 

The tactical advantages are clear. What’s also sadly clear is that these weapons will take an increasingly horrible toll on civilian populations around the world.

Read more about how mass-market military drones have changed the way wars are fought.",0.1% - 1.0%,Negative,Hardware,product
Abortion Pills via Telemedicine,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064871/abortion-pills-telemedicine-10-breakthrough-technologies-2023/,"Abortion pills via telemedicine: 10 Breakthrough Technologies 2023
Medication abortion has become increasingly common, but the US Supreme Court’s decision to overturn Roe v. Wade brought a new sense of urgency.
By Rebecca Ackermannarchive page
January 9, 2023
&quot;&quot;
AMRITA MARINO
WHO
Choix, Hey Jane, Aid Access, Just the Pill, Abortion on Demand, Planned Parenthood, Plan C

WHEN
Now

Access to abortion care has narrowed dramatically in the US. But there’s been one big shift in the other direction: the ability to access care without leaving home. In 2021, during the pandemic, the US Food and Drug Administration temporarily allowed health-care providers to mail patients two pills—mifepristone and misoprostol—that, when taken together, can induce an abortion. Years before, the FDA had found the pills to be safe and effective at ending a pregnancy in the first trimester, and by 2020 they accounted for more than half of all abortions in the US. Then, at the end of 2021, the FDA made its decision permanent. 

Six months later, the US Supreme Court ruled that abortion is not a constitutional right. As state “trigger laws” prohibiting abortion took effect, interest in and demand for abortion pills surged. Nonprofits like Aid Access and startups like Choix, Just the Pill, and Hey Jane were ready to help. Though the process varies by service, eligible patients generally sign up with a photo ID and then consult with a medical provider via video call, text, or app. The provider prescribes the pills, which the service ships to the patient. 


Access to medication abortion is not a solved problem. Aid Access, with headquarters in Europe, is unique in that it will ship pills to any US state. But most startups offering abortion pills by mail follow state laws, which means people living in the 13 states that ban abortion, or in the additional seven states that require a doctor to prescribe the medications in person, must travel across state lines or set up an alternative mailing address to use these services.

Still, the organizations helping people obtain abortion pills remotely have brought care to many at a critical time. Their foresight and tireless efforts meant these solutions were ready when people needed them.

Update: In January 2023, the FDA said it would permit online and brick-and-mortar retail pharmacies to dispense abortion pills to patients who have a prescription, in a step that could further expand access to these medications in states that allow it.",0.01% - 0.1%,Positive.,Biotech,product
Organs on Demand,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064867/engineered-organs-10-breakthrough-technologies-2023/,"Organs on demand: 10 Breakthrough Technologies 2023

Engineered organs could put an end to transplant waiting lists.
By Antonio Regaladoarchive page
January 9, 2023
doctor holding a heart in a specimen jar
JOE CAROTTA FOR NYU LANGONE HEALTH
WHO
eGenesis, Makana Therapeutics, United Therapeutics

WHEN
10 to 15 years

For two months last year, a 57-year-old man named David Bennett lived with a pig heart beating inside his chest. Surgeons at the University of Maryland had put it there to see: Could a gene-edited pig’s heart keep a person alive?

Far more people need an organ transplant to live than can get one. There are around 130,000 organ transplants each year around the world, but many more people die waiting for an organ or because they never even made it onto a transplant waiting list. 


Animal organs are one potential solution. But it’s not easy to overcome the human body’s natural revolt against them. For example, sugars on the surface of pig tissue can send our immune system into attack mode. Drugs can help mute the response, but it’s not enough. So biotech companies have used gene editing to modify pigs, removing those sugar molecules and adding other genes to make the pigs seem more human-like. 

By editing the DNA of pigs in this way, several biotech companies have now created animals whose organs are more compatible with human bodies. Though Bennett died, and a virus was found in the transplanted organ, his doctors claim the pig heart he received never developed classic signs of organ rejection. Now they’re planning studies with more patients.

In the future, organ engineering might not involve animals at all. Researchers are in the early stages of exploring how to engineer complex tissue from the ground up. Some are 3D-printing scaffolds in the shape of lungs. Others are cultivating blob-like “organoids” from stem cells to imitate specific organs. In the long term, researchers hope to grow custom organs in factories.

Whether they’re grown in animals or built inside manufacturing plants, an unlimited supply of organs could make transplantation more common, and give far more people access to replacement parts.

Read about Martine Rothblatt, the entrepreneur dreaming about factories of unlimited organs.",0.5% - 5.0%,Positive,Biotech,concept
The Inevitable EV,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064889/the-inevitable-ev-10-breakthrough-technologies-2023/,"The inevitable EV: 10 Breakthrough Technologies 2023
Electric vehicles have been available for decades. Now they’ve finally become mainstream.
By James Templearchive page
January 9, 2023
&quot;&quot;
NICK LITTLE
WHO
BYD, Hyundai, Tesla, Volkswagen

WHEN
Now

Advertisement

Electric vehicles are transforming the auto industry.

While sales have slowly ticked up for years, they’re now soaring. The emissions-free cars and trucks will likely account for 13% of all new auto sales globally in 2022, up from 4% just two years earlier, according to the International Energy Agency. They’re on track to make up about 30% of those sales by the end of this decade.


A mix of forces has propelled the vehicles from a niche choice to a mainstream option. 

Governments have enacted policies compelling automakers to retool and incentivizing consumers to make the switch. Notably, California and New York will require all new cars, trucks, and SUVs to be zero-emissions by 2035, and the EU had nearly finalized a similar rule at press time. 

Auto companies, in turn, are setting up supply chains, building manufacturing capacity, and releasing more models with better performance, across price points and product types. 

The Hongguang Mini, a tiny car that starts a little below $5,000, has become the best-selling electric vehicle in the world, reinforcing China’s dominance as the largest manufacturer of EVs.

A growing line-up of two- and three-wheelers from Hero Electric, Ather, and other companies helped EV sales triple in India over the last year (though the total number is still only around 430,000). And models ranging in size and price from the Chevy Bolt to the Ford F-150 Lightning are bringing more Americans into the electric fold.

There are still big challenges ahead. Most of the vehicles must become cheaper. Charging options need to be more convenient. Clean electricity generation will have to increase dramatically to accommodate the surge in vehicle charging. And it will be a massive undertaking to make enough batteries. But it’s now clear that the heyday of the gas-guzzler is dimming.",2.5% - 25%,Positive,Climate Tech,product
James Webb Space Telescope,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064854/james-webb-space-telescope-10-breakthrough-technologies-2023/,"James Webb Space Telescope: 10 Breakthrough Technologies 2023
A marvel of precision engineering, JWST could revolutionize our view of the early universe.
By Jonathan O'Callaghanarchive page
January 9, 2023
Carina Nebula image from JWST
NASA/ESA/STSCI
WHO
NASA, European Space Agency, Canadian Space Agency, Space Telescope Science Institute

WHEN
Now

Advertisement

Launched in December 2021 after decades of work, NASA’s $10 billion James Webb Space Telescope—a collaboration between the US, Europe, and Canada—is the largest telescope ever sent to space and some 100 times more powerful than its predecessor, the Hubble Space Telescope. It is also specifically designed to detect infrared radiation, allowing it to cut through dust and look far back in time to a period when the universe’s first stars and galaxies formed. 

JWST is tailor-made for this kind of astronomical time travel. Its main mirror is 21 feet across, three times the diameter of Hubble’s, giving it far greater resolving power. It carries a sunshield as big as a tennis court to protect its mirror and instruments from the heat and light of the sun. To help it get to space, engineers designed JWST’s mirror and sunshield to fold up so they would fit inside a rocket fairing, unfolding after launch as the telescope made its journey to its final orbit 1.5 million kilometers from Earth. 


Astronomers hope that with JWST, they will be able to piece together how the universe’s first galaxies came to be following the Big Bang. But that is not JWST’s only goal. The telescope is being used across the breadth of astronomy. It could provide unprecedented insight into planets in other solar systems, allowing us to work out what their atmospheres are made of. It will witness the birth of new worlds, take magnificent images of nebulae, probe the structure of galaxies, and much, much more.

New discoveries rain down almost every day and will do so for the lifetime of the telescope, estimated at more than 20 years.

Read more about how the James Webb Space Telescope broke the universe.",0.01% - 0.1%,Positive,Hardware,project
Ancient DNA Analysis,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064883/ancient-dna-analysis-10-breakthrough-technologies-2023/,"Ancient DNA analysis: 10 Breakthrough Technologies 2023
New methods that make damaged DNA legible are producing stunning revelations about the deep past.
By Hana Kirosarchive page
January 9, 2023
&quot;&quot;
DAVID ALABO
WHO
Max Planck Institute for Evolutionary Anthropology, David Reich Lab at Harvard

WHEN
Now

Advertisement

Scientists have long sought better tools to study teeth and bones from ancient humans. In the past, they’ve had to scour many ancient remains to find a sample preserved well enough to analyze.  

Now cheaper techniques and new methods that make damaged DNA legible to commercial sequencers are powering a boom in ancient DNA analysis. 


Today, scientists can even analyze microscopic traces of DNA found in dirt Neanderthals urinated in—no teeth or bones required. In November, the field now known as paleogenetics took center stage when Svante Pääbo, a geneticist at the Max Planck Institute for Evolutionary Anthropology, won a Nobel Prize for his foundational work.

Ancient DNA analysis has led to the discovery of two extinct species of human—Homo luzonensis and Denisovans—and taught us that modern humans carry a substantial amount of Denisovan and Neanderthal DNA. And the number of ancient human individuals for whom we now have whole-­genome data has jumped drastically, from just five in 2010 to 5,550 in 2020. 

By indicating that India’s population came from a mix of ancestors, these techniques have undermined the caste system. DNA from a 2,500-year-old battlefield in Sicily has revealed that ancient Greek armies were more diverse than historians depicted. 

Old samples can unravel modern health mysteries, too. Last year scientists identified a single mutation that made people 40% likelier to survive the Black Death—and it’s also a risk factor for autoimmune issues like Crohn’s disease. 

Differences in how cultures believe human remains should be treated will keep creating ethical and logistical questions for scholars seeking to work with ancient DNA. But its revelations are already rewriting history.",0.01% - 0.1%,Positive,Biotech,methodology
Battery Recycling,2023.0,https://www.technologyreview.com/2023/01/09/1066394/10-breakthrough-technologies-2023/,https://www.technologyreview.com/2023/01/09/1064886/battery-recycling-10-breakthrough-technologies-2023/,"Battery recycling: 10 Breakthrough Technologies 2023

New ways to recover the crucial metals in batteries could make EVs more affordable.
By Casey Crownhartarchive page
January 9, 2023
grid of the ends of AA batteries
REDWOOD MATERIALS
WHO
CATL, Umicore, Redwood Materials, Li-Cycle, Cirba

WHEN
Now

Advertisement

High-value metals recovered from old laptops, corroded power drills, and electric vehicles could power tomorrow’s cars, thanks to recycling advances that make it possible to turn old batteries into new ones. 

Demand for lithium-ion batteries is skyrocketing as electric vehicles become more common. Greater use of electric vehicles is good news for the climate. But supplies of the metals needed to build battery cells are already stretched thin, and demand for lithium could increase 20 times by 2050. 


Recycling may help. Older methods of processing spent batteries struggled to reliably recover enough of these individual metals to make recycling economical. But new approaches have swiftly changed that, enabling recyclers to more effectively dissolve the metals and separate them from battery waste. 

Recycling facilities can now recover nearly all of the cobalt and nickel and over 80% of the lithium from used batteries and manufacturing scrap left over from battery production—and recyclers plan to resell those metals for a price nearly competitive with that of mined materials. Aluminum, copper, and graphite are often recovered as well. 

China leads the world in battery recycling today, dominated by subsidiaries of major battery companies like CATL. The EU recently proposed extensive recycling regulations with mandates for battery manufacturers. And companies in North America, like Redwood Materials and Li-Cycle, are quickly scaling operations, funded by billions of dollars in public and private investment. 

Battery demand is expected to grow exponentially for decades. Recycling alone won’t be enough to satisfy it. And these new recycling processes aren’t perfect. But battery recycling factories will create a supply of materials the world needs to meet its climate goals.

Read more about how old batteries will help power tomorrow's EVs.",0.05-0.5%,Positive,Hardware,methodology
AI for Everything,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085096/artificial-intelligence-generative-ai-chatgpt-open-ai-breakthrough-technologies/,"AI for everything: 10 Breakthrough Technologies 2024
Generative AI tools like ChatGPT reached mass adoption in record time, and reset the course of an entire industry.
By Will Douglas Heavenarchive page
January 8, 2024
a 1950s style woman sitting at the breakfast table with a cell phone beaming red and a box of cereal that claims with similar red rays to now contain AI.
JENNIFER DIONISIO
WHO
Google, Meta, Microsoft, OpenAI

WHEN
Now

Advertisement

When OpenAI launched a free web app called ChatGPT in November 2022, nobody knew what was coming. But that low-key release changed everything.

By January, ChatGPT had become the fastest-growing web app ever, offering anyone with a browser access to one of the most powerful neural networks ever built. We were dazzled and disturbed.  


And that was only the start. In February, Microsoft and Google revealed rival plans to combine chatbots with search—plans that reimagined our daily interactions with the internet.  

Early demos weren’t great. Microsoft’s Bing Chat went off the rails, quick to churn out nonsense. Google’s Bard was caught making a factual error in its promo pitch. But the genie wasn’t going back in its bottle, no matter how weird it was. 

Related Story

a &quot;Gen AI&quot; scratch ticket with &quot;HOPE&quot; &quot;HYPE&quot; and &quot;NOPE&quot; revealed
These six questions will dictate the future of generative AI
Generative AI took the world by storm in 2023. Its future—and ours—will be shaped by what we do next.
Microsoft and Google have since moved beyond search to put chatbot-based assistants into the hands of billions of people via their office software. The tech promises to summarize emails and meetings; draft reports and replies; generate whole slide decks—titles, bullet points, and pictures—in seconds.

Microsoft and Meta released image-making models that let users generate shareable images of anything with a click. Cue a nonstop stream of zany mash-ups—and dozens of posts about Mickey Mouse and SpongeBob SquarePants flying a plane into the Twin Towers.

Google’s new phones now use AI to let you edit photos to a degree never seen before, exchanging sad faces for happy ones and overcast afternoons for perfect sunsets.

Never has such radical new technology gone from experimental prototype to consumer product so fast and at such scale. What’s clear is that we haven’t even begun to make sense of it all, let alone reckon with its impact.

Is the shine coming off? Maybe. With each release, the astonishing becomes more mundane. But 2023’s legacy is clear: billions have now looked AI in the face. Now we need to figure out exactly what’s looking back.",1-10%,Negative,Software,concept
Super-efficient Solar Cells,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085124/super-efficient-solar-cells-breakthrough-technologies/,"Super-efficient solar cells: 10 Breakthrough Technologies 2024
Solar cells that combine traditional silicon with cutting-edge perovskites could push the efficiency of solar panels to new heights.
By Emma Foehringer Merchantarchive page
January 8, 2024
Neighborhood scene with a home powered by solar panels has resident blow drying their hair, while the other resident makes eggs on the grill that resemble the solar units. Their dog looks happy in a solar-powered doghouse with a powered coffee cup next to its paw. A cyclist is passing by on a solar-powered bike.
SIMON LANDREIN
WHO
Beyond Silicon, Caelux, First Solar, Hanwha Q Cells, Oxford PV, Swift Solar, Tandem PV

WHEN
3 to 5 years

Advertisement

In November 2023, a buzzy solar technology broke yet another world record for efficiency. The previous record had existed for only about five months—and it likely won’t be long before it too is obsolete. This astonishing acceleration in efficiency gains comes from a special breed of next-­generation solar technology: perovskite tandem solar cells. These cells layer the traditional silicon with materials that share a unique crystal structure.

In the decade that scientists have been toying with perovskite solar technology, it has continued to best its own efficiency records, which measure how much of the sunlight that hits the cell is converted into electricity. Perovskites absorb different wavelengths of light from those absorbed by silicon cells, which account for 95% of the solar market today. When silicon and perovskites work together in tandem solar cells, they can utilize more of the solar spectrum, producing more electricity per cell. 


Technical efficiency levels for silicon-­based cells top out below 30%, while perovskite-only cells have reached experimental efficiencies of around 26%. But perovskite tandem cells have already exceeded 33% efficiency in the lab. That is the technology’s tantalizing promise: if deployed on a significant scale, perovskite tandem cells could produce more electricity than the legacy solar cells at a lower cost. 

Related Story

a green road winds into a distant wall of mountains. A car has skidded off and is smoking and a vulture picks at some green fragments.
Climate tech is back—and this time, it can’t afford to fail
A decade after the high profile bust of cleantech 1.0, venture-backed firms are again flourishing. We need them to succeed. Will they?
But perovskites have stumbled when it comes to actual deployment. Silicon solar cells can last for decades. Few perovskite tandem panels have even been tested outside. 

The electrochemical makeup of perovskites means they’re sensitive to sucking up water and degrading in heat, though researchers have been working to create better barriers around panels and shifting to more stable perovskite compounds. 

In May, UK-based Oxford PV said it had reached an efficiency of 28.6% for a commercial-size perovskite tandem cell, which is significantly larger than those used to test the materials in the lab, and it plans to deliver its first panels and ramp up manufacturing in 2024. Other companies could unveil products later this decade.",0.05% - 0.5%,Positive,Hardware,product
Apple Vision Pro,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085133/apple-vision-pro-micro-oled-mixed-reality-spatial-computer-virtual-reality-breakthrough-technologies/,"Apple Vision Pro: 10 Breakthrough Technologies 2024
Micro-OLED technology has been in development for more than a decade, but the Vision Pro will be the highest-profile demonstration of its abilities to date.
By Amy Nordrumarchive page
January 8, 2024

SIMOUL ALVA
WHO
Apple

WHEN
Later this year

Advertisement

History is littered with doomed face computers. Google Glass, Microsoft HoloLens, and even Meta’s Quest line all failed to catch fire. Now, it’s Apple’s turn to try. 

Later this year, Apple plans to start shipping its new Vision Pro, the company’s first mixed-reality headset. Its commercial success is very much an open question, but the Vision Pro is no doubt a breakthrough device, with a display radically better than any that has come before. 


Apple revealed the headset (which it calls a spatial computer) in June at its annual developers’ event, pitching it as a better way to watch films, experience photos, connect with others, and even read and create. 

Unlike virtual reality, mixed reality overlays digital content onto your real-world surroundings. Cameras and sensors embedded in the headset take what’s going on around you in a room and re-create it on a pair of screens—one for each eye. Then the headset adds in whatever digital content you’ve selected—a surfing video, for example, virtually projected right in front of your actual couch. 

Related Story

A frontal view of the Apple Vision Pro headset
These minuscule pixels are poised to take augmented reality by storm
LEDs and their organic counterparts are getting truly tiny. This could be the moment AR and VR companies have been waiting for.
The Vision Pro’s twin micro-OLED displays provide much higher resolution and sharper contrast than the liquid crystal displays found in most VR headsets—which means in addition to delivering mixed reality, the headset can also provide the most fully immersive experience we’ve ever seen. 

One big remaining question: What will people use it for? Apple has a suite of apps at the ready, and developers can create their own, but will it mostly be a fancy video viewer? Another: Will people actually wear it? Apple added virtual eyes to the front to show where the wearer is looking, but at the June event it only shared videos of people wearing the headsets. How good or weird will those virtual eyes look in person? And at $3,499, the Vision Pro will cost thousands of dollars more than other headsets. 

That said, no other company has been as successful at developing and marketing consumer technology over the years. Apple’s investment may give mixed reality its best chance yet to catch on.",0.01% - 0.1%,Positive,Hardware,product
Weight-loss Drugs,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085116/weight-loss-drugs-ozempic-wegovy-mounjaro-eli-lilly-novo-nordisk-breakthrough-technologies/,"Weight-loss drugs: 10 Breakthrough Technologies 2024
Weight-loss drugs like Wegovy and Mounjaro are wildly popular and effective, but their long-term health impacts are still unknown.
By Abdullahi Tsanniarchive page
January 8, 2024
a manicured hand reaches for a bottle of pills that is casting the shadow of a slimmer person
JENNIFER DIONISIO
WHO
Eli Lilly, Novartis, Novo Nordisk, Pfizer, Viking Therapeutics 

WHEN
Now

One-third of US adults have obesity, a condition that makes them more susceptible to heart disease, diabetes, and cancer. Anti-obesity drugs—including Wegovy and Mounjaro—could help address this public health crisis.

Success stories are everywhere online, from Reddit to TikTok. Novo Nordisk, the company behind two of the popular medications, has seen profits soar, and pharmacies have struggled to keep the drugs in stock.  


These medications help people lose weight by suppressing their appetite. Most were originally developed to treat type 2 diabetes, but in June 2021, Wegovy became the first drug to be approved for weight management since 2014. Semaglutide, the active ingredient in both Wegovy and Ozempic (a diabetes drug that’s often prescribed off-label for weight loss), mimics a hormone that the intestine releases after eating, causing you to feel full. Patients inject the drugs once a week at home and can lose about 12% to 15% of their body weight (though many hit a plateau after that). 

Related Story

a comic movie poster style image of a very large rodent and lots of smaller ones, with teaser text that reads, &quot;Inescapable! Incomparable! Impenetrable!&quot; across the top. Along the bottom is the title, &quot;Journey to the lair of the Hunger Demon by Adam Piore, based on research by Bradford Lowell and Mark Andermann.&quot;
We’ve never understood how hunger works. That might be about to change.
Scientists have spent decades trying to unravel the intricate mysteries of the human appetite. Are they on the verge of finally determining how this basic drive functions?
These drugs aren’t perfect—common side effects include nausea, diarrhea, and vomiting. Many patients must stay on the drugs for life to keep the weight off, and the long-term impacts of these treatments remain unknown. The drugs are also expensive, costing more than $1,000 per month, and most insurance plans don’t cover them for weight loss. 

Nevertheless, the treatments could improve the health of millions of people. Some studies even suggest that they alleviate symptoms of heart failure. Dozens of companies are now developing new versions of these weight-loss medications, some of which can be taken orally. 

In November, the US Food and Drug Administration approved Eli Lilly’s diabetes drug Zepbound for obesity. With about 70 new obesity treatments in development, six are now awaiting regulatory review.

In the coming year, expect to see more companies entering the final stages of trials and seeking approval as demand skyrockets.",-0.05% to -0.5%,Positive,Biotech,product
Enhanced Geothermal Systems,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085112/enhanced-geothermal-systems-renewable-energy-drilling-breakthrough-technologies/,"Enhanced geothermal systems: 10 Breakthrough Technologies 2024
Advanced drilling technology could unlock the potential of this carbon-free renewable energy source.
By June Kimarchive page
January 8, 2024
two workers use a giant drill to go through strata which is on a bedrock of battery that is connected back to power the drill
SIMON LANDREIN
WHO
AltaRock Energy, Fervo Energy, Utah FORGE lab

WHEN
3 to 5 years

Advertisement

Geothermal heat, an abundant and carbon-­free energy source, offers an alternative to fossil fuels that doesn’t vary with the weather or time of day. However, conventional geothermal plants require specific geological conditions—in particular, permeable rocks with water sources.

Because of this, geothermal accounts for less than 1% of global renewables capacity. But an emerging technology could let us exploit even more of the heat beneath our feet. 


Enhanced geothermal systems have been in development since the 1970s. Recent advances show that they could dramatically increase production of renewable energy. Fervo Energy tested one such system last year in Nevada and proved its commercial viability. The company is building another project in Utah, with a goal of providing constant, clean power by 2026.

With enhanced geothermal, companies can access geothermal heat in new locations. Hydraulic fracturing techniques—widely used by the oil and gas industry—are now being used to crack open relatively solid rocks, at depths much greater than existing geothermal wells. Water is then injected into these rocks to generate steam, which subsequently drives turbines to produce electricity. 

Related Story

Fervo&#039;s Drilling Rig lit up at night
2023 Climate Tech Companies to Watch: Fervo Energy and its geothermal power plants
Fervo Energy is using fracking techniques to harness the limitless power beneath our feet.
Fervo also looks to use enhanced geothermal techniques to create what are essentially giant underground batteries for the grid. By building up or relieving pressure in the wells, it can save up energy when demand is low and boost generation when it rises.

The technology is not without potential risks. In particular, the scientific community is divided on how hydraulic fracturing could affect seismic activity. While some believe earthquake risks are minimal, a 2017 incident in South Korea was linked to an enhanced geothermal project. 

Several other companies and labs are now advancing pilot projects and research in the field. Washington-based AltaRock Energy is developing specialized techniques to access extremely hot rocks, which could dramatically increase energy output. Utah FORGE, sponsored by the US Department of Energy, is drilling a well that can act as a test bed for enhanced geothermal technologies. Many of these projects are still in the experimental phase, but it’s increasingly clear that enhanced geothermal is a hot topic in the energy world.",0.001% - 0.01%,Positive,Hardware,methodology
Chiplets,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085120/chiplets-moores-law-avanced-micro-devices-intel-chips-breakthrough-technologies/,"Chiplets: 10 Breakthrough Technologies 2024
Chipmakers are betting that smaller, more specialized chips can extend the life of Moore’s Law.
By Mike Orcuttarchive page
January 8, 2024

SIMOUL ALVA
WHO
Advanced Micro Devices, Intel, Universal Chiplet Interconnect Express

WHEN
Now

Advertisement

Packaging. It may sound boring, but it’s an essential part of building computer systems. Now companies are defining what that looks like for a new generation of machines.  

For decades, chipmakers have improved performance by making transistors smaller and cramming more of them onto chips. The popular name for the trend is Moore’s Law. But that era is ending. It’s gotten immensely expensive to further shrink transistors and manufacture the complex chips that today’s high-tech industries demand. 


In response, manufacturers are turning to smaller, more modular “chiplets” that are designed for specific functions (such as storing data or processing signals) and can be linked together to build a system. The smaller a chip, the fewer defects it’s likely to contain, making manufacturing less expensive.

Related Story

grid of chips
We need a moonshot for computing
The US government aims to push microelectronics research forward. But maintaining competitiveness in the long term will require embracing uncertainty.
Companies including Advanced Micro Devices and Intel have been marketing systems based on chiplets for years. But whether chiplets can help the industry maintain performance gains at the pace of Moore’s Law will depend on packaging, which entails placing them side by side or stacking them, forming fast, high-bandwidth electrical connections between them, and encasing them in protective plastic. 

Manufacturers are still working out the best way to balance cost with performance. The $52.7 billion CHIPS Act, the 2022 US legislation aimed at shoring up the nation’s chip industry, directs $11 billion toward “advanced semiconductor” research and creates a National Advanced Packaging Manufacturing Program to foster collaboration between academia and industry. 

So far, chiplet adoption has been hindered by the lack of technical standards for packaging. That’s changing: the industry has embraced an open-source standard called Universal Chiplet Interconnect Express. In theory, standards will make it easy to combine chiplets made by different companies, which could give chipmakers more freedom in fast-moving fields like AI, aerospace, and automaking.",0.01% - 0.1%,Positive,Hardware,platform
The First Gene-editing Treatment (CRISPR),2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085101/crispr-gene-editing-sickle-cell-disease-breakthrough-technologies/,"The first gene-editing treatment: 10 Breakthrough Technologies 2024
Sickle-cell disease is the first illness to be beaten by CRISPR, but the new treatment comes with an expected price tag of $2 to $3 million.
By Antonio Regaladoarchive page
January 8, 2024
a scientist looks at a tall strand of DNA in a suspension of liquid. A hose sends the liquid back to an IV and into the arm of a patient seated comfortably in a domestic chair with two nice plants and a happy, observant cat.
SIMON LANDREIN
WHO
CRISPR Therapeutics, Editas Medicine, Precision BioSciences, Vertex Pharmaceuticals

WHEN
Now

The first gene-editing cure has arrived. Grateful patients are calling it “life changing.”

It was only 11 years ago that scientists first developed the potent DNA-snipping technology called CRISPR. Now they’ve brought CRISPR out of the lab and into real medicine with a treatment that cures the symptoms of sickle-cell disease.


Sickle-cell is caused by inheriting two bad copies of one of the genes that make hemoglobin. Symptoms include bouts of intense pain, and life expectancy with the disease is just 53 years. It affects 1 in 4,000 people in the US, nearly all of them African-American. 

So how did this disease become CRISPR’s first success? A fortuitous fact of biology is part of the answer. Our bodies harbor another way to make hemoglobin that turns off when we’re born. Researchers found that a simple DNA edit to cells from the bone marrow could turn it back on.

Related Story

a gloved hand reaches toward a section of DNA sequence gel
The lucky break behind the first CRISPR treatment
Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.
Many CRISPR treatments are in trials, but in 2022, Vertex Pharmaceuticals, based in Boston, was first to bring one to regulators for approval. That treatment was for sickle-cell. After their bone marrow was edited, nearly all the patients who volunteered in the trial were pain free. 

Good news. But the expected price tag of the gene-editing treatment is $2 to $3 million. And Vertex has no immediate plans to offer it in Africa—where sickle-cell disease is most common, and where it still kills children.

The company says this is because the treatment regimen is so complex. It involves a hospital stay; doctors remove the bone marrow, edit the cells, and then transplant them back. In countries that still struggle to cover basic health needs, the procedure remains too demanding. So simpler, cheaper ways to deliver CRISPR could come next.",1-10%,Positive,Biotech,Product
Exascale Computers,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085128/exascale-computing-breakthrough-technologies/,"Exascale computers: 10 Breakthrough Technologies 2024
Computers capable of crunching a quintillion operations per second are expanding the limits of what scientists can simulate.
By Sophia Chenarchive page
January 8, 2024

SIMOUL ALVA
WHO
Oak Ridge National Lab, Jülich Super­computing Centre, China’s Supercomputing Center in Wuxi

WHEN
Now

In May 2022, the global supercomputer rankings were shaken up by the launch of Frontier. Now the fastest supercomputer in the world, it can perform more than 1 quintillion (1018) floating-point operations per second. That’s a 1 followed by 18 zeros, also known as an exaflop. Essentially, Frontier can perform as many calculations in one second as 100,000 laptops.

With the launch of Frontier, located at Oak Ridge National Laboratory in Tennessee, the era of exascale computing officially began. Several more such exascale computers will soon join its ranks. In the US, researchers are installing two machines that will be about twice as fast as Frontier: El Capitan, at Lawrence Livermore National Laboratory in California, and Aurora, at Argonne National Laboratory in Illinois. Europe’s first exascale supercomputer, Jupiter, is expected to come online in late 2024. China reportedly also has exascale machines, although it has not released results from standard benchmark tests.


Related Story

Long rows of supercomputers with the name &quot;Frontier&quot; visible on the end
What’s next for the world’s fastest supercomputers
Scientists have begun running experiments on Frontier, the world’s first official exascale machine, while facilities worldwide build other machines to join the ranks.
Scientists and engineers are eager to use these turbocharged computers to advance a range of fields. Astrophysicists are already using Frontier to model the flow of gas in and out of the Milky Way; in addition to simulating motion on the scale of our galaxy, their model can zero in on exploding stars. This application showcases supercomputers’ unique ability to simulate physical objects at multiple scales simultaneously. 

The progress won’t stop here. For the last three decades, supercomputers have gotten about 10 times faster every four years or so. And the stewards of these machines are already planning the next models: Oak Ridge engineers are designing a supercomputer that will be three to five times faster than Frontier, likely to be unveiled in the coming decade. 

But one big challenge looms: the energy footprint. Frontier, which already employs energy-conserving innovations, draws enough power even while idling to run thousands of homes. Engineers will need to figure out how to build these behemoths not just for speed, but for environmental sustainability.",0.5% - 5.0%,Positive,Hardware,product
Heat Pumps,2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085105/heat-pumps-decarbonizing-homes-manufacturing-breakthrough-technologies/,"Heat pumps: 10 Breakthrough Technologies 2024
Heat pumps are a well-established technology. Now they’re starting to make real progress on decarbonizing homes, buildings, and even manufacturing.
By Casey Crownhartarchive page
January 8, 2024
a heat pump powering homes while a person carries additional connectors to local factories and businesses. A bird has also made a home on top of the unit.
SIMON LANDREIN
WHO
Daikin, Mitsubishi, Viessmann

WHEN
Now

Advertisement

We’ve entered the era of the heat pump. 

Heat pumps are appliances that can cool and heat spaces using electricity. Many buildings today are still heated with fossil fuels, specifically natural gas. Switching to electric heat pumps that run on renewable energy could help homes, offices, and even manufacturing facilities cut their emissions dramatically. 


While heat pumps have been used in buildings since the mid-20th century, the technology is breaking through in a new way. Global sales of heat pumps grew by 11% in 2022, the second consecutive year of double-digit growth, though that rate may have slowed in 2023. Europe saw the most dramatic shift, with a 40% growth in heat pump installations through 2022, largely driven by the energy crisis stemming from the Russia-Ukraine war and by efforts to move away from natural gas. 

Related Story

Bosch employee opening the housing of a heat pump in a cold chamber
Everything you need to know about the wild world of heat pumps
Heat pumps could help address climate change and save you money. Here’s how they work.
Asia is another hot spot, with China leading global installations and China and Japan together accounting for more than half of new patents filed on heat pump technology since 2010. New approaches are enabling heat pumps to reach higher temperatures, which could allow the technology to help clean up industrial manufacturing by supplying power to generate steam used in food processing and paper making. 

In total, heat pumps have the potential to cut global emissions by 500 million tons in 2030—as much as pulling all cars in Europe today off the roads. That would require the total number of heat pumps installed to reach about 600 million by the end of the decade. (That’s about 20% of the heating needs for all the world’s buildings.) 

There are still big challenges ahead for heat pumps, including ramping production to meet rising demand and ensuring that the electrical grid is robust enough to supply electricity to these and other climate-­focused technologies. But all signs indicate that heat pumps are entering their heyday.",0.5% - 5.0%,Positive,Hardware,product
Twitter Killers (alternatives like Bluesky and Threads),2024.0,https://www.technologyreview.com/2024/01/08/1085094/10-breakthrough-technologies-2024/,https://www.technologyreview.com/2024/01/08/1085109/decentralized-social-media-elon-musk-twitter-bluesky-mastodon-discord-breakthrough-technologies/,"Twitter killers: 10 Breakthrough Technologies 2024
Millions of people have fled the “bird site” owned by Elon Musk and flocked to decentralized social media services instead.
By Mat Honanarchive page
January 8, 2024
a hotel crime scene in in Hitchcock-era noir film lighting with a chalk outline of the twitter logo marked as exhibit 1. A tesla badge on the nightstand is marked 2 and a cell phone on the carpet is marked 3.
JENNIFER DIONISIO
WHO
Bluesky, Discord, Mastodon, Nostr, Threads

WHEN
Now

Advertisement

For the better part of 17 years, the roiling, rolling, fractious, sometimes funny, sometimes horrifying, never-ever-ending global conversation had a central home: Twitter. If you wanted to know what was happening and what people were talking about right now, it was the only game in town. 

But then Elon Musk purchased Twitter, renamed it X, fired most of its employees, and more or less eliminated its moderation and verification systems. He put in place a new financial structure that incentivized creators to spread and amplify lies and propaganda. Many people have begun casting about for a replacement service—ideally one that is beyond any individual’s control. 


Decentralized, or federated, social media allows for communication across independently hosted servers or platforms, using networking protocols such as ActivityPub, AT Protocol, or Nostr. It offers more granular moderation, more security against the whims of a corporate master or government censor, and the opportunity to control your social graph. It’s even possible to move from one server to another and follow the same people. 

Related Story

hand emerging from a phone holding a bag
How to fix the internet
If we want online discourse to improve, we need to move beyond the big platforms.
To be sure, the dream of a decentralized Twitter-like service has been around for years. History is littered with failed attempts—most notably App.net and Identi.ca. A real competitor never broke out because not enough people had a strong reason to leave Twitter, or a place to go if they did. Now they have both.

According to Similarweb, X’s traffic is down by nearly 20%, year over year. Another study, by Apptopia, found that the number of daily active users went from 141 million to 120 million. Meanwhile, decentralized services like Mastodon, Bluesky, and some Nostr clients have surged in popularity.

But it’s Threads, from Meta, that’s been the big winner. Meta disclosed in September that Threads already had nearly 100 million monthly users. (As of press time, Threads has not yet implemented ActivityPub, but it promises to do so.) Nerd favorite Mastodon is a distant second at 1.5 million active users but is growing, while the still invite-only Bluesky, which runs on the AT Protocol, is at 2 million. 

And of course, the real Twitter killer? That’s Elon Musk.",-0.05% to -0.5%,Negative,Software,platform
,,,,,0.1-1%,"There is no article to review. Please provide an actual article about the technology ""nan"".",Nanotech,concept
,,,,,3-30%,"Neither ""positive"" nor ""negative"" can be chosen as there is no description of the technology ""nan"".",Nanotech,concept
,,,,,0.01-0.1%,"Neither ""positive"" nor ""negative"" can be chosen as there is no information about the technology ""nan"" provided in the article, and ""nan"" doesn't seem to refer to a real technology.",Nanotech,product
Bill Gates Intro,1905.0,,,"Bill Gates: How we’ll invent the future

The thinking behind this year’s list of 10 Breakthrough Technologies began with the plow.
By Bill Gatesarchive page
February 27, 2019

IAN ALLEN
I was honored when MIT Technology Review invited me to be the first guest curator of its 10 Breakthrough Technologies. Narrowing down the list was difficult. I wanted to choose things that not only will create headlines in 2019 but captured this moment in technological history—which got me thinking about how innovation has evolved over time.

My mind went to—of all things—the plow. Plows are an excellent embodiment of the history of innovation. Humans have been using them since 4000 BCE, when Mesopotamian farmers aerated soil with sharpened sticks. We’ve been slowly tinkering with and improving them ever since, and today’s plows are technological marvels.

But what exactly is the purpose of a plow? It’s a tool that creates more: more seeds planted, more crops harvested, more food to go around. In places where nutrition is hard to come by, it’s no exaggeration to say that a plow gives people more years of life. The plow—like many technologies, both ancient and modern—is about creating more of something and doing it more efficiently, so that more people can benefit.

Contrast that with lab-grown meat, one of the innovations I picked for this year’s 10 Breakthrough Technologies list. Growing animal protein in a lab isn’t about feeding more people. There’s enough livestock to feed the world already, even as demand for meat goes up. Next-generation protein isn’t about creating more—it’s about making meat better. It lets us provide for a growing and wealthier world without contributing to deforestation or emitting methane. It also allows us to enjoy hamburgers without killing any animals.

Put another way, the plow improves our quantity of life, and lab-grown meat improves our quality of life. For most of human history, we’ve put most of our innovative capacity into the former. And our efforts have paid off: worldwide life expectancy rose from 34 years in 1913 to 60 in 1973 and has reached 71 today.

Because we’re living longer, our focus is starting to shift toward well-being. This transformation is happening slowly. If you divide scientific breakthroughs into these two categories—things that improve quantity of life and things that improve quality of life—the 2009 list looks not so different from this year’s. Like most forms of progress, the change is so gradual that it’s hard to perceive. It’s a matter of decades, not years—and I believe we’re only at the midpoint of the transition.

To be clear, I don’t think humanity will stop trying to extend life spans anytime soon. We’re still far from a world where everyone everywhere lives to old age in perfect health, and it’s going to take a lot of innovation to get us there. Plus, “quantity of life” and “quality of life” are not mutually exclusive. A malaria vaccine would both save lives and make life better for children who might otherwise have been left with developmental delays from the disease.

We’ve reached a point where we’re tackling both ideas at once, and that’s what makes this moment in history so interesting. If I had to predict what this list will look like a few years from now, I’d bet technologies that alleviate chronic disease will be a big theme. This won’t just include new drugs (although I would love to see new treatments for diseases like Alzheimer’s on the list). The innovations might look like a mechanical glove that helps a person with arthritis maintain flexibility, or an app that connects people experiencing major depressive episodes with the help they need.

If we could look even further out—let’s say the list 20 years from now—I would hope to see technologies that center almost entirely on well-being. I think the brilliant minds of the future will focus on more metaphysical questions: How do we make people happier? How do we create meaningful connections? How do we help everyone live a fulfilling life?

I would love to see these questions shape the 2039 list, because it would mean that we’ve successfully fought back disease (and dealt with climate change). I can’t imagine a greater sign of progress than that. For now, though, the innovations driving change are a mix of things that extend life and things that make it better. My picks reflect both. Each one gives me a different reason to be optimistic for the future, and I hope they inspire you, too.

My selections include amazing new tools that will one day save lives, from simple blood tests that predict premature birth to toilets that destroy deadly pathogens. I’m equally excited by how other technologies on the list will improve our lives. Wearable health monitors like the wrist-based ECG will warn heart patients of impending problems, while others let diabetics not only track glucose levels but manage their disease. Advanced nuclear reactors could provide carbon-free, safe, secure energy to the world.

One of my choices even offers us a peek at a future where society’s primary goal is personal fulfillment. Among many other applications, AI-driven personal agents might one day make your e-mail in-box more manageable—something that sounds trivial until you consider what possibilities open up when you have more free time.

The 30 minutes you used to spend reading e-mail could be spent doing other things. I know some people would use that time to get more work done—but I hope most would use it for pursuits like connecting with a friend over coffee, helping your child with homework, or even volunteering in your community.

That, I think, is a future worth working toward.

hide

by Bill Gates",0% - 1%,positive,Other,concept
Are you ready to be a techno-optimist again?,1905.0,https://www.technologyreview.com/2021/02/24/1017782/are-you-ready-to-be-a-techno-optimist-again/,,"Are you ready to be a techno-optimist again?
In 2001, we picked our first annual set of 10 breakthrough technologies. Here’s what their fates tell us about progress over the last two decades.
By David Rotmanarchive page
February 24, 2021

Twenty years ago, MIT Technology Review picked 10 emerging areas of innovation that we promised would “change the world.” It was a time of peak techno-optimism. Yes, the dot-com boom was in the midst of imploding; some insiders were already fretting about the end of Moore’s Law. (They still are, though the industry keeps finding ways to make computers more powerful.) But in many ways it was a glorious time for science and technology. 

A working draft of the human genome was published in February of 2001—a genetic blueprint that promised to reveal our deepest biological secrets. There was great excitement over recent breakthroughs in nanotechnology. Early advances in quantum and molecular computing portended a new, post–Moore’s Law era of computation. And then there was that amazing search engine with the funny name, rapidly gaining users and changing how they surfed the web and accessed information. Feeling lucky? 

Advertisement

So it’s worthwhile to look back at the initial “TR10,” as we now call our annual list, for clues to just how much progress we’ve made. 

First, let’s acknowledge that it was a thoughtful list. We eschewed robotic exoskeletons and human cloning, as well as molecular nanomanufacturing and the dreaded gray goo of the nano doomsayers—all hot topics of the day. Instead we focused on fundamental advances in information technology, materials, and biotech. Most of the technologies are still familiar: data mining, natural-language processing, microfluidics, brain-machine interfaces, biometrics (like facial recognition), and robot design. 

So how well did these technologies fulfill the dreams we had for them two decades ago? Here are a few lessons from the 2001 list. 


MIT TECHNOLOGY REVIEW
Lesson 1: 
Progress is often slow
Our first selection, brain-machine interfaces, begins with a description of the neuroscientist Miguel Nicolelis recording the electric signals from the brain of a very cute owl monkey named Belle as she thinks about how to get a few drops of apple juice. Flash forward to late summer 2020, as Elon Musk shows off the brain signals from a very cute pig named Gertrude, gaining oohs and ahhs from adoring fans attending the demonstration for Neuralink, his brain-machine startup. 

Related Story


Elon Musk’s Neuralink is neuroscience theater
Elon Musk’s livestreamed brain implant event made promises that will be hard to keep.
An observer at Musk’s event might have been forgiven for wondering whether 20 years had really passed since Nicolelis’s experiment. Both men had similar visions for directly connecting the brain to computing devices via implanted chips. As our biomedicine editor, Antonio Regalado, wrote in 2001, “Nicolelis sees the effort as part of the impending revolution that could eventually make [brain interfaces] as common as Palm Pilots.” 

That claim has come true, but thanks only to the demise of Palm Pilots, not the popularity of brain-machine interfaces. Despite some encouraging human experiments over the years, such interfaces remain a scientific and medical oddity. As it turns out, neuroscience is very difficult. There has been success in shrinking the electronics and making the implants wireless, but progress in the science has been slower, hindering the visions Nicolelis and Musk hoped to realize. (A footnote to lesson one: success often depends on whether a series of advances can all come together. Making brain interfaces practical requires advances in both the science and the gadgetry.) 

Lesson 2: 
Sometimes it takes a crisis
We chose microfluidics in 2001 because of some remarkable advances in moving tiny amounts of biological samples around on a small device—a so-called lab-on-a-chip. These promised quick diagnostic tests and the ability to automate drug and genomic experiments. 

Since then, microfluidics has found valuable uses in biology research. Clever advances continued, such as ultra-cheap and easy-to-use paper diagnostic tests (“Paper Diagnostics” was a TR10 in 2009). But the field has fallen short of its promise of transforming testing. There simply wasn’t an overwhelming demand for the technology. It’s fair to say that microfluidics became a scientific backwater. 

Covid-19 ended that. Conventional tests rely on multistep procedures done in an analytical lab; this is expensive and slow. Suddenly, there is an appetite for a fast and cheap lab-on-a-chip solution. It took a few months for researchers to dust off the technology, but now covid-19 diagnostics using microfluidics are appearing. These techniques, including one that uses CRISPR gene editing, promise to make covid tests far more accessible and widely used. 

Lesson 3:
Be careful what you wish for
In 2001, Joseph Atick, one of the pioneers of biometrics, saw facial recognition as a way for people to interface with their gadgets and computers more securely and easily. It would give the cell phones and personal digital assistants that were increasingly popular a way to recognize their owners, spelling the end of PINs and passwords. Part of that vision eventually came true with such applications as Apple’s FaceID. But facial recognition also took a turn that Atick now says “shocks me.” 

Related Story


Podcast: Facial recognition is quietly being used to control access to housing and social services
In this episode, we look at the debate over how and whether public funds should be used to deploy facial recognition in vulnerable communities
In 2001, facial-recognition algorithms were limited. They required instructions from humans, in mathematical form, on how to identify the distinguishing features of a face. And every face in the database of faces to be recognized had to be laboriously scanned into the software. 

Then came the boom in social media. Whereas in the early days, Atick says, he would have been thrilled with 100,000 images in facial-recognition databases, suddenly machine-learning algorithms could be trained on billions of faces, scraped from Facebook, LinkedIn, and other sites. There were now hundreds of these algorithms, and they trained themselves, simply by ingesting and comparing images—no expert human help required.  

But that remarkable advance came with a trade-off: no one really understands the reasoning the machines use. And that’s a problem now that facial recognition is increasingly relied on for sensitive tasks like identifying criminal suspects. “I did not envision a world where these machines would take over and make decisions for us,” says Atick. 

Lesson 4:
The trajectory of progress matters
“Hello again, Sidney P. Manyclicks. We have recommendations for you. Customers who bought this also bought …”

The recommendation engines described in this, the opening of our 2001 article on data mining, seemed impressive at the time. Another potential use of data mining circa 2001 also sounded thrilling: computer-­searchable video libraries. Today, it all seems utterly mundane.

Thanks to ever increasing computational power, the exploding size of databases, and closely related advances in artificial intelligence, data mining (the term is now often interchangeable with AI) rules the business world. It’s the lifeblood of big tech companies, from Google and its subsidiary YouTube to Amazon and Facebook. It powers advertising and, yes, sales of everything from shoes to insurance, using personalized recommendation engines.

Have these technologies made our lives not just more convenient, but better in ways that we care about?
Yet these great successes mask an underlying failure that became particularly evident during the pandemic. We have not exploited the power of big data in areas that matter most. 

At almost every step, from the first signs of the virus to testing and hospitalization to the rollout of vaccines, we’ve missed many opportunities to gather data and mine it for critical information. We could have learned so much more about how the virus spreads, how it evolves, how to treat it, and how to allocate resources, potentially saving countless lives. We didn’t seem to have a clue about how to collect the data we needed.

Overall, then, the 10 technologies we picked in 2001 are still relevant; none has been forsaken; and some have been remarkable, even world-changing, successes. But the real test of progress is more difficult: Have these technologies made our lives not just more convenient, but better in ways that we care about? How do we measure that progress?

What makes you happy? 
The common way to gauge economic progress is by measuring gross domestic product (GDP). It was formulated in the 1930s in the US to help us understand how well the economy was recovering from the Great Depression. And though one of its chief architects, Simon Kuznets, warned that GDP shouldn’t be mistaken for a measure of the country’s well-being and the prosperity of its people, generations of economists and politicians have done just that, scrutinizing GDP numbers for clues to the health of the economy and even the pace of technological progress.

Economists can tease out what they call total factor productivity (TFP) from GDP statistics; it’s basically a measure of how much innovation contributes to growth. In theory, new inventions should increase productivity and cause the economy to grow faster. Yet the picture has not been great over the last two decades. Since around the mid-2000s—shortly after our first TR10 list—growth in TFP has been sluggish and disappointing, especially given the flood of new technologies coming from places like Silicon Valley. 

Some economists think the explanation may be that our innovations are not as far-reaching as we think. But it’s also possible that GDP, which was designed to measure the industrial production of the mid-20th century, does not account for the economic benefits of digital products, especially when they’re free to use, like search engines and social media.


Stanford economist Erik Brynjolfsson and his colleagues have created a new measure to try to capture the contribution of these digital goods. Called GDP-B (the “B” is for benefits), it is calculated by using online surveys to ask people just how much they value various digital services. What would you have to be paid, for example, to live a month without Facebook?

The calculations suggest that US consumers have gained some $225 billion in uncounted value from Facebook alone since 2004. Wikipedia added $42 billion. Whether GDP-B could fully account for the seeming slowdown in productivity is uncertain, but it does provide evidence that many economists and policymakers may have undervalued the digital revolution. And that, says Brynjolfsson, has important implications for how much we should invest in digital infrastructure and prioritize certain areas of innovation.

GDP-B is one of a larger set of efforts to find statistics that more accurately reflect the changes we care about. The idea is not to throw out GDP, but to complement it with other metrics that more broadly reflect what we might call “progress.”

Another such measure is the Social Progress Index, which was created by a pair of economists, MIT’s Scott Stern and Harvard’s Michael Porter. It collects data from 163 countries on factors including environmental quality, access to health care and education, traffic deaths, and crime. While wealthier countries, unsurprisingly, tend to do better on this index, Stern says the idea is to look at where social progress diverges from GDP per capita. That shows how some countries, even poor ones, are better than others at turning economic growth into valued social changes. 

Survey from 13 countries shows generation gap
""Imagining when the covid-19 pandemic is over ... which should your country prioritize more?""

Bar chart
SOURCE: IPSOS/SOCIAL PROGRESS IMPERATIVE
The US, with one of the world’s highest levels of GDP per capita, is 28th in the index, and is one of only four countries whose scores have declined since 2014. Norway, which is similarly wealthy, was ranked first in 2020 (see chart below). Some poorer countries also outperform. 

“Very often the decisions about innovation and technology are about its economic impact,” says Stern. “There’s nothing wrong with that. But are we directing the economic rewards to areas that will advance social progress?”


A similar thought lies behind another alternative to GDP, developed by Diane Coyle and her colleagues at the Bennett Institute for Public Policy in Cambridge, UK. Their measure of what they call the wealth economy is based on what they define as the assets of a society, including its human capital (the health and skills of its people), natural capital (its resources and the health of the environment), and social capital (trust and social cohesion).

It’s a hugely ambitious project that attempts to create a couple of key measurements for each asset. Those numbers, says Coyle, are meant to inform better decisions about technology and innovation, including decisions on the priorities for government investment. She says the approach allows you to ask, “What is the technology doing for people?”

The value of these various alternatives to GDP is that they provide a broader picture of how our lives are changing as a result of technology. Had they been in place 20 years ago, they might have shined light on crises we were late in seeing, such as the growth of income inequality and the rapid deterioration of our climate. If 20 years ago was a time of peak techno-­optimism, it might have prompted us to ask, “Optimism about what?” 


Born-again hope
About a decade ago, the techno-optimist narrative began to fall apart.

In 2011 Tyler Cowen, an economist at George Mason University in Virginia, wrote The Great Stagnation, arguing that the technologies that seemed so impressive at the time—especially social media and smartphone apps—were doing little to stimulate economic growth and improve people’s lives. The Rise and Fall of American Growth, a 2016 bestseller by Robert Gordon, another prominent economist, ran to more than 700 pages, detailing the reasons for the slowdown in TFP after 2004. The temporary boom from the internet, he declared, was over. 

The books helped kick off an era of techno-­pessimism, at least among economists. And in the last few years, problems like misinformation on social media, the precarious livelihoods of gig-economy workers, and the creepier uses of data mining have fueled a broader pessimist outlook—a sense that Big Tech not only isn’t making society better but is making it worse. 

These days, however, Cowen is returning to the optimist camp. He’s calling for more research to explain progress and how to create it, but he says it’s “a more positive story” than it was a few years ago. The apparent success of covid vaccines based on messenger RNA has him excited. So do breakthroughs in using AI to predict protein folding, the powerful gene-editing tool CRISPR, new types of batteries for electric vehicles, and advances in solar power. 

“What is the technology doing for people?”
Diane Coyle
An anticipated boom in funding from both governments and businesses could amplify the impact of these new technologies. President Joe Biden has pledged hundreds of billions in infrastructure spending, including more than $300 billion over the next four years for R&D. The EU has its own massive stimulus bill. And there are signs of a new round of venture capital investments, especially targeting green tech. 

If the techno-optimists are right, then our 10 breakthrough technologies for 2021 could have a bright future. The science behind mRNA vaccines could open a new era of medicine in which we manipulate our immune system to transform cancer treatment, among other things. Lithium-metal batteries could finally make electric cars palatable for millions of consumers. Green hydrogen could help replace fossil fuels. The advances that made GPT-3 possible could lead to literate computers as the next big step in artificial intelligence. 

Still, the fate of the technologies on the 2001 list tells us that progress won’t happen just because of the breakthroughs themselves. We will need new infrastructure for green hydrogen and electric cars; new urgency for mRNA science; and new thinking around AI and the opportunities it presents in solving social problems. In short, we need political will.

But the most important lesson from the 2001 list is the simplest: Whether these breakthroughs fulfill their potential depends on how we choose to use them. And perhaps that’s the greatest reason for renewed optimism, because by developing new ways of measuring progress, as economists like Coyle are doing, we can also create new aspirations for these brilliant new technologies. If we can see beyond conventional economic growth and start measuring how innovations improve the lives of as many people as possible, we have a much greater chance of creating a better world.","I'm not aware of any specific technology called ""Are you ready to be a techno-optimist again?"", so I'll assume this is a hypothetical or rhetorical question.

That being said, here's my attempt at providing an estimate:

0% - 1%","The article discusses the potential impact of technological advancements on society and the economy. It highlights the limitations of using GDP as a measure of progress and introduces alternative measures such as the ""Wealth Economy"" developed by Diane Coyle and her colleagues.

The article also notes that about a decade ago, the techno-optimist narrative began to fall apart, with economists like Tyler Cowen and Robert Gordon arguing that technologies were not stimulating economic growth or improving people's lives. However, Cowen has recently returned to the optimist camp, citing breakthroughs in areas such as mRNA vaccines, AI, and green tech.

The article concludes that while technological advancements have the potential to improve society, their impact depends on how they are used. It emphasizes the need for new infrastructure, urgency, and thinking around these technologies to fulfill their potential. Ultimately, it suggests that by developing new ways of measuring progress and creating new aspirations for these technologies, we can create a better world.

Some key points from the article include:

* GDP is not an adequate measure of progress and alternative measures are needed.
* Technological advancements have the potential to improve society, but their impact depends on how they are used.
* Breakthroughs in areas such as mRNA vaccines, AI, and green tech offer new opportunities for growth and improvement.
* New infrastructure, urgency, and thinking are needed to fulfill the potential of these technologies.
* Developing new ways of measuring progress can help create a better world.

The article also mentions that Norway was ranked first in 2020 in terms of innovation and technology, while some poorer countries also outperform. The US is 28th in the index, and its score has declined since 2014.",Other,"The article discusses the relationship between technological progress and societal well-being, questioning whether breakthroughs in technology necessarily lead to improvements in people's lives. It notes that while the past two decades have seen tremendous advancements in fields like AI, biotechnology, and renewable energy, many problems such as income inequality, climate change, and social unrest persist.

The article cites economists who argue that traditional measures of economic growth, such as GDP, are insufficient for capturing the impact of technological progress on society. Alternative measures, such as those proposed by Diane Coyle and her colleagues at the Bennett Institute for Public Policy, focus on a broader range of assets, including human capital, natural capital, and social capital.

The article also notes that even among economists, there has been a shift from techno-optimism to techno-pessimism in recent years. However, some experts, such as Tyler Cowen, are now returning to a more optimistic outlook, citing breakthroughs in fields like mRNA vaccines, AI, and renewable energy.

Ultimately, the article argues that whether technological progress leads to improvements in people's lives depends on how we choose to use these breakthroughs. It emphasizes the need for new infrastructure, new urgency around scientific research, and new thinking around the opportunities presented by technologies like AI. By developing new ways of measuring progress and creating new aspirations for these technologies, we can increase our chances of creating a better world.

Some key points from the article include:

* Traditional measures of economic growth, such as GDP, may not be sufficient for capturing the impact of technological progress on society.
* Alternative measures of progress, such as those proposed by Diane Coyle and her colleagues, focus on a broader range of assets, including human capital, natural capital, and social capital.
* There has been a shift from techno-optimism to techno-pessimism among economists in recent years, but some experts are now returning to a more optimistic outlook.
* Breakthroughs in fields like mRNA vaccines, AI, and renewable energy offer opportunities for significant progress, but their impact depends on how we choose to use them.
* New infrastructure, new urgency around scientific research, and new thinking around the opportunities presented by technologies like AI are needed to realize the potential of these breakthroughs."
2022 Intro,1905.0,,,"2022’s seismic shift in US tech policy will change how we innovate
Three bills investing hundreds of billions into technological development could change the way we think about government’s role in growing prosperity.
By David Rotmanarchive page
January 9, 2023
President Joe Biden speaks during a groundbreaking for a new Intel computer chip facility.
AP PHOTO/MANUEL BALCE CENETA

This essay is part of MIT Technology Review's 10 Breakthrough Technologies 2023. Explore the full list here.

It was the perfect political photo op. The occasion was the September groundbreaking for Intel’s massive $20 billion chip manufacturing complex in the suburbs of Columbus, Ohio. Backhoes dotted a construction site that stretched across hundreds of flat, empty acres. At a simple podium with the presidential seal, Joe Biden talked about putting an end to the term “Rust Belt,” a name popularized in the 1980s in reference to the Midwest’s rapidly declining manufacturing sector.

It was a presidential victory lap after the passage of some landmark US legislation, beginning with the infrastructure bill in late 2021. Together, three major bills promise hundreds of billions in federal investments to transform the nation’s technology landscape. While ending the Rust Belt might be typical political hyperbole, you get the point: the spending spree is meant to revive the country’s economy by rebuilding its industrial base. 

The dollar amounts are jaw-dropping. The bills include $550 billion in new spending over the next five years in the Infrastructure Investment and Jobs Act, $280 billion in the CHIPS and Science Act (which prompted Intel to go ahead on the Ohio construction), and another roughly $390 billion for clean energy in the Inflation Reduction Act. Among the investments is the most aggressive federal funding for science and technology in decades. But the greatest long-term impact of the legislative flurry could come from its bold embrace of something that has long been a political third rail in the US: industrial policy. 


That means deliberate government interventions, including financial incentives and investments, favoring growth in particular industries or technologies—say, for national security reasons or to address problems such as climate change. Think of US support for semiconductor manufacturing in the 1980s or the creation during the Cold War of the Defense Advanced Research Projects Agency (DARPA), which led to the internet and GPS.  

But for decades now, free-market advocates have disparaged industrial policy as a foolhardy attempt to pick economic winners. Since the early 1980s and the era of Ronald Reagan, US politicians and many mainstream economists have disdained it. In reality, it never completely went away. President Obama toyed with elements of it in trying to revive manufacturing in the US after the 2008 recession; President Trump turned to it in his Operation Warp Speed to mobilize industry around covid vaccine development. But for the most part, it has seemed foreign to US political thinking: it was something China does, something Japan, South Korea, and France used to do (remember the Concorde?). 


Flourish logoA Flourish chart
The US has effective and productive free markets. And, of course, we have Silicon Valley, our own engine of economic growth, propelling the economy forward. All we need to do is unleash that engine by loosening regulations and cutting taxes. Or so the dominant narrative went. 

That narrative began crumbling long before the covid-19 pandemic made clear the need for the government to help bolster critical industrial sectors and supply chains. An unblinking faith in free markets has led to globalization, helping to gut many of the country’s industries, particularly in manufacturing. For a while, the economic argument was that it didn’t matter where you made stuff; cheap commodities were good for living standards, and the country should focus on high-tech growth. 

The problem is that high-tech growth has been limited, anemic, and unevenly distributed. Income inequality has climbed to high levels. The Rust Belt and other sections of the middle of the country keep getting rustier. Despite impressive advances in artificial intelligence and other areas of high tech, the nation’s prosperity has largely benefited people in only a few regions; notably, experts have begun identifying a handful of superstar cities, including San Francisco, Seattle, and Boston, that are booming while the rest of the country suffers. Perhaps most telling, growth of productivity—particularly the kind related to innovation, called total factor productivity—has been sluggish for several decades now in the US and many other rich countries. 

I wrote about the failure of technologies such as social media and artificial intelligence to boost productivity growth in the mid-2010s, in an essay titled “Tech slowdown threatens the American Dream.” Since then, the situation hasn’t gotten any better, roiling US politics and fueling a mood of economic malaise. 

Advertisement

What’s changed now is that the new legislation, which passed with some degree of bipartisan support in Congress, signals a strong appetite across the political spectrum for the US government to reengage with the country’s industrial base. After decades of declining federal investment in R&D, which dropped from 1.2% of GDP in the late 1970s to below 0.8% in recent years, the CHIPS and Science Act alone authorizes some $174 billion for research at places like the National Science Foundation.

Related Story


Are you ready to be a techno-optimist again?
In 2001, we picked our first annual set of 10 breakthrough technologies. Here’s what their fates tell us about progress over the last two decades.
Part of the reason the legislation received such broad support is that the funding provisions are a bit of Rorschach test. Some see measures to defend critical national technology businesses like chip production against the threat from China, and to make sure we don’t lose the global race in areas such as AI and quantum computing. Others see green jobs and efforts to address climate change, and a return to the post–World War II recognition that investing in science and research is critical to economic well-being. 

Still, despite the differences in motivation, the federal government’s willingness to embrace hawkish industrial policy is at least providing a chance to rethink the role the state plays in innovation. “It’s not just an opportunity—it’s a necessity,” says Dan Breznitz, the Peter J. Munk professor of Innovation Studies at the University of Toronto and co-director of its Innovation Policy Lab. After decades, he says, it’s time the US government got back in the game of “understanding the importance of merging innovation strategy with industrial policy.” 

Likewise, the European Union, South Korea and Japan, countries in the Middle East, and various other members of the Organization for Economic Cooperation and Development are all “back on the industrial-policy bandwagon,” says Dani Rodrik, an economist at Harvard. “It’s not like industrial policy ever went away,” says Rodrik, “but now it’s at the center of the conversation.” Instead of being embarrassed by the topic, he says, politicians are now touting it as a strategy. 

For economists like Diane Coyle, an expert on productivity and the emerging digital economy, the need for industrial policy to promote targeted growth is obvious at a time when productivity is stagnant, climate change is reaching a crisis point, and the rapid digitalization of the economy is worsening inequality. “We absolutely do need industrial policy in the kind of economy we have now,” says Coyle, the co-director of the Bennett Institute for Public Policy at the University of Cambridge. “But the catch, of course, is it’s difficult to do, and governments often don’t do it well.” 

What about Solyndra?
The well-worn critique that industrial policy asks governments to pick winners, something they aren’t particularly good at, doesn’t really hold up to scrutiny. For every Solyndra (a solar company that received a half-billion-dollar federal loan guarantee before flaming out, and the favorite example of a disastrous losing pick), there is a Tesla—funded around the same time by a federal loan. But the criticism does have some truth to it; industrial policy requires, well, policies. It requires choices. 

The US legislation passed over the last year is really a series of different industrial and innovation strategies. There’s a classic industrial policy that singles out support to the chip industry; a green industrial policy in the Inflation Reduction Act (which is often called the climate bill) that broadly favors specific types of companies such as EV manufacturers; and other spending choices and policies scattered throughout the bills that aim to create new jobs. Arguably the most important provisions, at least according to some economists, are those designed to boost federal support for R&D.

There is no obvious, coherent vision tying it all together. 

For now, says David Victor, a professor of innovation and public policy at the University of California, San Diego, that’s fine. “It’s more like industrial policy à la carte,” he says. It’s based on what is politically possible, appeasing different interests, from labor to industry to climate activists. Now, says Victor, “we need to turn it into as effective industrial policy as possible.”


Flourish logoA Flourish hierarchy chart
One challenge will be dealing with potentially conflicting priorities. For example, the climate bill’s generous tax incentives for electric vehicles come with a few stipulations. The EVs must be assembled in North America. What’s more, the battery components must be made or assembled in North America and the critical metals going into the batteries must be mined in the US or by its free-trade partners. That might boost long-term domestic manufacturing, creating jobs and building more reliable supply chains, but it also could create a bottleneck in EV production. If that happens, it could slow down efforts to reduce carbon emissions. 

Various other trade-offs and choices loom as the country ramps up its technology investments. To help make better choices, Erica Fuchs, a professor of engineering and public policy at Carnegie Mellon, and her collaborators have started a pilot project, funded by the NSF, that will use advanced data analysis and cross-disciplinary expertise from a team of university researchers to better inform policy makers on technology decisions.

Called the National Network for Critical Technology Assessment, it’s meant to provide useful information on different options to meet various geopolitical and economic objectives. For example, given US dependency on China for lithium and the Democratic Republic of the Congo for cobalt, and given the risks of those supply chains, what is the potential value of innovations in battery recycling, alternative battery chemistries (such as ones that don’t use cobalt), and alternative extraction technologies? Likewise, there are questions around what parts of domestic battery manufacturing are most important for creating US jobs. 

While much analysis has already gone into writing the legislation, says Fuchs, many more questions will come up as the government attempts to spend the allocated funds to best realize legislative goals. She hopes the project will eventually lead to a larger network of experts from academia, industry, and government that provide the tools to clarify and quantify opportunities emerging from US innovation policies. 

A new story
Any new narrative that the government can promote innovation and use it to foster economic prosperity is still very much a work in progress. It’s not yet clear how the various provisions in the different pieces of legislation will play out. Perhaps most worrisome, the large jumps in funding for R&D in the CHIPS and Science Act are simply authorizations—recommendations that Congress will need to work into the budget anew every year. A switch in political mood could quickly kill the funding.


But perhaps the greatest unknown is how the federal funding will affect local economies and the welfare of millions of Americans who have suffered decades of lost manufacturing and declining job opportunities. Economists have long argued that technological advances are what drive economic growth. But over the last few decades, the prosperity resulting from such advances has been largely restricted to a few high-tech industries and has mostly benefited a relatively small elite. Can the public once again be convinced that innovation can lead to widespread prosperity? 


Flourish logoInteractive content by Flourish
One worry is that while the recent legislation strongly supports semiconductor manufacturing and assorted clean technologies, the bills do little to create good jobs where they are most needed, says Harvard’s Rodrik. “In terms of bang for the buck,” he says, investing in advanced manufacturing and semiconductors “is one of the least effective ways of creating good jobs.” There is, he says, a “kind of manufacturing nostalgia” and a belief that rebuilding this sector will bring the middle class back. But that’s illusory, he says, since today’s advanced manufacturing is highly automated, and facilities tend to employ relatively few workers. 

Rodrik proposes what he calls an industrial policy for good jobs that would move beyond manufacturing and target the service sector, where by far the most jobs are in the US. His plan calls for investing in new technologies and companies that would improve productivity in jobs long thought of as low-skilled. For example, he points to opportunities to increase the capabilities of people working in long-term care, an area that is exploding as the population ages, by giving them digital tools. 

We also need to drop the pretensions around Silicon Valley’s role in creating widespread prosperity. A little more than six years ago, I wrote an essay titled “Dear Silicon Valley: Forget flying cars, give us economic growth.” Even with the advent of AI and driverless cars, economists were fretting over slow productivity growth. The inability of those in Silicon Valley to develop and commercialize the types of technologies and innovations that produce growth across a broad swath of the economy was clear. 

The tech industry gave us Zoom to survive the pandemic, and Amazon went on a hiring spree, but none of this led to a widespread economic expansion. We’re still waiting for the long-anticipated economy-­wide productivity boom from AI. These days, I would tweak the message: Forget about Silicon Valley and look elsewhere for economic transformation. 

Related Story


Why tech didn’t save us from covid-19
America's paralysis reveals a deep and fundamental flaw in how the nation thinks about innovation.
If not Silicon Valley and other centers of innovation, where will that transformation come from? Though federal legislation has kick-started the discussion about industrial policy and innovation strategies, any real change will have to happen through efforts by cities and states. Each city, says Breznitz of the University of Toronto, will need to figure things out for itself, creating innovation strategies that work for its people on the basis of its industrial base, educational resources, and type of workforce. And, he admonishes, cities need to stop pinning their hopes on an elusive high-tech strategy modeled on Silicon Valley. 

“Two hundred cities in the US are all trying to look like Silicon Valley,” Breznitz says, adding, “I don’t know why. Maybe they’ve never been to Silicon Valley?”

A key, he says, is recognizing that inventions are just one stage of innovation. Local governments need to support what he calls continuous innovation by helping local companies and industries offer improved and cheaper products and services. It might not be as glamorous as coming up with a novel idea for a radical new business, but it’s how most companies and regions become more productive and localities prosper. 

Creating a convincing narrative that large parts of the country buy into will take time. But that, says UCSD’s Victor, is precisely the point of industrial policy: “You begin to change the facts on the ground. You create new industries and jobs. And then the politics shift.”

Before that happens, of course, lots can go wrong. Successful industrial policy depends on consistent and disciplined choices by politicians. You can decide for yourself whether you think they will manage that. 

But one reason for renewed optimism is that today’s technologies, especially artificial intelligence, robotics, genomic medicine, and advanced computation, provide vast opportunities to improve our lives, especially in areas like education, health care, and other services. If the government, at the national and local level, can find ways to help turn that innovation into prosperity across the economy, then we will truly have begun to rewrite the prevailing political narrative.",0% - 10%,"The article discusses the efforts of the US government to revitalize its manufacturing sector and create new jobs through industrial policy. The recently passed CHIPS and Science Act provides funding for research and development in areas such as semiconductors and clean technologies. However, experts argue that this approach may not be effective in creating good jobs, as advanced manufacturing is highly automated and employs relatively few workers.

Harvard economist Dani Rodrik proposes an industrial policy that targets the service sector, where most jobs are located, and invests in new technologies to improve productivity in areas such as long-term care. He also suggests that Silicon Valley's role in creating widespread prosperity has been overstated, and that innovation strategies should focus on improving the lives of people across the economy.

Experts argue that cities and states will play a crucial role in implementing industrial policy and creating innovation strategies that work for their specific regions. They need to stop trying to emulate Silicon Valley and instead focus on supporting local companies and industries to offer improved and cheaper products and services.

The article concludes that creating a convincing narrative around industrial policy will take time, but it is necessary to change the facts on the ground and create new industries and jobs. With consistent and disciplined choices by politicians, successful industrial policy can lead to widespread prosperity.

Some of the key points in the article include:

* The US government has passed legislation to revitalize its manufacturing sector and create new jobs through industrial policy.
* Experts argue that this approach may not be effective in creating good jobs, as advanced manufacturing is highly automated and employs relatively few workers.
* An alternative approach targets the service sector, where most jobs are located, and invests in new technologies to improve productivity.
* Cities and states will play a crucial role in implementing industrial policy and creating innovation strategies that work for their specific regions.
* Silicon Valley's role in creating widespread prosperity has been overstated, and innovation strategies should focus on improving the lives of people across the economy.
* Creating a convincing narrative around industrial policy will take time, but it is necessary to change the facts on the ground and create new industries and jobs.",Hardware,"The US has recently passed several pieces of legislation aimed at boosting innovation and economic growth, including the CHIPS and Science Act. However, it's unclear how these provisions will play out in practice, and there are concerns about whether they will benefit local economies and create good jobs.

One worry is that investing in advanced manufacturing and semiconductors may not be the most effective way to create jobs, as these industries tend to be highly automated. Instead, some experts argue for an industrial policy that targets the service sector, where most jobs are located, and invests in new technologies and companies that can improve productivity in areas like long-term care.

There is also a need to move beyond the idea that Silicon Valley will drive widespread economic growth. Despite its reputation as a hub of innovation, the tech industry has not delivered on promises of broad-based economic expansion. Instead, cities and states may need to develop their own innovation strategies, focusing on continuous innovation and supporting local companies and industries.

Creating a convincing narrative around industrial policy will take time, but it's possible that by changing the facts on the ground and creating new industries and jobs, the politics will shift in favor of policies that promote widespread prosperity. The current technologies, such as AI and robotics, provide vast opportunities to improve lives, especially in areas like education and healthcare.

Ultimately, successful industrial policy depends on consistent and disciplined choices by politicians. While there are reasons for optimism, there is also a risk that the efforts will falter or be derailed by political shifts."
